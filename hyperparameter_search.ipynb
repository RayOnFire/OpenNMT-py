{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[TOC]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onmt\n",
    "from onmt.trainer import Trainer\n",
    "from onmt.utils.logging import logger\n",
    "from onmt.utils.loss import build_loss_compute\n",
    "import onmt.opts as opts\n",
    "from onmt.inputters.inputter import build_dataset_iter, \\\n",
    "    load_old_vocab, old_style_vocab\n",
    "from onmt.model_builder import build_model\n",
    "from onmt.utils.optimizers import Optimizer\n",
    "from onmt.utils.misc import set_random_seed\n",
    "from onmt.models import build_model_saver\n",
    "from onmt.utils.logging import init_logger, logger\n",
    "from onmt.utils.misc import split_corpus\n",
    "from onmt.translate.translator import build_translator\n",
    "\n",
    "import itertools\n",
    "import configargparse\n",
    "import os\n",
    "from itertools import chain\n",
    "import torch\n",
    "import subprocess, shlex, re, uuid, hashlib, json, gc, shutil, sys\n",
    "from pprint import pprint\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ArgumentHelper:\n",
    "    def build(self, arg_dict):\n",
    "        arg_total = self.base_arg_dict.copy()\n",
    "        arg_total.update(arg_dict)\n",
    "        return arg_total\n",
    "    \n",
    "    @staticmethod\n",
    "    def save_args(arg_dict, filename):\n",
    "        with open(filename + '.txt', 'w') as f:\n",
    "            json.dump(arg_dict, f, indent=4)\n",
    "    \n",
    "    @staticmethod\n",
    "    def convert_dict_to_arg_array(arg_dict):\n",
    "        arr = []\n",
    "        for k, v in arg_dict.items():\n",
    "            arr.append('-' + k)\n",
    "            if v is not None:\n",
    "                arr.append(str(v))\n",
    "        return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_translate_args(model_id):\n",
    "    return {\n",
    "        'src': 'data/nmt15/test_en',\n",
    "        'tgt': 'data/nmt15/test_vi',\n",
    "        'out': '/mnt/drive-2t/pred/opennmt_' + model_id + '.txt',\n",
    "        'replace_unk': None,\n",
    "        'gpu': '0',\n",
    "        'batch_size': '16'\n",
    "    }\n",
    "\n",
    "def build_translate_args_reverse(model_id):\n",
    "    return {\n",
    "        'src': 'data/nmt15/test_vi',\n",
    "        'tgt': 'data/nmt15/test_en',\n",
    "        'out': '/mnt/drive-2t/pred/opennmt_' + model_id + '.txt',\n",
    "        'replace_unk': None,\n",
    "        'gpu': '0',\n",
    "        'batch_size': '16'\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_path_with_no_checkpoint(model_path, log_path):\n",
    "    useless_file = []\n",
    "    available_model = set()\n",
    "    useless_log = []\n",
    "    for root, dirs, files in os.walk(model_path):\n",
    "        path = root.split(os.sep)\n",
    "        for file in files:\n",
    "            absolute_filename = os.path.join(root, file)\n",
    "            model_id = re.findall(r'opennmt_(.+)_step_\\d+\\.pt', file)\n",
    "            if len(model_id) != 0:\n",
    "                available_model.add(model_id[0])\n",
    "    \n",
    "    for root, dirs, files in os.walk(model_path):\n",
    "        path = root.split(os.sep)\n",
    "        for file in files:\n",
    "            absolute_filename = os.path.join(root, file)\n",
    "            model_id = re.findall(r'opennmt_(.+).txt', file)\n",
    "            if len(model_id) != 0 and model_id[0] not in available_model:\n",
    "                useless_file.append(absolute_filename)\n",
    "    \n",
    "    dirs = os.listdir(log_path)\n",
    "    for d in dirs:\n",
    "        model_id = re.findall(r'opennmt_(.+)', d)\n",
    "        if len(model_id) != 0 and model_id[0] not in available_model:\n",
    "            useless_log.append(os.path.join(log_path, d))\n",
    "    \n",
    "#     print(available_model)\n",
    "#     print(useless_file)\n",
    "#     print(useless_log)\n",
    "    for f in useless_file:\n",
    "        os.remove(f)\n",
    "    for f in useless_log:\n",
    "        print(f)\n",
    "        !rm -rf $f\n",
    "\n",
    "# clean_path_with_no_checkpoint('/mnt/drive-2t/model', '/mnt/drive-2t/log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_human(model_id):\n",
    "    model_args = '/mnt/drive-2t/model/opennmt_' + model_id + '.txt'\n",
    "    !cat $model_args\n",
    "    pred = '/mnt/drive-2t/pred/opennmt_' + model_id + '.txt'\n",
    "    ref = 'data/nmt15/test_en'\n",
    "    print()\n",
    "    print(bleu_evaluate(pred, ref), bleu_evaluate(pred, ref, '0.25 0.25 0.25 0.25'))\n",
    "    pred_head = !head $pred\n",
    "    ref_head = !head $ref\n",
    "    pairs = list(zip(pred_head, ref_head))\n",
    "    for pair in pairs:\n",
    "        print(pair[0])\n",
    "        print()\n",
    "        print(pair[1])\n",
    "        print('-------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BleuHelper:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.python_interpreter = '/home/ray/.conda/envs/dl/bin/python'\n",
    "        self.bleu_script = '/home/ray/Smooth_BLEU/bleu.py'\n",
    "        self.nltk_script = '/home/ray/Smooth_BLEU/nltk_bleu.py'\n",
    "        self.bleu_pred = '/home/ray/OpenNMT-py/data/nmt15/pred.txt'\n",
    "        self.bleu_reference = '/home/ray/OpenNMT-py/data/nmt15/test_vi'\n",
    "        self.bleu_weight = '0.25 0.25 0.25'\n",
    "    \n",
    "    def smooth_bleu_evaluate(self, pred, reference, weight='0.25 0.25 0.25'):\n",
    "        p = subprocess.Popen([self.python_interpreter, self.bleu_script,\n",
    "                              '-t', pred,\n",
    "                              '-r', reference,\n",
    "                              '-w', weight], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        p.wait()\n",
    "        out, err = p.communicate()\n",
    "        if p.returncode != 0:\n",
    "            print(err)\n",
    "            raise Exception()\n",
    "        out = out.decode()\n",
    "        err = err.decode()\n",
    "        bleu_score = re.findall(r'BLEU = ([1-9\\.]+)', err)\n",
    "        return float(bleu_score[0])\n",
    "\n",
    "    def nltk_bleu_evaluate(self, pred, reference, weight='0.25 0.25 0.25'):\n",
    "        p = subprocess.Popen([self.python_interpreter, self.nltk_script,\n",
    "                              '-t', pred,\n",
    "                              '-r', reference,\n",
    "                              '-w', weight], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "        p.wait()\n",
    "        out, err = p.communicate()\n",
    "        if p.returncode != 0:\n",
    "            print(err)\n",
    "            raise Exception()\n",
    "        out = out.decode()\n",
    "        err = err.decode()\n",
    "        bleu_score = re.findall(r'BLEU = ([1-9\\.]+)', err)\n",
    "        return float(bleu_score[0])\n",
    "\n",
    "bleu_helper = BleuHelper()\n",
    "def bleu_evaluate(pred, reference, weight='0.25 0.25 0.25'):\n",
    "    smooth = bleu_helper.smooth_bleu_evaluate(pred, reference, weight)\n",
    "#     nltk_ = bleu_helper.nltk_bleu_evaluate(pred, reference, weight)\n",
    "#     return smooth, nltk_\n",
    "    return smooth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searchable Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# searchable hyperparameters\n",
    "class SearchableHyperparameters:\n",
    "    \n",
    "    def __init__(self):\n",
    "        embedding_size = ['64', '128', '256', '512']\n",
    "        encoder_type = ['rnn', 'brnn', 'mean', 'transformer']\n",
    "        decoder_type = encoder_type\n",
    "        layers = ['1', '2', '3', '4', '5', '6']\n",
    "        rnn_size = ['64', '128', '256', '512']\n",
    "        rnn_type = ['LSTM', 'GRU', 'SRU']\n",
    "        global_attention = ['dot', 'general', 'mlp']\n",
    "        self_attention_type = ['scaled_dot', 'average']\n",
    "        heads = [2, 4, 6, 8, 10]\n",
    "        transformer_ff = [1024, 2048, 4096]\n",
    "\n",
    "        normalization = ['sents', 'tokens']\n",
    "        valid_steps = 1000\n",
    "        train_steps = 10000\n",
    "        dropout = 0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Trainer Class\n",
    "\n",
    "Features:\n",
    "1. Early stop supported.\n",
    "2. Custom validation evaluation functions.(TODO)\n",
    "3. Custom callbacks. (TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyTrainer(Trainer):\n",
    "    def train(self,\n",
    "              train_iter,\n",
    "              train_steps,\n",
    "              save_checkpoint_steps=5000,\n",
    "              valid_iter=None,\n",
    "              valid_steps=10000,\n",
    "              early_stop_round=50000,\n",
    "              early_stop_threshold=1):\n",
    "        self.last_model_saver = None\n",
    "        self.last_step = None\n",
    "        self.last_moving_average = None\n",
    "        if valid_iter is None:\n",
    "            logger.info('Start training loop without validation...')\n",
    "        else:\n",
    "            logger.info('Start training loop and validate every %d steps...',\n",
    "                        valid_steps)\n",
    "\n",
    "        total_stats = onmt.utils.Statistics()\n",
    "        report_stats = onmt.utils.Statistics()\n",
    "        self._start_report_manager(start_time=total_stats.start_time)\n",
    "        \n",
    "        valid_early_stop_loss = []\n",
    "        last_accent_valid_acc = float('-inf')\n",
    "        decent_round = 0\n",
    "        for i, (batches, normalization) in enumerate(\n",
    "                self._accum_batches(train_iter)):\n",
    "            if i == 0:\n",
    "                print(i, 'Testing GPU memory capability with batch shape:', batches[0].src[0].shape, batches[0].tgt.shape)\n",
    "            step = self.optim.training_step\n",
    "            if decent_round == 0:  # 如果没有处于下降过程，则保存前一个model，如果当前step下降，则一直保存这个model\n",
    "                self.last_model_saver = deepcopy(self.model_saver)\n",
    "                self.last_step = step\n",
    "                self.last_moving_average = deepcopy(self.moving_average)\n",
    "\n",
    "            self._gradient_accumulation(\n",
    "                batches, normalization, total_stats,\n",
    "                report_stats)\n",
    "\n",
    "            if self.average_decay > 0 and i % self.average_every == 0:\n",
    "                self._update_average(step)\n",
    "                            \n",
    "            # 向tensorboard输出loss\n",
    "            if step % self.report_manager.report_every == 0:\n",
    "                tensorboard_writer = self.report_manager.tensorboard_writer\n",
    "                tensorboard_writer.add_scalar('train/loss', report_stats.loss, step)\n",
    "\n",
    "            report_stats = self._maybe_report_training(\n",
    "                step, train_steps,\n",
    "                self.optim.learning_rate(),\n",
    "                report_stats)\n",
    "\n",
    "            \n",
    "            if valid_iter is not None and step % valid_steps == 0:\n",
    "                print('last accent valid acc:', last_accent_valid_acc)\n",
    "                if step == valid_steps:\n",
    "                    valid_stats = self.validate(\n",
    "                        valid_iter, verbose=1, moving_average=self.moving_average)\n",
    "                else:\n",
    "                    valid_stats = self.validate(\n",
    "                        valid_iter, moving_average=self.moving_average)\n",
    "                valid_stats = self._maybe_gather_stats(valid_stats)\n",
    "                tensorboard_writer.add_scalar('valid/loss', valid_stats.loss, step)\n",
    "#                 valid_acc = valid_stats.accuracy()\n",
    "                valid_acc = -valid_stats.loss\n",
    "                \n",
    "#                 self._report_step(self.optim.learning_rate(),\n",
    "#                                   step, valid_stats=valid_stats)\n",
    "#                 if last_accent_valid_acc + early_stop_threshold > valid_acc:\n",
    "#                     print('meet early stop condition, prev best loss:', last_accent_valid_acc, 'current loss:', valid_acc)\n",
    "#                     if self.model_saver is not None:\n",
    "#                         self.model_saver.save(step, moving_average=self.moving_average)\n",
    "#                     return total_stats\n",
    "                \n",
    "#                 last_accent_valid_acc = valid_acc\n",
    "                \n",
    "                if valid_acc < last_accent_valid_acc + early_stop_threshold:  # 开始下降\n",
    "                    if decent_round == 0:  # 保存最后一个上升model\n",
    "                        print('Save last accent model with acc:', last_accent_valid_acc)\n",
    "                    decent_round += 1\n",
    "                    print('Decent round:', decent_round)\n",
    "                else:\n",
    "                    decent_round = 0\n",
    "                    last_accent_valid_acc = valid_acc\n",
    "\n",
    "                self._report_step(self.optim.learning_rate(),\n",
    "                                  step, valid_stats=valid_stats)\n",
    "                if decent_round == early_stop_round:  # 停止\n",
    "                    if self.last_model_saver is not None:\n",
    "                        print('meet early stop condition, prev best loss:', last_accent_valid_acc, 'current loss:', valid_acc)\n",
    "                        self.last_model_saver.save(self.last_step, moving_average=self.last_moving_average)\n",
    "                    return total_stats\n",
    "\n",
    "#                 if len(valid_early_stop_loss) == early_stop_round:\n",
    "#                     # 条件为当前valid loss要超过前n个valid loss最好的那个加threshold (X)\n",
    "#                     # 应该允许valid loss暂时下降，但不能持续下降超过n个valid round\n",
    "#                     # 如果持续下降，回到最好的model\n",
    "#                     if valid_loss - max(valid_early_stop_loss) < early_stop_threshold:\n",
    "#                         print('meet early stop condition, prev best loss:', max(valid_early_stop_loss), 'current loss:', valid_loss)\n",
    "#                         if self.model_saver is not None:\n",
    "#                             self.model_saver.save(step, moving_average=self.moving_average)\n",
    "#                             self._report_step(self.optim.learning_rate(),\n",
    "#                                               step, valid_stats=valid_stats)\n",
    "#                         return total_stats\n",
    "#                     else:\n",
    "#                         valid_early_stop_loss.pop(0)\n",
    "#                         valid_early_stop_loss.append(valid_loss)\n",
    "\n",
    "            if (self.model_saver is not None\n",
    "                    and (save_checkpoint_steps != 0\n",
    "                         and step % save_checkpoint_steps == 0)):\n",
    "                self.model_saver.save(step, moving_average=self.moving_average)\n",
    "\n",
    "            if train_steps > 0 and step >= train_steps:\n",
    "                break\n",
    "\n",
    "        if self.model_saver is not None:\n",
    "            self.model_saver.save(step, moving_average=self.moving_average)\n",
    "        return total_stats\n",
    "    \n",
    "    def validate(self, valid_iter, verbose=0, moving_average=None):\n",
    "        if moving_average:\n",
    "            valid_model = deepcopy(self.model)\n",
    "            for avg, param in zip(self.moving_average,\n",
    "                                  valid_model.parameters()):\n",
    "                param.data = avg.data.half() if self.model_dtype == \"fp16\" \\\n",
    "                    else avg.data\n",
    "        else:\n",
    "            valid_model = self.model\n",
    "\n",
    "        # Set model in validating mode.\n",
    "        valid_model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            stats = onmt.utils.Statistics()\n",
    "\n",
    "            for batch in valid_iter:\n",
    "#                 print('Testing GPU memory capability with batch shape:', batch.src[0].shape, batch.tgt.shape)\n",
    "                src, src_lengths = batch.src if isinstance(batch.src, tuple) \\\n",
    "                                   else (batch.src, None)\n",
    "                tgt = batch.tgt\n",
    "\n",
    "                # F-prop through the model.\n",
    "                outputs, attns = valid_model(src, tgt, src_lengths)\n",
    "\n",
    "                # Compute loss.\n",
    "                _, batch_stats = self.valid_loss(batch, outputs, attns)\n",
    "\n",
    "                # Update statistics.\n",
    "                stats.update(batch_stats)\n",
    "\n",
    "        if moving_average:\n",
    "            del valid_model\n",
    "        else:\n",
    "            # Set model back to training mode.\n",
    "            valid_model.train()\n",
    "\n",
    "        return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_trainer(opt, device_id, model, fields, optim, model_saver=None):\n",
    "    tgt_field = fields['tgt'][0][1].base_field\n",
    "    train_loss = build_loss_compute(model, tgt_field, opt)\n",
    "    valid_loss = build_loss_compute(\n",
    "        model, tgt_field, opt, train=False)\n",
    "\n",
    "    trunc_size = opt.truncated_decoder  # Badly named...\n",
    "    shard_size = opt.max_generator_batches if opt.model_dtype == 'fp32' else 0\n",
    "    norm_method = opt.normalization\n",
    "    grad_accum_count = opt.accum_count\n",
    "    n_gpu = opt.world_size\n",
    "    average_decay = opt.average_decay\n",
    "    average_every = opt.average_every\n",
    "    if device_id >= 0:\n",
    "        gpu_rank = opt.gpu_ranks[device_id]\n",
    "    else:\n",
    "        gpu_rank = 0\n",
    "        n_gpu = 0\n",
    "    gpu_verbose_level = opt.gpu_verbose_level\n",
    "\n",
    "    report_manager = onmt.utils.build_report_manager(opt)\n",
    "    trainer = MyTrainer(model, train_loss, valid_loss, optim, trunc_size,\n",
    "                        shard_size, norm_method,\n",
    "                        grad_accum_count, n_gpu, gpu_rank,\n",
    "                        gpu_verbose_level, report_manager,\n",
    "                        model_saver=model_saver if gpu_rank == 0 else None,\n",
    "                        average_decay=average_decay,\n",
    "                        average_every=average_every,\n",
    "                        model_dtype=opt.model_dtype)\n",
    "    return trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Entry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def _check_save_model_path(opt):\n",
    "    save_model_path = os.path.abspath(opt.save_model)\n",
    "    model_dirname = os.path.dirname(save_model_path)\n",
    "    if not os.path.exists(model_dirname):\n",
    "        os.makedirs(model_dirname)\n",
    "\n",
    "\n",
    "def _tally_parameters(model):\n",
    "    enc = 0\n",
    "    dec = 0\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'encoder' in name:\n",
    "            enc += param.nelement()\n",
    "        else:\n",
    "            dec += param.nelement()\n",
    "    return enc + dec, enc, dec\n",
    "\n",
    "\n",
    "def training_opt_postprocessing(opt, device_id):\n",
    "    if opt.word_vec_size != -1:\n",
    "        opt.src_word_vec_size = opt.word_vec_size\n",
    "        opt.tgt_word_vec_size = opt.word_vec_size\n",
    "\n",
    "    if opt.layers != -1:\n",
    "        opt.enc_layers = opt.layers\n",
    "        opt.dec_layers = opt.layers\n",
    "\n",
    "    if opt.rnn_size != -1:\n",
    "        opt.enc_rnn_size = opt.rnn_size\n",
    "        opt.dec_rnn_size = opt.rnn_size\n",
    "\n",
    "        # this check is here because audio allows the encoder and decoder to\n",
    "        # be different sizes, but other model types do not yet\n",
    "        same_size = opt.enc_rnn_size == opt.dec_rnn_size\n",
    "        assert opt.model_type == 'audio' or same_size, \\\n",
    "            \"The encoder and decoder rnns must be the same size for now\"\n",
    "\n",
    "    opt.brnn = opt.encoder_type == \"brnn\"\n",
    "\n",
    "    assert opt.rnn_type != \"SRU\" or opt.gpu_ranks, \\\n",
    "        \"Using SRU requires -gpu_ranks set.\"\n",
    "\n",
    "    if torch.cuda.is_available() and not opt.gpu_ranks:\n",
    "        logger.info(\"WARNING: You have a CUDA device, \\\n",
    "                    should run with -gpu_ranks\")\n",
    "\n",
    "    if device_id >= 0:\n",
    "        torch.cuda.set_device(device_id)\n",
    "    set_random_seed(opt.seed, device_id >= 0)\n",
    "\n",
    "    return opt\n",
    "\n",
    "\n",
    "def train_main(opt, device_id):\n",
    "    opt = training_opt_postprocessing(opt, device_id)\n",
    "    init_logger(opt.log_file)\n",
    "    # Load checkpoint if we resume from a previous training.\n",
    "    if opt.train_from:\n",
    "        logger.info('Loading checkpoint from %s' % opt.train_from)\n",
    "        checkpoint = torch.load(opt.train_from,\n",
    "                                map_location=lambda storage, loc: storage)\n",
    "\n",
    "        # Load default opts values then overwrite it with opts from\n",
    "        # the checkpoint. It's usefull in order to re-train a model\n",
    "        # after adding a new option (not set in checkpoint)\n",
    "        dummy_parser = configargparse.ArgumentParser()\n",
    "        opts.model_opts(dummy_parser)\n",
    "        default_opt = dummy_parser.parse_known_args([])[0]\n",
    "\n",
    "        model_opt = default_opt\n",
    "        model_opt.__dict__.update(checkpoint['opt'].__dict__)\n",
    "        logger.info('Loading vocab from checkpoint at %s.' % opt.train_from)\n",
    "        vocab = checkpoint['vocab']\n",
    "    else:\n",
    "        checkpoint = None\n",
    "        model_opt = opt\n",
    "        vocab = torch.load(opt.data + '.vocab.pt')\n",
    "\n",
    "    # check for code where vocab is saved instead of fields\n",
    "    # (in the future this will be done in a smarter way)\n",
    "    if old_style_vocab(vocab):\n",
    "        data_type = opt.model_type\n",
    "        fields = load_old_vocab(vocab, data_type, dynamic_dict=opt.copy_attn)\n",
    "    else:\n",
    "        fields = vocab\n",
    "\n",
    "    # Report src and tgt vocab sizes, including for features\n",
    "    for side in ['src', 'tgt']:\n",
    "        for name, f in fields[side]:\n",
    "            try:\n",
    "                f_iter = iter(f)\n",
    "            except TypeError:\n",
    "                f_iter = [(name, f)]\n",
    "            for sn, sf in f_iter:\n",
    "                if sf.use_vocab:\n",
    "                    logger.info(' * %s vocab size = %d' % (sn, len(sf.vocab)))\n",
    "\n",
    "    # Build model.\n",
    "    model = build_model(model_opt, opt, fields, checkpoint)\n",
    "    n_params, enc, dec = _tally_parameters(model)\n",
    "    logger.info('encoder: %d' % enc)\n",
    "    logger.info('decoder: %d' % dec)\n",
    "    logger.info('* number of parameters: %d' % n_params)\n",
    "    _check_save_model_path(opt)\n",
    "\n",
    "    # Build optimizer.\n",
    "    optim = Optimizer.from_opt(model, opt, checkpoint=checkpoint)\n",
    "\n",
    "    # Build model saver\n",
    "    model_saver = build_model_saver(model_opt, opt, model, fields, optim)\n",
    "\n",
    "    trainer = build_trainer(\n",
    "        opt, device_id, model, fields, optim, model_saver=model_saver)\n",
    "\n",
    "    # this line is kind of a temporary kludge because different objects expect\n",
    "    # fields to have a different structure\n",
    "    dataset_fields = dict(chain.from_iterable(fields.values()))\n",
    "\n",
    "    train_iter = build_dataset_iter(\"train\", dataset_fields, opt)\n",
    "    valid_iter = build_dataset_iter(\n",
    "        \"valid\", dataset_fields, opt, is_train=False)\n",
    "\n",
    "    if len(opt.gpu_ranks):\n",
    "        logger.info('Starting training on GPU: %s' % opt.gpu_ranks)\n",
    "    else:\n",
    "        logger.info('Starting training on CPU, could be very slow')\n",
    "    train_steps = opt.train_steps\n",
    "    if opt.single_pass and train_steps > 0:\n",
    "        logger.warning(\"Option single_pass is enabled, ignoring train_steps.\")\n",
    "        train_steps = 0\n",
    "    trainer.train(\n",
    "        train_iter,\n",
    "        train_steps,\n",
    "        save_checkpoint_steps=opt.save_checkpoint_steps,\n",
    "        valid_iter=valid_iter,\n",
    "        valid_steps=opt.valid_steps,\n",
    "        early_stop_round=opt.early_stop_round,\n",
    "        early_stop_threshold=opt.early_stop_threshold)\n",
    "\n",
    "    if opt.tensorboard:\n",
    "        trainer.report_manager.tensorboard_writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Home-made Runner Class\n",
    "\n",
    "Features:\n",
    "1. Automatic hyperparameters search. (TODO)\n",
    "2. Reusable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseRunner:\n",
    "    \n",
    "    def __init__(self, data_path):\n",
    "        self.data_path = data_path\n",
    "        self.model_id = None\n",
    "    \n",
    "    def run(self, option, model_id=None, train_from=None):\n",
    "        model_id = model_id if model_id is not None else BaseRunner.generate_id()\n",
    "        self.model_id = model_id\n",
    "        self.prepare_save_file()\n",
    "        arg_dict = {\n",
    "            'data': self.data_path,\n",
    "            'save_model': self.model_path,\n",
    "            'gpu_ranks': 0,\n",
    "            'tensorboard': None,\n",
    "            'tensorboard_log_dir': self.log_path + '/log'\n",
    "        }\n",
    "        arg_dict.update(option)\n",
    "        if train_from is not None:\n",
    "            arg_dict['train_from'] = train_from\n",
    "        ArgumentHelper.save_args(arg_dict, self.model_path)\n",
    "        parser = configargparse.ArgumentParser(\n",
    "            description='train.py',\n",
    "            formatter_class=configargparse.ArgumentDefaultsHelpFormatter)\n",
    "\n",
    "        opts.add_md_help_argument(parser)\n",
    "        opts.model_opts(parser)\n",
    "        opts.train_opts(parser)\n",
    "        parser.add_argument('-early_stop_round', type=int, default=10000, help='Early stop round')\n",
    "        parser.add_argument('-early_stop_threshold', type=float, default=1, help='Early stop threshold')\n",
    "\n",
    "        opt = parser.parse_args(ArgumentHelper.convert_dict_to_arg_array(arg_dict))\n",
    "        train_main(opt, 0)\n",
    "\n",
    "    def translate_main(self, opt, logger, model_id):\n",
    "        translator = build_translator(opt, report_score=True)\n",
    "        src_shards = split_corpus(opt.src, opt.shard_size)\n",
    "        tgt_shards = split_corpus(opt.tgt, opt.shard_size) \\\n",
    "            if opt.tgt is not None else [None]*opt.shard_size\n",
    "        shard_pairs = zip(src_shards, tgt_shards)\n",
    "\n",
    "        for i, (src_shard, tgt_shard) in enumerate(shard_pairs):\n",
    "            logger.info(\"Translating shard %d.\" % i)\n",
    "            translator.translate(\n",
    "                src=src_shard,\n",
    "                tgt=tgt_shard,\n",
    "                src_dir=opt.src_dir,\n",
    "                batch_size=opt.batch_size,\n",
    "                attn_debug=opt.attn_debug\n",
    "            )\n",
    "\n",
    "    def translate(self, args, model_id=None):\n",
    "        model_id = model_id if model_id is not None else self.model_id\n",
    "        if model_id is None:\n",
    "            raise Exception()\n",
    "        self.model_id = model_id\n",
    "        self.prepare_save_file()\n",
    "        args['model'] = self.model_path + '_step_' + str(BaseRunner.get_latest_model(\n",
    "                                                        self.model_dir,\n",
    "                                                        self.model_prefix,\n",
    "                                                        self.model_id)) + '.pt'\n",
    "        parser = configargparse.ArgumentParser(\n",
    "            description='translate.py',\n",
    "            config_file_parser_class=configargparse.YAMLConfigFileParser,\n",
    "            formatter_class=configargparse.ArgumentDefaultsHelpFormatter)\n",
    "        opts.config_opts(parser)\n",
    "        opts.add_md_help_argument(parser)\n",
    "        opts.translate_opts(parser)\n",
    "\n",
    "        opt = parser.parse_args(ArgumentHelper.convert_dict_to_arg_array(args))\n",
    "        logger = init_logger(opt.log_file)\n",
    "        self.translate_main(opt, logger, model_id)\n",
    "\n",
    "    @staticmethod\n",
    "    def generate_id():\n",
    "        return hashlib.md5(str(uuid.uuid4()).encode()).hexdigest()\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_latest_model(model_dir, model_prefix, model_id):\n",
    "        all_steps = []\n",
    "        for root, dirs, files in os.walk(model_dir):\n",
    "            path = root.split(os.sep)\n",
    "            for file in files:\n",
    "                absolute_filename = os.path.join(root, file)\n",
    "                if model_id in absolute_filename:\n",
    "                    steps = re.findall(model_prefix + model_id + '_step_(\\d+)\\.pt', absolute_filename)\n",
    "                    if len(steps) > 0:\n",
    "                        all_steps.append(int(steps[0]))\n",
    "        return max(all_steps)\n",
    "\n",
    "    def prepare_save_file(self):\n",
    "        model_id = self.model_id\n",
    "        model_dir = '/mnt/drive-2t/model/'\n",
    "        if not os.path.exists(model_dir):\n",
    "            os.makedirs(model_dir)\n",
    "        model_prefix = 'opennmt_'\n",
    "        model_path = model_dir + model_prefix + model_id\n",
    "        log_dir = '/mnt/drive-2t/log/'\n",
    "        if not os.path.exists(log_dir):\n",
    "            os.makedirs(log_dir)\n",
    "        log_prefix = model_prefix\n",
    "        log_path = log_dir + log_prefix + model_id\n",
    "        if not os.path.exists(log_path):\n",
    "            os.mkdir(log_path)\n",
    "        self.model_path = model_path\n",
    "        self.log_path = log_path\n",
    "        self.model_prefix = model_prefix\n",
    "        self.model_dir = model_dir\n",
    "\n",
    "\n",
    "runner = BaseRunner('data/nmt15')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_runner = BaseRunner('data/nmt15-reverse')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_allocated(0) / 1024 / 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.memory_cached(0) / 1024 / 1024 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.max_memory_cached(0) / 1024 / 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vars = {k: v for k, v in globals().items() if not k.startswith('_') and k not in ['In', 'Out']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'get_ipython': \"<class 'method'>\",\n",
       " 'exit': \"<class 'IPython.core.autocall.ZMQExitAutocall'>\",\n",
       " 'quit': \"<class 'IPython.core.autocall.ZMQExitAutocall'>\",\n",
       " 'Trainer': \"<class 'type'>\",\n",
       " 'logger': \"<class 'logging.RootLogger'>\",\n",
       " 'Optimizer': \"<class 'type'>\",\n",
       " 'chain': \"<class 'type'>\",\n",
       " 'ArgumentHelper': \"<class 'type'>\",\n",
       " 'BleuHelper': \"<class 'type'>\",\n",
       " 'bleu_helper': \"<class '__main__.BleuHelper'>\",\n",
       " 'SearchableHyperparameters': \"<class 'type'>\",\n",
       " 'MyTrainer': \"<class 'type'>\",\n",
       " 'BaseRunner': \"<class 'type'>\",\n",
       " 'runner': \"<class '__main__.BaseRunner'>\",\n",
       " 'reverse_runner': \"<class '__main__.BaseRunner'>\"}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "var_type = {k: str(type(v)) for k, v in all_vars.items() if 'function' not in str(type(v)) and\n",
    "                                                            'module' not in str(type(v))}\n",
    "var_type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "del reverse_runner\n",
    "gc.collect()\n",
    "reverse_runner = BaseRunner('data/nmt15-reverse')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example #0: Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%bash\n",
    "# python preprocess.py \\\n",
    "#      -train_src data/nmt15/train_vi \\\n",
    "#      -train_tgt data/nmt15/train_en \\\n",
    "#      -valid_src data/nmt15/valid_vi \\\n",
    "#      -valid_tgt data/nmt15/valid_en \\\n",
    "#      -save_data data/nmt15-reverse \\\n",
    "#      --src_seq_length 200 \\\n",
    "#      --tgt_seq_length 200 \\\n",
    "#      --src_words_min_frequency 3 \\\n",
    "#      --tgt_words_min_frequency 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example #1: Train A Basic LSTM Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-16 19:26:39,631 INFO]  * src vocab size = 8870\n",
      "[2019-02-16 19:26:39,632 INFO]  * tgt vocab size = 20071\n",
      "[2019-02-16 19:26:39,633 INFO] Building model...\n",
      "[2019-02-16 19:26:43,167 INFO] NMTModel(\n",
      "  (encoder): RNNEncoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(8870, 500, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (rnn): GRU(500, 250, num_layers=2, dropout=0.3, bidirectional=True)\n",
      "  )\n",
      "  (decoder): InputFeedRNNDecoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(20071, 500, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.3)\n",
      "    (rnn): StackedGRU(\n",
      "      (dropout): Dropout(p=0.3)\n",
      "      (layers): ModuleList(\n",
      "        (0): GRUCell(1000, 500)\n",
      "        (1): GRUCell(500, 500)\n",
      "      )\n",
      "    )\n",
      "    (attn): GlobalAttention(\n",
      "      (linear_in): Linear(in_features=500, out_features=500, bias=False)\n",
      "      (linear_out): Linear(in_features=1000, out_features=500, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (generator): Sequential(\n",
      "    (0): Linear(in_features=500, out_features=20071, bias=True)\n",
      "    (1): Cast()\n",
      "    (2): LogSoftmax()\n",
      "  )\n",
      ")\n",
      "[2019-02-16 19:26:43,170 INFO] encoder: 6691000\n",
      "[2019-02-16 19:26:43,170 INFO] decoder: 24597071\n",
      "[2019-02-16 19:26:43,171 INFO] * number of parameters: 31288071\n",
      "[2019-02-16 19:26:43,300 INFO] Starting training on GPU: [0]\n",
      "[2019-02-16 19:26:43,300 INFO] Start training loop and validate every 500 steps...\n",
      "[2019-02-16 19:26:44,816 INFO] Loading dataset from data/nmt15-reverse.train.0.pt, number of examples: 95844\n",
      "[2019-02-16 19:27:10,643 INFO] Step 100/20000; acc:   8.01; ppl: 836.81; xent: 6.73; lr: 0.00100; 5730/4884 tok/s;     27 sec\n",
      "[2019-02-16 19:27:36,635 INFO] Step 200/20000; acc:  18.37; ppl: 300.49; xent: 5.71; lr: 0.00100; 6074/5181 tok/s;     53 sec\n",
      "[2019-02-16 19:28:03,130 INFO] Step 300/20000; acc:  24.21; ppl: 178.19; xent: 5.18; lr: 0.00100; 6034/5130 tok/s;     80 sec\n",
      "[2019-02-16 19:28:30,257 INFO] Step 400/20000; acc:  27.07; ppl: 133.80; xent: 4.90; lr: 0.00100; 5874/5001 tok/s;    107 sec\n",
      "[2019-02-16 19:28:57,538 INFO] Step 500/20000; acc:  30.67; ppl: 101.22; xent: 4.62; lr: 0.00100; 5784/4919 tok/s;    134 sec\n",
      "[2019-02-16 19:28:57,643 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last accent valid acc: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-16 19:29:13,092 INFO] Validation perplexity: 75.9304\n",
      "[2019-02-16 19:29:13,093 INFO] Validation accuracy: 32.9474\n",
      "[2019-02-16 19:29:42,160 INFO] Step 600/20000; acc:  31.85; ppl: 88.06; xent: 4.48; lr: 0.00100; 3590/3064 tok/s;    179 sec\n",
      "[2019-02-16 19:30:11,635 INFO] Step 700/20000; acc:  32.60; ppl: 79.42; xent: 4.37; lr: 0.00100; 5425/4624 tok/s;    208 sec\n",
      "[2019-02-16 19:30:41,019 INFO] Step 800/20000; acc:  34.85; ppl: 65.90; xent: 4.19; lr: 0.00100; 5312/4527 tok/s;    238 sec\n",
      "[2019-02-16 19:31:11,013 INFO] Step 900/20000; acc:  35.54; ppl: 60.60; xent: 4.10; lr: 0.00100; 5210/4449 tok/s;    268 sec\n",
      "[2019-02-16 19:31:41,459 INFO] Step 1000/20000; acc:  35.98; ppl: 57.13; xent: 4.05; lr: 0.00100; 5233/4462 tok/s;    298 sec\n",
      "[2019-02-16 19:31:41,569 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last accent valid acc: 32.94736280636874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-16 19:31:57,461 INFO] Validation perplexity: 42.9186\n",
      "[2019-02-16 19:31:57,462 INFO] Validation accuracy: 38.7609\n",
      "[2019-02-16 19:32:28,021 INFO] Step 1100/20000; acc:  36.74; ppl: 53.38; xent: 3.98; lr: 0.00100; 3430/2899 tok/s;    345 sec\n",
      "[2019-02-16 19:32:59,141 INFO] Step 1200/20000; acc:  37.88; ppl: 48.85; xent: 3.89; lr: 0.00100; 5121/4354 tok/s;    376 sec\n",
      "[2019-02-16 19:33:30,448 INFO] Step 1300/20000; acc:  38.31; ppl: 45.87; xent: 3.83; lr: 0.00100; 5035/4287 tok/s;    407 sec\n",
      "[2019-02-16 19:34:01,939 INFO] Step 1400/20000; acc:  38.50; ppl: 44.79; xent: 3.80; lr: 0.00100; 4983/4254 tok/s;    439 sec\n",
      "[2019-02-16 19:34:34,977 INFO] Loading dataset from data/nmt15-reverse.train.0.pt, number of examples: 95844\n",
      "[2019-02-16 19:34:35,735 INFO] Step 1500/20000; acc:  39.53; ppl: 40.56; xent: 3.70; lr: 0.00100; 4622/3948 tok/s;    472 sec\n",
      "[2019-02-16 19:34:35,833 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last accent valid acc: 38.76092348449154\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-16 19:34:51,810 INFO] Validation perplexity: 35.5603\n",
      "[2019-02-16 19:34:51,811 INFO] Validation accuracy: 40.1882\n",
      "[2019-02-16 19:35:24,221 INFO] Step 1600/20000; acc:  39.47; ppl: 39.27; xent: 3.67; lr: 0.00100; 3281/2794 tok/s;    521 sec\n",
      "[2019-02-16 19:35:56,277 INFO] Step 1700/20000; acc:  40.03; ppl: 36.84; xent: 3.61; lr: 0.00100; 4873/4159 tok/s;    553 sec\n",
      "[2019-02-16 19:36:28,540 INFO] Step 1800/20000; acc:  40.35; ppl: 35.74; xent: 3.58; lr: 0.00100; 4921/4195 tok/s;    585 sec\n",
      "[2019-02-16 19:37:00,908 INFO] Step 1900/20000; acc:  40.62; ppl: 33.79; xent: 3.52; lr: 0.00100; 4932/4186 tok/s;    618 sec\n",
      "[2019-02-16 19:37:33,105 INFO] Step 2000/20000; acc:  41.61; ppl: 30.66; xent: 3.42; lr: 0.00100; 4935/4206 tok/s;    650 sec\n",
      "[2019-02-16 19:37:33,206 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last accent valid acc: 40.18819952527848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-16 19:37:49,352 INFO] Validation perplexity: 27.8196\n",
      "[2019-02-16 19:37:49,353 INFO] Validation accuracy: 42.5583\n",
      "[2019-02-16 19:38:21,894 INFO] Step 2100/20000; acc:  41.08; ppl: 31.50; xent: 3.45; lr: 0.00100; 3265/2781 tok/s;    699 sec\n",
      "[2019-02-16 19:38:54,809 INFO] Step 2200/20000; acc:  41.33; ppl: 30.37; xent: 3.41; lr: 0.00100; 5003/4254 tok/s;    732 sec\n",
      "[2019-02-16 19:39:26,821 INFO] Step 2300/20000; acc:  42.86; ppl: 26.85; xent: 3.29; lr: 0.00100; 4729/4042 tok/s;    764 sec\n",
      "[2019-02-16 19:39:59,215 INFO] Step 2400/20000; acc:  42.43; ppl: 27.51; xent: 3.31; lr: 0.00100; 4872/4153 tok/s;    796 sec\n",
      "[2019-02-16 19:40:31,632 INFO] Step 2500/20000; acc:  42.53; ppl: 26.94; xent: 3.29; lr: 0.00100; 4856/4147 tok/s;    828 sec\n",
      "[2019-02-16 19:40:31,736 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last accent valid acc: 42.55825117569142\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-16 19:40:47,906 INFO] Validation perplexity: 24.2594\n",
      "[2019-02-16 19:40:47,907 INFO] Validation accuracy: 44.3825\n",
      "[2019-02-16 19:41:20,243 INFO] Step 2600/20000; acc:  42.58; ppl: 26.75; xent: 3.29; lr: 0.00100; 3291/2779 tok/s;    877 sec\n",
      "[2019-02-16 19:41:52,519 INFO] Step 2700/20000; acc:  42.85; ppl: 26.17; xent: 3.26; lr: 0.00100; 4923/4189 tok/s;    909 sec\n",
      "[2019-02-16 19:42:24,653 INFO] Step 2800/20000; acc:  43.14; ppl: 25.44; xent: 3.24; lr: 0.00100; 4968/4234 tok/s;    941 sec\n",
      "[2019-02-16 19:42:57,047 INFO] Step 2900/20000; acc:  42.78; ppl: 26.04; xent: 3.26; lr: 0.00100; 4921/4181 tok/s;    974 sec\n",
      "[2019-02-16 19:43:29,383 INFO] Loading dataset from data/nmt15-reverse.train.0.pt, number of examples: 95844\n",
      "[2019-02-16 19:43:30,719 INFO] Step 3000/20000; acc:  43.80; ppl: 23.84; xent: 3.17; lr: 0.00100; 4489/3853 tok/s;   1007 sec\n",
      "[2019-02-16 19:43:30,825 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last accent valid acc: 44.38246197338359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-16 19:43:46,838 INFO] Validation perplexity: 23.3733\n",
      "[2019-02-16 19:43:46,839 INFO] Validation accuracy: 44.1958\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save last accent model with acc: 44.38246197338359\n",
      "Decent round: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-16 19:43:57,297 INFO] Step 3100/20000; acc:  43.29; ppl: 24.43; xent: 3.20; lr: 0.00100; 6015/5117 tok/s;   1034 sec\n",
      "[2019-02-16 19:44:07,500 INFO] Step 3200/20000; acc:  44.24; ppl: 22.65; xent: 3.12; lr: 0.00100; 15308/13106 tok/s;   1044 sec\n",
      "[2019-02-16 19:44:17,771 INFO] Step 3300/20000; acc:  44.39; ppl: 22.42; xent: 3.11; lr: 0.00100; 15447/13149 tok/s;   1054 sec\n",
      "[2019-02-16 19:44:28,229 INFO] Step 3400/20000; acc:  43.80; ppl: 23.15; xent: 3.14; lr: 0.00100; 15351/13012 tok/s;   1065 sec\n",
      "[2019-02-16 19:44:38,503 INFO] Step 3500/20000; acc:  44.46; ppl: 21.53; xent: 3.07; lr: 0.00100; 15546/13248 tok/s;   1075 sec\n",
      "[2019-02-16 19:44:38,602 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last accent valid acc: 44.38246197338359\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-16 19:44:54,706 INFO] Validation perplexity: 21.3142\n",
      "[2019-02-16 19:44:54,706 INFO] Validation accuracy: 45.4404\n",
      "[2019-02-16 19:45:27,174 INFO] Step 3600/20000; acc:  44.33; ppl: 21.77; xent: 3.08; lr: 0.00100; 3268/2782 tok/s;   1124 sec\n",
      "[2019-02-16 19:45:59,961 INFO] Step 3700/20000; acc:  43.94; ppl: 22.30; xent: 3.10; lr: 0.00100; 5043/4294 tok/s;   1157 sec\n",
      "[2019-02-16 19:46:31,851 INFO] Step 3800/20000; acc:  45.29; ppl: 20.00; xent: 3.00; lr: 0.00100; 4734/4038 tok/s;   1189 sec\n",
      "[2019-02-16 19:47:04,186 INFO] Step 3900/20000; acc:  44.96; ppl: 20.51; xent: 3.02; lr: 0.00100; 4819/4113 tok/s;   1221 sec\n",
      "[2019-02-16 19:47:36,747 INFO] Step 4000/20000; acc:  44.64; ppl: 20.66; xent: 3.03; lr: 0.00100; 4888/4170 tok/s;   1253 sec\n",
      "[2019-02-16 19:47:36,853 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last accent valid acc: 45.44036199738636\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-16 19:47:52,896 INFO] Validation perplexity: 20.4828\n",
      "[2019-02-16 19:47:52,897 INFO] Validation accuracy: 46.056\n",
      "[2019-02-16 19:48:25,243 INFO] Step 4100/20000; acc:  44.97; ppl: 20.37; xent: 3.01; lr: 0.00100; 3306/2791 tok/s;   1302 sec\n",
      "[2019-02-16 19:48:57,435 INFO] Step 4200/20000; acc:  44.82; ppl: 20.37; xent: 3.01; lr: 0.00100; 4901/4177 tok/s;   1334 sec\n",
      "[2019-02-16 19:49:31,888 INFO] Step 4300/20000; acc:  45.10; ppl: 19.87; xent: 2.99; lr: 0.00100; 4628/3942 tok/s;   1369 sec\n",
      "[2019-02-16 19:50:05,929 INFO] Step 4400/20000; acc:  45.20; ppl: 20.21; xent: 3.01; lr: 0.00100; 4694/3984 tok/s;   1403 sec\n",
      "[2019-02-16 19:50:39,843 INFO] Loading dataset from data/nmt15-reverse.train.0.pt, number of examples: 95844\n",
      "[2019-02-16 19:50:41,860 INFO] Step 4500/20000; acc:  46.30; ppl: 18.51; xent: 2.92; lr: 0.00100; 4212/3627 tok/s;   1439 sec\n",
      "[2019-02-16 19:50:41,960 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last accent valid acc: 46.05598869202663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-16 19:50:58,357 INFO] Validation perplexity: 20.2581\n",
      "[2019-02-16 19:50:58,358 INFO] Validation accuracy: 46.0093\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save last accent model with acc: 46.05598869202663\n",
      "Decent round: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-16 19:51:09,444 INFO] Step 4600/20000; acc:  45.16; ppl: 19.73; xent: 2.98; lr: 0.00100; 5952/5044 tok/s;   1466 sec\n",
      "[2019-02-16 19:51:19,774 INFO] Step 4700/20000; acc:  46.45; ppl: 17.76; xent: 2.88; lr: 0.00100; 14648/12564 tok/s;   1476 sec\n",
      "[2019-02-16 19:51:30,803 INFO] Step 4800/20000; acc:  46.08; ppl: 18.27; xent: 2.91; lr: 0.00100; 14344/12228 tok/s;   1488 sec\n",
      "[2019-02-16 19:51:41,655 INFO] Step 4900/20000; acc:  45.56; ppl: 19.12; xent: 2.95; lr: 0.00100; 14802/12540 tok/s;   1498 sec\n",
      "[2019-02-16 19:51:52,228 INFO] Step 5000/20000; acc:  46.23; ppl: 17.71; xent: 2.87; lr: 0.00100; 15149/12931 tok/s;   1509 sec\n",
      "[2019-02-16 19:51:52,326 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last accent valid acc: 46.05598869202663\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-16 19:52:09,675 INFO] Validation perplexity: 19.466\n",
      "[2019-02-16 19:52:09,676 INFO] Validation accuracy: 46.3827\n",
      "[2019-02-16 19:52:09,677 INFO] Saving checkpoint /mnt/drive-2t/model/opennmt_182b910f202193cc4dad1cd43d74b3ba_step_5000.pt\n",
      "[2019-02-16 19:52:45,211 INFO] Step 5100/20000; acc:  45.78; ppl: 18.62; xent: 2.92; lr: 0.00100; 3010/2557 tok/s;   1562 sec\n",
      "[2019-02-16 19:53:18,194 INFO] Step 5200/20000; acc:  45.60; ppl: 18.66; xent: 2.93; lr: 0.00100; 4988/4254 tok/s;   1595 sec\n",
      "[2019-02-16 19:53:50,068 INFO] Step 5300/20000; acc:  46.69; ppl: 17.04; xent: 2.84; lr: 0.00100; 4778/4071 tok/s;   1627 sec\n",
      "[2019-02-16 19:54:22,319 INFO] Step 5400/20000; acc:  46.05; ppl: 17.96; xent: 2.89; lr: 0.00100; 4774/4081 tok/s;   1659 sec\n",
      "[2019-02-16 19:54:56,007 INFO] Step 5500/20000; acc:  46.05; ppl: 17.86; xent: 2.88; lr: 0.00100; 4762/4060 tok/s;   1693 sec\n",
      "[2019-02-16 19:54:56,109 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last accent valid acc: 46.38269311120396\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-16 19:55:12,230 INFO] Validation perplexity: 19.6073\n",
      "[2019-02-16 19:55:12,231 INFO] Validation accuracy: 46.4263\n",
      "[2019-02-16 19:55:44,551 INFO] Step 5600/20000; acc:  46.27; ppl: 17.80; xent: 2.88; lr: 0.00100; 3324/2802 tok/s;   1741 sec\n",
      "[2019-02-16 19:56:16,982 INFO] Step 5700/20000; acc:  46.02; ppl: 17.82; xent: 2.88; lr: 0.00100; 4849/4138 tok/s;   1774 sec\n",
      "[2019-02-16 19:56:49,088 INFO] Step 5800/20000; acc:  46.46; ppl: 17.10; xent: 2.84; lr: 0.00100; 4948/4207 tok/s;   1806 sec\n",
      "[2019-02-16 19:57:21,669 INFO] Step 5900/20000; acc:  46.08; ppl: 17.82; xent: 2.88; lr: 0.00100; 4933/4191 tok/s;   1838 sec\n",
      "[2019-02-16 19:57:52,727 INFO] Loading dataset from data/nmt15-reverse.train.0.pt, number of examples: 95844\n",
      "[2019-02-16 19:57:55,415 INFO] Step 6000/20000; acc:  47.16; ppl: 16.41; xent: 2.80; lr: 0.00100; 4502/3876 tok/s;   1872 sec\n",
      "[2019-02-16 19:57:55,515 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last accent valid acc: 46.42625370042761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-16 19:58:11,547 INFO] Validation perplexity: 19.7417\n",
      "[2019-02-16 19:58:11,548 INFO] Validation accuracy: 46.0991\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save last accent model with acc: 46.42625370042761\n",
      "Decent round: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-16 19:58:22,277 INFO] Step 6100/20000; acc:  46.09; ppl: 17.95; xent: 2.89; lr: 0.00100; 6142/5197 tok/s;   1899 sec\n",
      "[2019-02-16 19:58:32,096 INFO] Step 6200/20000; acc:  47.79; ppl: 15.46; xent: 2.74; lr: 0.00100; 15250/13108 tok/s;   1909 sec\n",
      "[2019-02-16 19:58:42,382 INFO] Step 6300/20000; acc:  47.25; ppl: 16.27; xent: 2.79; lr: 0.00100; 15329/13077 tok/s;   1919 sec\n",
      "[2019-02-16 19:58:52,780 INFO] Step 6400/20000; acc:  46.33; ppl: 17.28; xent: 2.85; lr: 0.00100; 15393/13036 tok/s;   1929 sec\n",
      "[2019-02-16 19:59:03,129 INFO] Step 6500/20000; acc:  47.24; ppl: 16.29; xent: 2.79; lr: 0.00100; 15577/13304 tok/s;   1940 sec\n",
      "[2019-02-16 19:59:03,228 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last accent valid acc: 46.42625370042761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-16 19:59:19,387 INFO] Validation perplexity: 18.9723\n",
      "[2019-02-16 19:59:19,387 INFO] Validation accuracy: 46.6885\n",
      "[2019-02-16 19:59:51,847 INFO] Step 6600/20000; acc:  46.65; ppl: 16.79; xent: 2.82; lr: 0.00100; 3288/2788 tok/s;   1989 sec\n",
      "[2019-02-16 20:00:24,680 INFO] Step 6700/20000; acc:  46.33; ppl: 17.49; xent: 2.86; lr: 0.00100; 5005/4270 tok/s;   2021 sec\n",
      "[2019-02-16 20:00:56,631 INFO] Step 6800/20000; acc:  47.79; ppl: 15.31; xent: 2.73; lr: 0.00100; 4755/4059 tok/s;   2053 sec\n",
      "[2019-02-16 20:01:28,653 INFO] Step 6900/20000; acc:  47.42; ppl: 16.08; xent: 2.78; lr: 0.00100; 4722/4037 tok/s;   2085 sec\n",
      "[2019-02-16 20:02:01,100 INFO] Step 7000/20000; acc:  46.83; ppl: 16.55; xent: 2.81; lr: 0.00100; 4981/4251 tok/s;   2118 sec\n",
      "[2019-02-16 20:02:01,208 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last accent valid acc: 46.68850622738628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-16 20:02:17,315 INFO] Validation perplexity: 18.4581\n",
      "[2019-02-16 20:02:17,316 INFO] Validation accuracy: 47.0503\n",
      "[2019-02-16 20:02:49,632 INFO] Step 7100/20000; acc:  46.99; ppl: 16.38; xent: 2.80; lr: 0.00100; 3368/2831 tok/s;   2166 sec\n",
      "[2019-02-16 20:03:21,835 INFO] Step 7200/20000; acc:  47.18; ppl: 16.40; xent: 2.80; lr: 0.00100; 4873/4163 tok/s;   2199 sec\n",
      "[2019-02-16 20:03:53,980 INFO] Step 7300/20000; acc:  47.07; ppl: 16.25; xent: 2.79; lr: 0.00100; 4893/4158 tok/s;   2231 sec\n",
      "[2019-02-16 20:04:26,426 INFO] Step 7400/20000; acc:  46.79; ppl: 16.77; xent: 2.82; lr: 0.00100; 4937/4199 tok/s;   2263 sec\n",
      "[2019-02-16 20:04:56,971 INFO] Loading dataset from data/nmt15-reverse.train.0.pt, number of examples: 95844\n",
      "[2019-02-16 20:05:00,283 INFO] Step 7500/20000; acc:  47.67; ppl: 15.52; xent: 2.74; lr: 0.00100; 4502/3871 tok/s;   2297 sec\n",
      "[2019-02-16 20:05:00,392 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last accent valid acc: 47.05032581542756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-16 20:05:16,329 INFO] Validation perplexity: 18.8753\n",
      "[2019-02-16 20:05:16,330 INFO] Validation accuracy: 46.8627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save last accent model with acc: 47.05032581542756\n",
      "Decent round: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-16 20:05:27,080 INFO] Step 7600/20000; acc:  47.01; ppl: 16.61; xent: 2.81; lr: 0.00100; 6141/5202 tok/s;   2324 sec\n",
      "[2019-02-16 20:05:36,924 INFO] Step 7700/20000; acc:  48.47; ppl: 14.51; xent: 2.67; lr: 0.00100; 15240/13088 tok/s;   2334 sec\n",
      "[2019-02-16 20:05:47,331 INFO] Step 7800/20000; acc:  47.50; ppl: 15.99; xent: 2.77; lr: 0.00100; 15378/13131 tok/s;   2344 sec\n",
      "[2019-02-16 20:05:57,594 INFO] Step 7900/20000; acc:  47.31; ppl: 16.09; xent: 2.78; lr: 0.00100; 15477/13086 tok/s;   2354 sec\n",
      "[2019-02-16 20:06:07,830 INFO] Step 8000/20000; acc:  47.79; ppl: 15.35; xent: 2.73; lr: 0.00100; 15611/13345 tok/s;   2365 sec\n",
      "[2019-02-16 20:06:07,942 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last accent valid acc: 47.05032581542756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-16 20:06:24,022 INFO] Validation perplexity: 18.673\n",
      "[2019-02-16 20:06:24,023 INFO] Validation accuracy: 47.1299\n",
      "[2019-02-16 20:06:56,672 INFO] Step 8100/20000; acc:  47.16; ppl: 16.14; xent: 2.78; lr: 0.00100; 3349/2835 tok/s;   2413 sec\n",
      "[2019-02-16 20:07:29,401 INFO] Step 8200/20000; acc:  47.22; ppl: 16.23; xent: 2.79; lr: 0.00100; 4927/4220 tok/s;   2446 sec\n",
      "[2019-02-16 20:08:01,244 INFO] Step 8300/20000; acc:  48.72; ppl: 14.35; xent: 2.66; lr: 0.00100; 4801/4086 tok/s;   2478 sec\n",
      "[2019-02-16 20:08:33,293 INFO] Step 8400/20000; acc:  48.01; ppl: 15.14; xent: 2.72; lr: 0.00100; 4692/4017 tok/s;   2510 sec\n",
      "[2019-02-16 20:09:06,415 INFO] Step 8500/20000; acc:  46.62; ppl: 16.76; xent: 2.82; lr: 0.00100; 5057/4295 tok/s;   2543 sec\n",
      "[2019-02-16 20:09:06,519 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last accent valid acc: 47.12989056513197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-16 20:09:22,591 INFO] Validation perplexity: 18.828\n",
      "[2019-02-16 20:09:22,592 INFO] Validation accuracy: 47.1517\n",
      "[2019-02-16 20:09:54,320 INFO] Step 8600/20000; acc:  47.83; ppl: 15.09; xent: 2.71; lr: 0.00100; 3277/2768 tok/s;   2591 sec\n",
      "[2019-02-16 20:10:26,588 INFO] Step 8700/20000; acc:  47.60; ppl: 15.63; xent: 2.75; lr: 0.00100; 4894/4174 tok/s;   2623 sec\n",
      "[2019-02-16 20:10:58,481 INFO] Step 8800/20000; acc:  47.98; ppl: 15.09; xent: 2.71; lr: 0.00100; 4895/4164 tok/s;   2655 sec\n",
      "[2019-02-16 20:11:31,214 INFO] Step 8900/20000; acc:  46.97; ppl: 16.44; xent: 2.80; lr: 0.00100; 5043/4275 tok/s;   2688 sec\n",
      "[2019-02-16 20:12:00,867 INFO] Loading dataset from data/nmt15-reverse.train.0.pt, number of examples: 95844\n",
      "[2019-02-16 20:12:04,867 INFO] Step 9000/20000; acc:  48.87; ppl: 14.19; xent: 2.65; lr: 0.00100; 4485/3863 tok/s;   2722 sec\n",
      "[2019-02-16 20:12:04,969 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last accent valid acc: 47.151670859743795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-16 20:12:20,897 INFO] Validation perplexity: 18.7054\n",
      "[2019-02-16 20:12:20,898 INFO] Validation accuracy: 47.1134\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save last accent model with acc: 47.151670859743795\n",
      "Decent round: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-16 20:12:31,720 INFO] Step 9100/20000; acc:  47.56; ppl: 15.50; xent: 2.74; lr: 0.00100; 6162/5218 tok/s;   2748 sec\n",
      "[2019-02-16 20:12:41,529 INFO] Step 9200/20000; acc:  49.03; ppl: 13.73; xent: 2.62; lr: 0.00100; 15130/13023 tok/s;   2758 sec\n",
      "[2019-02-16 20:12:51,830 INFO] Step 9300/20000; acc:  48.38; ppl: 14.83; xent: 2.70; lr: 0.00100; 15374/13150 tok/s;   2769 sec\n",
      "[2019-02-16 20:13:02,122 INFO] Step 9400/20000; acc:  47.80; ppl: 15.51; xent: 2.74; lr: 0.00100; 15452/13065 tok/s;   2779 sec\n",
      "[2019-02-16 20:13:12,427 INFO] Step 9500/20000; acc:  48.39; ppl: 14.54; xent: 2.68; lr: 0.00100; 15636/13334 tok/s;   2789 sec\n",
      "[2019-02-16 20:13:12,522 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last accent valid acc: 47.151670859743795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-16 20:13:28,758 INFO] Validation perplexity: 19.1499\n",
      "[2019-02-16 20:13:28,759 INFO] Validation accuracy: 47.1992\n",
      "[2019-02-16 20:14:01,706 INFO] Step 9600/20000; acc:  47.75; ppl: 15.33; xent: 2.73; lr: 0.00100; 3295/2794 tok/s;   2838 sec\n",
      "[2019-02-16 20:14:34,590 INFO] Step 9700/20000; acc:  47.64; ppl: 15.57; xent: 2.75; lr: 0.00100; 4892/4179 tok/s;   2871 sec\n",
      "[2019-02-16 20:15:06,548 INFO] Step 9800/20000; acc:  49.31; ppl: 13.65; xent: 2.61; lr: 0.00100; 4757/4060 tok/s;   2903 sec\n",
      "[2019-02-16 20:15:38,612 INFO] Step 9900/20000; acc:  48.44; ppl: 14.56; xent: 2.68; lr: 0.00100; 4730/4039 tok/s;   2935 sec\n",
      "[2019-02-16 20:16:11,982 INFO] Step 10000/20000; acc:  47.45; ppl: 15.94; xent: 2.77; lr: 0.00100; 5033/4280 tok/s;   2969 sec\n",
      "[2019-02-16 20:16:12,086 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last accent valid acc: 47.199231911243075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-16 20:16:28,177 INFO] Validation perplexity: 18.7275\n",
      "[2019-02-16 20:16:28,178 INFO] Validation accuracy: 47.3072\n",
      "[2019-02-16 20:16:28,179 INFO] Saving checkpoint /mnt/drive-2t/model/opennmt_182b910f202193cc4dad1cd43d74b3ba_step_10000.pt\n",
      "[2019-02-16 20:17:02,166 INFO] Step 10100/20000; acc:  48.27; ppl: 14.52; xent: 2.68; lr: 0.00100; 3114/2630 tok/s;   3019 sec\n",
      "[2019-02-16 20:17:34,283 INFO] Step 10200/20000; acc:  48.01; ppl: 14.99; xent: 2.71; lr: 0.00100; 4887/4174 tok/s;   3051 sec\n",
      "[2019-02-16 20:18:06,535 INFO] Step 10300/20000; acc:  47.87; ppl: 14.98; xent: 2.71; lr: 0.00100; 4978/4215 tok/s;   3083 sec\n",
      "[2019-02-16 20:18:39,101 INFO] Step 10400/20000; acc:  47.67; ppl: 15.57; xent: 2.75; lr: 0.00100; 4998/4248 tok/s;   3116 sec\n",
      "[2019-02-16 20:19:08,145 INFO] Loading dataset from data/nmt15-reverse.train.0.pt, number of examples: 95844\n",
      "[2019-02-16 20:19:12,693 INFO] Step 10500/20000; acc:  49.03; ppl: 13.89; xent: 2.63; lr: 0.00100; 4426/3815 tok/s;   3149 sec\n",
      "[2019-02-16 20:19:12,792 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last accent valid acc: 47.30724439268538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-16 20:19:28,738 INFO] Validation perplexity: 19.0263\n",
      "[2019-02-16 20:19:28,739 INFO] Validation accuracy: 47.2206\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save last accent model with acc: 47.30724439268538\n",
      "Decent round: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-16 20:19:39,490 INFO] Step 10600/20000; acc:  47.80; ppl: 15.32; xent: 2.73; lr: 0.00100; 6199/5246 tok/s;   3176 sec\n",
      "[2019-02-16 20:19:49,328 INFO] Step 10700/20000; acc:  49.44; ppl: 13.35; xent: 2.59; lr: 0.00100; 15081/12990 tok/s;   3186 sec\n",
      "[2019-02-16 20:19:59,815 INFO] Step 10800/20000; acc:  48.59; ppl: 14.44; xent: 2.67; lr: 0.00100; 15390/13132 tok/s;   3197 sec\n",
      "[2019-02-16 20:20:10,096 INFO] Step 10900/20000; acc:  48.33; ppl: 14.61; xent: 2.68; lr: 0.00100; 15359/13012 tok/s;   3207 sec\n",
      "[2019-02-16 20:20:20,346 INFO] Step 11000/20000; acc:  48.53; ppl: 14.25; xent: 2.66; lr: 0.00100; 15591/13301 tok/s;   3217 sec\n",
      "[2019-02-16 20:20:20,445 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last accent valid acc: 47.30724439268538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-16 20:20:36,544 INFO] Validation perplexity: 19.2336\n",
      "[2019-02-16 20:20:36,545 INFO] Validation accuracy: 47.189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decent round: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-16 20:20:47,418 INFO] Step 11100/20000; acc:  47.69; ppl: 15.37; xent: 2.73; lr: 0.00100; 6082/5144 tok/s;   3244 sec\n",
      "[2019-02-16 20:20:58,134 INFO] Step 11200/20000; acc:  47.92; ppl: 14.96; xent: 2.71; lr: 0.00100; 14992/12818 tok/s;   3255 sec\n",
      "[2019-02-16 20:21:08,759 INFO] Step 11300/20000; acc:  48.55; ppl: 14.43; xent: 2.67; lr: 0.00100; 14724/12523 tok/s;   3265 sec\n",
      "[2019-02-16 20:21:18,249 INFO] Step 11400/20000; acc:  49.26; ppl: 13.36; xent: 2.59; lr: 0.00100; 15398/13223 tok/s;   3275 sec\n",
      "[2019-02-16 20:21:29,847 INFO] Step 11500/20000; acc:  46.98; ppl: 16.12; xent: 2.78; lr: 0.00100; 14660/12447 tok/s;   3287 sec\n",
      "[2019-02-16 20:21:29,947 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last accent valid acc: 47.30724439268538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-16 20:21:46,172 INFO] Validation perplexity: 19.1337\n",
      "[2019-02-16 20:21:46,173 INFO] Validation accuracy: 47.2335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decent round: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-16 20:21:55,936 INFO] Step 11600/20000; acc:  48.88; ppl: 13.97; xent: 2.64; lr: 0.00100; 5888/4974 tok/s;   3313 sec\n",
      "[2019-02-16 20:22:06,083 INFO] Step 11700/20000; acc:  48.25; ppl: 14.82; xent: 2.70; lr: 0.00100; 15423/13169 tok/s;   3323 sec\n",
      "[2019-02-16 20:22:16,364 INFO] Step 11800/20000; acc:  48.01; ppl: 14.87; xent: 2.70; lr: 0.00100; 15561/13198 tok/s;   3333 sec\n",
      "[2019-02-16 20:22:27,079 INFO] Step 11900/20000; acc:  47.59; ppl: 15.76; xent: 2.76; lr: 0.00100; 15451/13096 tok/s;   3344 sec\n",
      "[2019-02-16 20:22:36,999 INFO] Loading dataset from data/nmt15-reverse.train.0.pt, number of examples: 95844\n",
      "[2019-02-16 20:22:38,782 INFO] Step 12000/20000; acc:  49.11; ppl: 13.88; xent: 2.63; lr: 0.00100; 12870/11077 tok/s;   3355 sec\n",
      "[2019-02-16 20:22:38,896 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last accent valid acc: 47.30724439268538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-16 20:22:54,767 INFO] Validation perplexity: 19.2466\n",
      "[2019-02-16 20:22:54,768 INFO] Validation accuracy: 46.9552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decent round: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-16 20:23:05,842 INFO] Step 12100/20000; acc:  47.35; ppl: 15.85; xent: 2.76; lr: 0.00100; 6208/5251 tok/s;   3383 sec\n",
      "[2019-02-16 20:23:15,557 INFO] Step 12200/20000; acc:  49.83; ppl: 12.82; xent: 2.55; lr: 0.00100; 14673/12697 tok/s;   3392 sec\n",
      "[2019-02-16 20:23:26,640 INFO] Step 12300/20000; acc:  48.20; ppl: 14.50; xent: 2.67; lr: 0.00100; 14648/12510 tok/s;   3403 sec\n",
      "[2019-02-16 20:23:37,437 INFO] Step 12400/20000; acc:  48.31; ppl: 14.79; xent: 2.69; lr: 0.00100; 14591/12360 tok/s;   3414 sec\n",
      "[2019-02-16 20:23:48,124 INFO] Step 12500/20000; acc:  48.29; ppl: 14.55; xent: 2.68; lr: 0.00100; 15101/12862 tok/s;   3425 sec\n",
      "[2019-02-16 20:23:48,223 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last accent valid acc: 47.30724439268538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-16 20:24:04,205 INFO] Validation perplexity: 19.3363\n",
      "[2019-02-16 20:24:04,205 INFO] Validation accuracy: 47.2952\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decent round: 5\n",
      "meet early stop condition, prev best loss: 47.30724439268538 current loss: 47.29524300585845\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NMTModel' object has no attribute 'optim'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-4f5eab3db178>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mtotal_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mtotal_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mreverse_runner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mreverse_runner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-c2dcbd608b8e>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, option, model_id, train_from)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mArgumentHelper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_dict_to_arg_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mtrain_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtranslate_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-13-9faf6c3b82ea>\u001b[0m in \u001b[0;36mtrain_main\u001b[0;34m(opt, device_id)\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mvalid_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0mearly_stop_round\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mearly_stop_round\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         early_stop_threshold=opt.early_stop_threshold)\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensorboard\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-6cff7a8dd936>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_iter, train_steps, save_checkpoint_steps, valid_iter, valid_steps, early_stop_round, early_stop_threshold)\u001b[0m\n\u001b[1;32m     71\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_model_saver\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m                         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'meet early stop condition, prev best loss:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlast_accent_valid_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'current loss:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_acc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m                         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_model_saver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_model_saver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmoving_average\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast_model_saver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoving_average\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mtotal_stats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/dl/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    533\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[0;32m--> 535\u001b[0;31m             type(self).__name__, name))\n\u001b[0m\u001b[1;32m    536\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    537\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NMTModel' object has no attribute 'optim'"
     ]
    }
   ],
   "source": [
    "model_args = {\n",
    "    'encoder_type': 'brnn',\n",
    "    'rnn_type': 'GRU'\n",
    "}\n",
    "train_args = {\n",
    "    'batch_size': 64,\n",
    "    'train_steps': 20000,\n",
    "    'optim': 'adam',\n",
    "    'learning_rate': 0.001,\n",
    "    'valid_step': 500,\n",
    "    'valid_batch': 64,\n",
    "    'early_stop_round': 5,\n",
    "    'early_stop_threshold': 0.01,\n",
    "    'report_every': 100\n",
    "}\n",
    "\n",
    "total_args = {}\n",
    "total_args.update(model_args)\n",
    "total_args.update(train_args)\n",
    "reverse_runner.run(total_args)\n",
    "reverse_runner.model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-15 20:13:46,283 INFO]  * src vocab size = 8870\n",
      "[2019-02-15 20:13:46,284 INFO]  * tgt vocab size = 20071\n",
      "[2019-02-15 20:13:46,284 INFO] Building model...\n",
      "[2019-02-15 20:13:49,837 INFO] NMTModel(\n",
      "  (encoder): RNNEncoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(8870, 500, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (rnn): LSTM(500, 500, num_layers=2, dropout=0.3)\n",
      "  )\n",
      "  (decoder): InputFeedRNNDecoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(20071, 500, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.3)\n",
      "    (rnn): StackedLSTM(\n",
      "      (dropout): Dropout(p=0.3)\n",
      "      (layers): ModuleList(\n",
      "        (0): LSTMCell(1000, 500)\n",
      "        (1): LSTMCell(500, 500)\n",
      "      )\n",
      "    )\n",
      "    (attn): GlobalAttention(\n",
      "      (linear_in): Linear(in_features=500, out_features=500, bias=False)\n",
      "      (linear_out): Linear(in_features=1000, out_features=500, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (generator): Sequential(\n",
      "    (0): Linear(in_features=500, out_features=20071, bias=True)\n",
      "    (1): Cast()\n",
      "    (2): LogSoftmax()\n",
      "  )\n",
      ")\n",
      "[2019-02-15 20:13:49,838 INFO] encoder: 8443000\n",
      "[2019-02-15 20:13:49,839 INFO] decoder: 25849071\n",
      "[2019-02-15 20:13:49,839 INFO] * number of parameters: 34292071\n",
      "[2019-02-15 20:13:49,966 INFO] Starting training on GPU: [0]\n",
      "[2019-02-15 20:13:49,966 INFO] Start training loop and validate every 500 steps...\n",
      "[2019-02-15 20:13:51,397 INFO] Loading dataset from data/nmt15-reverse.train.0.pt, number of examples: 95844\n",
      "[2019-02-15 20:14:07,847 INFO] Step 100/20000; acc:   9.43; ppl: 645.62; xent: 6.47; lr: 0.00100; 17789/15143 tok/s;     18 sec\n",
      "[2019-02-15 20:14:24,112 INFO] Step 200/20000; acc:  19.38; ppl: 246.54; xent: 5.51; lr: 0.00100; 19625/16747 tok/s;     34 sec\n",
      "[2019-02-15 20:14:40,390 INFO] Step 300/20000; acc:  25.44; ppl: 148.64; xent: 5.00; lr: 0.00100; 19520/16584 tok/s;     50 sec\n",
      "[2019-02-15 20:14:56,377 INFO] Step 400/20000; acc:  29.43; ppl: 107.96; xent: 4.68; lr: 0.00100; 19676/16743 tok/s;     66 sec\n",
      "[2019-02-15 20:15:12,542 INFO] Step 500/20000; acc:  32.75; ppl: 81.31; xent: 4.40; lr: 0.00100; 19503/16602 tok/s;     83 sec\n",
      "[2019-02-15 20:15:12,644 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n",
      "[2019-02-15 20:15:28,678 INFO] Validation perplexity: 60.4439\n",
      "[2019-02-15 20:15:28,679 INFO] Validation accuracy: 35.5294\n",
      "[2019-02-15 20:15:44,786 INFO] Step 600/20000; acc:  35.23; ppl: 64.40; xent: 4.17; lr: 0.00100; 9795/8339 tok/s;    115 sec\n",
      "[2019-02-15 20:16:01,149 INFO] Step 700/20000; acc:  37.31; ppl: 53.63; xent: 3.98; lr: 0.00100; 19331/16527 tok/s;    131 sec\n",
      "[2019-02-15 20:16:11,082 INFO] Loading dataset from data/nmt15-reverse.train.0.pt, number of examples: 95844\n",
      "[2019-02-15 20:16:19,121 INFO] Step 800/20000; acc:  39.50; ppl: 44.12; xent: 3.79; lr: 0.00100; 17315/14763 tok/s;    149 sec\n",
      "[2019-02-15 20:16:35,010 INFO] Step 900/20000; acc:  40.37; ppl: 39.91; xent: 3.69; lr: 0.00100; 19320/16465 tok/s;    165 sec\n",
      "[2019-02-15 20:16:52,871 INFO] Step 1000/20000; acc:  41.20; ppl: 36.09; xent: 3.59; lr: 0.00100; 19288/16409 tok/s;    183 sec\n",
      "[2019-02-15 20:16:52,969 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n",
      "[2019-02-15 20:17:09,590 INFO] Validation perplexity: 28.7625\n",
      "[2019-02-15 20:17:09,591 INFO] Validation accuracy: 43.9891\n",
      "[2019-02-15 20:17:24,907 INFO] Step 1100/20000; acc:  43.47; ppl: 30.11; xent: 3.40; lr: 0.00100; 9120/7809 tok/s;    215 sec\n",
      "[2019-02-15 20:17:42,898 INFO] Step 1200/20000; acc:  42.76; ppl: 30.43; xent: 3.42; lr: 0.00100; 18991/16016 tok/s;    233 sec\n",
      "[2019-02-15 20:17:57,592 INFO] Step 1300/20000; acc:  45.59; ppl: 24.15; xent: 3.18; lr: 0.00100; 20187/17255 tok/s;    248 sec\n",
      "[2019-02-15 20:18:15,639 INFO] Step 1400/20000; acc:  44.49; ppl: 25.62; xent: 3.24; lr: 0.00100; 19054/16131 tok/s;    266 sec\n",
      "[2019-02-15 20:18:32,479 INFO] Loading dataset from data/nmt15-reverse.train.0.pt, number of examples: 95844\n",
      "[2019-02-15 20:18:33,102 INFO] Step 1500/20000; acc:  47.10; ppl: 20.78; xent: 3.03; lr: 0.00100; 16980/14562 tok/s;    283 sec\n",
      "[2019-02-15 20:18:33,199 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n",
      "[2019-02-15 20:18:49,771 INFO] Validation perplexity: 20.2072\n",
      "[2019-02-15 20:18:49,772 INFO] Validation accuracy: 47.6602\n",
      "[2019-02-15 20:19:06,055 INFO] Step 1600/20000; acc:  46.84; ppl: 20.78; xent: 3.03; lr: 0.00100; 9481/8089 tok/s;    316 sec\n",
      "[2019-02-15 20:19:22,674 INFO] Step 1700/20000; acc:  47.28; ppl: 19.77; xent: 2.98; lr: 0.00100; 19358/16529 tok/s;    333 sec\n",
      "[2019-02-15 20:19:39,030 INFO] Step 1800/20000; acc:  48.23; ppl: 18.28; xent: 2.91; lr: 0.00100; 19294/16379 tok/s;    349 sec\n",
      "[2019-02-15 20:19:55,846 INFO] Step 1900/20000; acc:  48.18; ppl: 18.07; xent: 2.89; lr: 0.00100; 19365/16438 tok/s;    366 sec\n",
      "[2019-02-15 20:20:11,824 INFO] Step 2000/20000; acc:  49.39; ppl: 16.23; xent: 2.79; lr: 0.00100; 19458/16583 tok/s;    382 sec\n",
      "[2019-02-15 20:20:11,924 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n",
      "[2019-02-15 20:20:28,649 INFO] Validation perplexity: 16.876\n",
      "[2019-02-15 20:20:28,650 INFO] Validation accuracy: 49.5915\n",
      "[2019-02-15 20:20:44,703 INFO] Step 2100/20000; acc:  49.64; ppl: 15.70; xent: 2.75; lr: 0.00100; 9494/8093 tok/s;    415 sec\n",
      "[2019-02-15 20:21:01,144 INFO] Step 2200/20000; acc:  49.99; ppl: 15.31; xent: 2.73; lr: 0.00100; 19296/16499 tok/s;    431 sec\n",
      "[2019-02-15 20:21:10,809 INFO] Loading dataset from data/nmt15-reverse.train.0.pt, number of examples: 95844\n",
      "[2019-02-15 20:21:19,225 INFO] Step 2300/20000; acc:  50.94; ppl: 14.07; xent: 2.64; lr: 0.00100; 17189/14630 tok/s;    449 sec\n",
      "[2019-02-15 20:21:35,118 INFO] Step 2400/20000; acc:  50.80; ppl: 14.21; xent: 2.65; lr: 0.00100; 19330/16503 tok/s;    465 sec\n",
      "[2019-02-15 20:21:52,893 INFO] Step 2500/20000; acc:  50.55; ppl: 14.25; xent: 2.66; lr: 0.00100; 19232/16373 tok/s;    483 sec\n",
      "[2019-02-15 20:21:52,993 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n",
      "[2019-02-15 20:22:09,584 INFO] Validation perplexity: 15.1759\n",
      "[2019-02-15 20:22:09,585 INFO] Validation accuracy: 50.7512\n",
      "[2019-02-15 20:22:24,882 INFO] Step 2600/20000; acc:  52.20; ppl: 12.63; xent: 2.54; lr: 0.00100; 9190/7853 tok/s;    515 sec\n",
      "[2019-02-15 20:22:43,081 INFO] Step 2700/20000; acc:  51.07; ppl: 13.58; xent: 2.61; lr: 0.00100; 18970/15990 tok/s;    533 sec\n",
      "[2019-02-15 20:22:57,516 INFO] Step 2800/20000; acc:  53.51; ppl: 11.27; xent: 2.42; lr: 0.00100; 20176/17265 tok/s;    548 sec\n",
      "[2019-02-15 20:23:15,630 INFO] Step 2900/20000; acc:  51.67; ppl: 12.79; xent: 2.55; lr: 0.00100; 18946/16045 tok/s;    566 sec\n",
      "[2019-02-15 20:23:32,270 INFO] Loading dataset from data/nmt15-reverse.train.0.pt, number of examples: 95844\n",
      "[2019-02-15 20:23:33,265 INFO] Step 3000/20000; acc:  53.82; ppl: 10.87; xent: 2.39; lr: 0.00100; 17059/14621 tok/s;    583 sec\n",
      "[2019-02-15 20:23:33,361 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n",
      "[2019-02-15 20:23:49,957 INFO] Validation perplexity: 14.0014\n",
      "[2019-02-15 20:23:49,958 INFO] Validation accuracy: 51.7086\n",
      "[2019-02-15 20:24:06,195 INFO] Step 3100/20000; acc:  53.08; ppl: 11.38; xent: 2.43; lr: 0.00100; 9439/8072 tok/s;    616 sec\n",
      "[2019-02-15 20:24:23,028 INFO] Step 3200/20000; acc:  53.24; ppl: 11.39; xent: 2.43; lr: 0.00100; 19317/16444 tok/s;    633 sec\n",
      "[2019-02-15 20:24:39,280 INFO] Step 3300/20000; acc:  54.12; ppl: 10.62; xent: 2.36; lr: 0.00100; 19247/16355 tok/s;    649 sec\n",
      "[2019-02-15 20:24:56,030 INFO] Step 3400/20000; acc:  53.60; ppl: 10.90; xent: 2.39; lr: 0.00100; 19243/16351 tok/s;    666 sec\n",
      "[2019-02-15 20:25:12,027 INFO] Step 3500/20000; acc:  54.77; ppl: 10.04; xent: 2.31; lr: 0.00100; 19480/16616 tok/s;    682 sec\n",
      "[2019-02-15 20:25:12,122 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n",
      "[2019-02-15 20:25:28,843 INFO] Validation perplexity: 13.4834\n",
      "[2019-02-15 20:25:28,844 INFO] Validation accuracy: 52.2358\n",
      "[2019-02-15 20:25:45,173 INFO] Step 3600/20000; acc:  54.57; ppl: 10.09; xent: 2.31; lr: 0.00100; 9624/8174 tok/s;    715 sec\n",
      "[2019-02-15 20:26:01,414 INFO] Step 3700/20000; acc:  55.12; ppl:  9.72; xent: 2.27; lr: 0.00100; 19277/16502 tok/s;    731 sec\n",
      "[2019-02-15 20:26:10,764 INFO] Loading dataset from data/nmt15-reverse.train.0.pt, number of examples: 95844\n",
      "[2019-02-15 20:26:19,555 INFO] Step 3800/20000; acc:  55.46; ppl:  9.37; xent: 2.24; lr: 0.00100; 17129/14616 tok/s;    750 sec\n",
      "[2019-02-15 20:26:35,399 INFO] Step 3900/20000; acc:  55.17; ppl:  9.63; xent: 2.27; lr: 0.00100; 19368/16510 tok/s;    765 sec\n",
      "[2019-02-15 20:26:53,212 INFO] Step 4000/20000; acc:  54.90; ppl:  9.80; xent: 2.28; lr: 0.00100; 19237/16369 tok/s;    783 sec\n",
      "[2019-02-15 20:26:53,314 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n",
      "[2019-02-15 20:27:09,881 INFO] Validation perplexity: 12.9821\n",
      "[2019-02-15 20:27:09,882 INFO] Validation accuracy: 52.7937\n",
      "[2019-02-15 20:27:25,163 INFO] Step 4100/20000; acc:  56.38; ppl:  8.85; xent: 2.18; lr: 0.00100; 9200/7864 tok/s;    815 sec\n",
      "[2019-02-15 20:27:43,220 INFO] Step 4200/20000; acc:  55.20; ppl:  9.58; xent: 2.26; lr: 0.00100; 19001/16021 tok/s;    833 sec\n",
      "[2019-02-15 20:27:57,637 INFO] Step 4300/20000; acc:  57.69; ppl:  8.01; xent: 2.08; lr: 0.00100; 20100/17227 tok/s;    848 sec\n",
      "[2019-02-15 20:28:15,868 INFO] Step 4400/20000; acc:  55.53; ppl:  9.26; xent: 2.23; lr: 0.00100; 19020/16081 tok/s;    866 sec\n",
      "[2019-02-15 20:28:32,182 INFO] Loading dataset from data/nmt15-reverse.train.0.pt, number of examples: 95844\n",
      "[2019-02-15 20:28:33,507 INFO] Step 4500/20000; acc:  57.63; ppl:  7.99; xent: 2.08; lr: 0.00100; 17028/14601 tok/s;    884 sec\n",
      "[2019-02-15 20:28:33,606 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n",
      "[2019-02-15 20:28:50,196 INFO] Validation perplexity: 12.8824\n",
      "[2019-02-15 20:28:50,197 INFO] Validation accuracy: 53.0092\n",
      "[2019-02-15 20:29:06,423 INFO] Step 4600/20000; acc:  56.66; ppl:  8.54; xent: 2.14; lr: 0.00100; 9449/8088 tok/s;    916 sec\n",
      "[2019-02-15 20:29:23,210 INFO] Step 4700/20000; acc:  56.58; ppl:  8.59; xent: 2.15; lr: 0.00100; 19292/16408 tok/s;    933 sec\n",
      "[2019-02-15 20:29:39,428 INFO] Step 4800/20000; acc:  57.48; ppl:  8.06; xent: 2.09; lr: 0.00100; 19248/16368 tok/s;    949 sec\n",
      "[2019-02-15 20:29:56,182 INFO] Step 4900/20000; acc:  56.82; ppl:  8.37; xent: 2.12; lr: 0.00100; 19370/16412 tok/s;    966 sec\n",
      "[2019-02-15 20:30:12,062 INFO] Step 5000/20000; acc:  58.20; ppl:  7.66; xent: 2.04; lr: 0.00100; 19430/16641 tok/s;    982 sec\n",
      "[2019-02-15 20:30:12,159 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n",
      "[2019-02-15 20:30:28,892 INFO] Validation perplexity: 12.7988\n",
      "[2019-02-15 20:30:28,892 INFO] Validation accuracy: 52.9848\n",
      "[2019-02-15 20:30:28,893 INFO] Saving checkpoint /mnt/drive-2t/model/opennmt_18afdcd77652c3d69836a58403953bac_step_5000.pt\n",
      "[2019-02-15 20:30:47,760 INFO] Step 5100/20000; acc:  57.55; ppl:  7.94; xent: 2.07; lr: 0.00100; 9085/7701 tok/s;   1018 sec\n",
      "[2019-02-15 20:31:03,856 INFO] Step 5200/20000; acc:  58.36; ppl:  7.52; xent: 2.02; lr: 0.00100; 19250/16488 tok/s;   1034 sec\n",
      "[2019-02-15 20:31:12,893 INFO] Loading dataset from data/nmt15-reverse.train.0.pt, number of examples: 95844\n",
      "[2019-02-15 20:31:22,157 INFO] Step 5300/20000; acc:  58.35; ppl:  7.49; xent: 2.01; lr: 0.00100; 17120/14589 tok/s;   1052 sec\n",
      "[2019-02-15 20:31:38,174 INFO] Step 5400/20000; acc:  57.97; ppl:  7.78; xent: 2.05; lr: 0.00100; 19476/16587 tok/s;   1068 sec\n",
      "[2019-02-15 20:31:55,461 INFO] Step 5500/20000; acc:  58.04; ppl:  7.71; xent: 2.04; lr: 0.00100; 19140/16322 tok/s;   1085 sec\n",
      "[2019-02-15 20:31:55,559 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n",
      "[2019-02-15 20:32:12,125 INFO] Validation perplexity: 12.5926\n",
      "[2019-02-15 20:32:12,126 INFO] Validation accuracy: 53.3782\n",
      "[2019-02-15 20:32:27,523 INFO] Step 5600/20000; acc:  59.20; ppl:  7.15; xent: 1.97; lr: 0.00100; 9249/7896 tok/s;   1118 sec\n",
      "[2019-02-15 20:32:45,754 INFO] Step 5700/20000; acc:  57.72; ppl:  7.84; xent: 2.06; lr: 0.00100; 19013/16043 tok/s;   1136 sec\n",
      "[2019-02-15 20:33:00,104 INFO] Step 5800/20000; acc:  60.48; ppl:  6.47; xent: 1.87; lr: 0.00100; 20167/17241 tok/s;   1150 sec\n",
      "[2019-02-15 20:33:18,290 INFO] Step 5900/20000; acc:  58.14; ppl:  7.58; xent: 2.03; lr: 0.00100; 18958/16056 tok/s;   1168 sec\n",
      "[2019-02-15 20:33:34,345 INFO] Loading dataset from data/nmt15-reverse.train.0.pt, number of examples: 95844\n",
      "[2019-02-15 20:33:36,010 INFO] Step 6000/20000; acc:  60.17; ppl:  6.59; xent: 1.89; lr: 0.00100; 16951/14544 tok/s;   1186 sec\n",
      "[2019-02-15 20:33:36,107 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n",
      "[2019-02-15 20:33:52,709 INFO] Saving checkpoint /mnt/drive-2t/model/opennmt_18afdcd77652c3d69836a58403953bac_step_6000.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meet early stop condition, prev loss: 52.793656155822454 current loss: 53.52796323130673\n"
     ]
    }
   ],
   "source": [
    "train_args['valid_batch'] = 64\n",
    "reverse_runner.run(train_args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'18afdcd77652c3d69836a58403953bac'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reverse_runner.model_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example #2: Train A Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-17 13:47:46,400 INFO]  * src vocab size = 8870\n",
      "[2019-02-17 13:47:46,402 INFO]  * tgt vocab size = 20071\n",
      "[2019-02-17 13:47:46,403 INFO] Building model...\n",
      "[2019-02-17 13:47:50,630 INFO] NMTModel(\n",
      "  (encoder): TransformerEncoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(8870, 512, padding_idx=1)\n",
      "        )\n",
      "        (pe): PositionalEncoding(\n",
      "          (dropout): Dropout(p=0.1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (transformer): ModuleList(\n",
      "      (0): TransformerEncoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (softmax): Softmax()\n",
      "          (dropout): Dropout(p=0.1)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (layer_norm): LayerNorm(torch.Size([512]), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1)\n",
      "          (relu): ReLU()\n",
      "          (dropout_2): Dropout(p=0.1)\n",
      "        )\n",
      "        (layer_norm): LayerNorm(torch.Size([512]), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1)\n",
      "      )\n",
      "      (1): TransformerEncoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (softmax): Softmax()\n",
      "          (dropout): Dropout(p=0.1)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (layer_norm): LayerNorm(torch.Size([512]), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1)\n",
      "          (relu): ReLU()\n",
      "          (dropout_2): Dropout(p=0.1)\n",
      "        )\n",
      "        (layer_norm): LayerNorm(torch.Size([512]), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1)\n",
      "      )\n",
      "      (2): TransformerEncoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (softmax): Softmax()\n",
      "          (dropout): Dropout(p=0.1)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (layer_norm): LayerNorm(torch.Size([512]), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1)\n",
      "          (relu): ReLU()\n",
      "          (dropout_2): Dropout(p=0.1)\n",
      "        )\n",
      "        (layer_norm): LayerNorm(torch.Size([512]), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1)\n",
      "      )\n",
      "      (3): TransformerEncoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (softmax): Softmax()\n",
      "          (dropout): Dropout(p=0.1)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (layer_norm): LayerNorm(torch.Size([512]), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1)\n",
      "          (relu): ReLU()\n",
      "          (dropout_2): Dropout(p=0.1)\n",
      "        )\n",
      "        (layer_norm): LayerNorm(torch.Size([512]), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1)\n",
      "      )\n",
      "      (4): TransformerEncoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (softmax): Softmax()\n",
      "          (dropout): Dropout(p=0.1)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (layer_norm): LayerNorm(torch.Size([512]), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1)\n",
      "          (relu): ReLU()\n",
      "          (dropout_2): Dropout(p=0.1)\n",
      "        )\n",
      "        (layer_norm): LayerNorm(torch.Size([512]), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1)\n",
      "      )\n",
      "      (5): TransformerEncoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (softmax): Softmax()\n",
      "          (dropout): Dropout(p=0.1)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (layer_norm): LayerNorm(torch.Size([512]), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1)\n",
      "          (relu): ReLU()\n",
      "          (dropout_2): Dropout(p=0.1)\n",
      "        )\n",
      "        (layer_norm): LayerNorm(torch.Size([512]), eps=1e-06, elementwise_affine=True)\n",
      "        (dropout): Dropout(p=0.1)\n",
      "      )\n",
      "    )\n",
      "    (layer_norm): LayerNorm(torch.Size([512]), eps=1e-06, elementwise_affine=True)\n",
      "  )\n",
      "  (decoder): TransformerDecoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(20071, 512, padding_idx=1)\n",
      "        )\n",
      "        (pe): PositionalEncoding(\n",
      "          (dropout): Dropout(p=0.1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (transformer_layers): ModuleList(\n",
      "      (0): TransformerDecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (softmax): Softmax()\n",
      "          (dropout): Dropout(p=0.1)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (context_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (softmax): Softmax()\n",
      "          (dropout): Dropout(p=0.1)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (layer_norm): LayerNorm(torch.Size([512]), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1)\n",
      "          (relu): ReLU()\n",
      "          (dropout_2): Dropout(p=0.1)\n",
      "        )\n",
      "        (layer_norm_1): LayerNorm(torch.Size([512]), eps=1e-06, elementwise_affine=True)\n",
      "        (layer_norm_2): LayerNorm(torch.Size([512]), eps=1e-06, elementwise_affine=True)\n",
      "        (drop): Dropout(p=0.1)\n",
      "      )\n",
      "      (1): TransformerDecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (softmax): Softmax()\n",
      "          (dropout): Dropout(p=0.1)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (context_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (softmax): Softmax()\n",
      "          (dropout): Dropout(p=0.1)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (layer_norm): LayerNorm(torch.Size([512]), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1)\n",
      "          (relu): ReLU()\n",
      "          (dropout_2): Dropout(p=0.1)\n",
      "        )\n",
      "        (layer_norm_1): LayerNorm(torch.Size([512]), eps=1e-06, elementwise_affine=True)\n",
      "        (layer_norm_2): LayerNorm(torch.Size([512]), eps=1e-06, elementwise_affine=True)\n",
      "        (drop): Dropout(p=0.1)\n",
      "      )\n",
      "      (2): TransformerDecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (softmax): Softmax()\n",
      "          (dropout): Dropout(p=0.1)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (context_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (softmax): Softmax()\n",
      "          (dropout): Dropout(p=0.1)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (layer_norm): LayerNorm(torch.Size([512]), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1)\n",
      "          (relu): ReLU()\n",
      "          (dropout_2): Dropout(p=0.1)\n",
      "        )\n",
      "        (layer_norm_1): LayerNorm(torch.Size([512]), eps=1e-06, elementwise_affine=True)\n",
      "        (layer_norm_2): LayerNorm(torch.Size([512]), eps=1e-06, elementwise_affine=True)\n",
      "        (drop): Dropout(p=0.1)\n",
      "      )\n",
      "      (3): TransformerDecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (softmax): Softmax()\n",
      "          (dropout): Dropout(p=0.1)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (context_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (softmax): Softmax()\n",
      "          (dropout): Dropout(p=0.1)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (layer_norm): LayerNorm(torch.Size([512]), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1)\n",
      "          (relu): ReLU()\n",
      "          (dropout_2): Dropout(p=0.1)\n",
      "        )\n",
      "        (layer_norm_1): LayerNorm(torch.Size([512]), eps=1e-06, elementwise_affine=True)\n",
      "        (layer_norm_2): LayerNorm(torch.Size([512]), eps=1e-06, elementwise_affine=True)\n",
      "        (drop): Dropout(p=0.1)\n",
      "      )\n",
      "      (4): TransformerDecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (softmax): Softmax()\n",
      "          (dropout): Dropout(p=0.1)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (context_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (softmax): Softmax()\n",
      "          (dropout): Dropout(p=0.1)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (layer_norm): LayerNorm(torch.Size([512]), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1)\n",
      "          (relu): ReLU()\n",
      "          (dropout_2): Dropout(p=0.1)\n",
      "        )\n",
      "        (layer_norm_1): LayerNorm(torch.Size([512]), eps=1e-06, elementwise_affine=True)\n",
      "        (layer_norm_2): LayerNorm(torch.Size([512]), eps=1e-06, elementwise_affine=True)\n",
      "        (drop): Dropout(p=0.1)\n",
      "      )\n",
      "      (5): TransformerDecoderLayer(\n",
      "        (self_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (softmax): Softmax()\n",
      "          (dropout): Dropout(p=0.1)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (context_attn): MultiHeadedAttention(\n",
      "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
      "          (softmax): Softmax()\n",
      "          (dropout): Dropout(p=0.1)\n",
      "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
      "        )\n",
      "        (feed_forward): PositionwiseFeedForward(\n",
      "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
      "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
      "          (layer_norm): LayerNorm(torch.Size([512]), eps=1e-06, elementwise_affine=True)\n",
      "          (dropout_1): Dropout(p=0.1)\n",
      "          (relu): ReLU()\n",
      "          (dropout_2): Dropout(p=0.1)\n",
      "        )\n",
      "        (layer_norm_1): LayerNorm(torch.Size([512]), eps=1e-06, elementwise_affine=True)\n",
      "        (layer_norm_2): LayerNorm(torch.Size([512]), eps=1e-06, elementwise_affine=True)\n",
      "        (drop): Dropout(p=0.1)\n",
      "      )\n",
      "    )\n",
      "    (layer_norm): LayerNorm(torch.Size([512]), eps=1e-06, elementwise_affine=True)\n",
      "  )\n",
      "  (generator): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=20071, bias=True)\n",
      "    (1): Cast()\n",
      "    (2): LogSoftmax()\n",
      "  )\n",
      ")\n",
      "[2019-02-17 13:47:50,635 INFO] encoder: 23456768\n",
      "[2019-02-17 13:47:50,635 INFO] decoder: 45797991\n",
      "[2019-02-17 13:47:50,636 INFO] * number of parameters: 69254759\n",
      "[2019-02-17 13:47:50,802 INFO] Starting training on GPU: [0]\n",
      "[2019-02-17 13:47:50,802 INFO] Start training loop and validate every 200 steps...\n",
      "[2019-02-17 13:47:52,234 INFO] Loading dataset from data/nmt15-reverse.train.0.pt, number of examples: 95844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Testing GPU memory capability with batch shape: torch.Size([200, 16, 1]) torch.Size([190, 16, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-17 13:49:28,193 INFO] Step 100/100000; loss: 2621894.360388 acc:   9.87; ppl: 1468.88; xent: 7.29; lr: 0.00100; 4701/3692 tok/s;     97 sec\n",
      "[2019-02-17 13:50:44,865 INFO] Step 200/100000; loss: 1312841.478624 acc:  19.83; ppl: 207.88; xent: 5.34; lr: 0.00100; 3932/3208 tok/s;    174 sec\n",
      "[2019-02-17 13:50:44,973 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last accent valid acc: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-17 13:51:07,936 INFO] Validation perplexity: 169.087\n",
      "[2019-02-17 13:51:07,937 INFO] Validation accuracy: 17.9167\n",
      "[2019-02-17 13:52:20,121 INFO] Step 300/100000; loss: 1025134.557983 acc:  22.01; ppl: 155.75; xent: 5.05; lr: 0.00100; 2590/2132 tok/s;    269 sec\n",
      "[2019-02-17 13:53:28,372 INFO] Step 400/100000; loss: 846213.914762 acc:  23.66; ppl: 125.23; xent: 4.83; lr: 0.00100; 3088/2567 tok/s;    338 sec\n",
      "[2019-02-17 13:53:28,476 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last accent valid acc: -1154209.2971801758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-17 13:53:51,487 INFO] Validation perplexity: 129.13\n",
      "[2019-02-17 13:53:51,488 INFO] Validation accuracy: 20.3286\n",
      "[2019-02-17 13:54:57,907 INFO] Step 500/100000; loss: 725905.292447 acc:  25.47; ppl: 105.02; xent: 4.65; lr: 0.00100; 2048/1742 tok/s;    427 sec\n",
      "[2019-02-17 13:56:02,954 INFO] Step 600/100000; loss: 625419.903428 acc:  27.69; ppl: 90.40; xent: 4.50; lr: 0.00100; 2486/2135 tok/s;    492 sec\n",
      "[2019-02-17 13:56:03,061 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last accent valid acc: -1093558.2692260742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-17 13:56:25,877 INFO] Validation perplexity: 103.692\n",
      "[2019-02-17 13:56:25,878 INFO] Validation accuracy: 24.0854\n",
      "[2019-02-17 13:57:29,123 INFO] Step 700/100000; loss: 524134.457756 acc:  31.63; ppl: 68.98; xent: 4.23; lr: 0.00100; 1663/1437 tok/s;    578 sec\n",
      "[2019-02-17 13:58:31,544 INFO] Step 800/100000; loss: 438472.353240 acc:  36.38; ppl: 51.59; xent: 3.94; lr: 0.00100; 2041/1781 tok/s;    641 sec\n",
      "[2019-02-17 13:58:31,649 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last accent valid acc: -1044200.5935668945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-17 13:58:54,671 INFO] Validation perplexity: 81.9583\n",
      "[2019-02-17 13:58:54,672 INFO] Validation accuracy: 29.1896\n",
      "[2019-02-17 13:59:55,934 INFO] Step 900/100000; loss: 368895.661441 acc:  40.03; ppl: 39.85; xent: 3.69; lr: 0.00100; 1344/1186 tok/s;    725 sec\n",
      "[2019-02-17 14:00:56,234 INFO] Step 1000/100000; loss: 312602.137179 acc:  43.20; ppl: 31.77; xent: 3.46; lr: 0.00100; 1670/1499 tok/s;    785 sec\n",
      "[2019-02-17 14:00:56,338 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last accent valid acc: -991282.8646850586\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-17 14:01:19,221 INFO] Validation perplexity: 77.1449\n",
      "[2019-02-17 14:01:19,222 INFO] Validation accuracy: 30.3808\n",
      "[2019-02-17 14:02:18,851 INFO] Step 1100/100000; loss: 265746.847168 acc:  45.54; ppl: 26.43; xent: 3.27; lr: 0.00100; 1076/982 tok/s;    868 sec\n",
      "[2019-02-17 14:03:17,673 INFO] Step 1200/100000; loss: 223866.080187 acc:  48.00; ppl: 21.79; xent: 3.08; lr: 0.00100; 1327/1235 tok/s;    927 sec\n",
      "[2019-02-17 14:03:17,791 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last accent valid acc: -977666.166809082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-17 14:03:40,701 INFO] Validation perplexity: 96.9517\n",
      "[2019-02-17 14:03:40,702 INFO] Validation accuracy: 28.7322\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save last accent model with acc: -977666.166809082\n",
      "Decent round: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-17 14:04:11,112 INFO] Step 1300/100000; loss: 185233.869574 acc:  51.04; ppl: 17.77; xent: 2.88; lr: 0.00100; 1254/1205 tok/s;    980 sec\n",
      "[2019-02-17 14:04:40,535 INFO] Step 1400/100000; loss: 150791.138944 acc:  53.29; ppl: 14.73; xent: 2.69; lr: 0.00100; 1866/1905 tok/s;   1010 sec\n",
      "[2019-02-17 14:04:40,640 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last accent valid acc: -977666.166809082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-17 14:05:03,622 INFO] Validation perplexity: 88.2704\n",
      "[2019-02-17 14:05:03,622 INFO] Validation accuracy: 27.9317\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decent round: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-17 14:05:32,770 INFO] Loading dataset from data/nmt15-reverse.train.0.pt, number of examples: 95844\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "CUDA out of memory. Tried to allocate 23.62 MiB (GPU 0; 5.76 GiB total capacity; 3.86 GiB already allocated; 35.06 MiB free; 228.79 MiB cached)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-3b178c0af35d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mtotal_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m \u001b[0mreverse_runner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0mreverse_runner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-c2dcbd608b8e>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, option, model_id, train_from)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mopt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mArgumentHelper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_dict_to_arg_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marg_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mtrain_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtranslate_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-11-9faf6c3b82ea>\u001b[0m in \u001b[0;36mtrain_main\u001b[0;34m(opt, device_id)\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0mvalid_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalid_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0mearly_stop_round\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mearly_stop_round\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m         early_stop_threshold=opt.early_stop_threshold)\n\u001b[0m\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mopt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensorboard\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-149e061c9ebb>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, train_iter, train_steps, save_checkpoint_steps, valid_iter, valid_steps, early_stop_round, early_stop_threshold)\u001b[0m\n\u001b[1;32m     36\u001b[0m             self._gradient_accumulation(\n\u001b[1;32m     37\u001b[0m                 \u001b[0mbatches\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalization\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_stats\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m                 report_stats)\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage_decay\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maverage_every\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/OpenNMT-py/onmt/trainer.py\u001b[0m in \u001b[0;36m_gradient_accumulation\u001b[0;34m(self, true_batches, normalization, total_stats, report_stats)\u001b[0m\n\u001b[1;32m    315\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad_accum_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m                 \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtgt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msrc_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbptt\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbptt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m                 \u001b[0mbptt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/dl/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/OpenNMT-py/onmt/models/model.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, src, tgt, lengths, bptt)\u001b[0m\n\u001b[1;32m     47\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minit_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_bank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         dec_out, attns = self.decoder(tgt, memory_bank,\n\u001b[0;32m---> 49\u001b[0;31m                                       memory_lengths=lengths)\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdec_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/dl/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/OpenNMT-py/onmt/decoders/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tgt, memory_bank, step, **kwargs)\u001b[0m\n\u001b[1;32m    207\u001b[0m                 \u001b[0mtgt_pad_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m                 \u001b[0mlayer_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m                 step=step)\n\u001b[0m\u001b[1;32m    210\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/dl/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/OpenNMT-py/onmt/decoders/transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs, memory_bank, src_pad_mask, tgt_pad_mask, layer_cache, step)\u001b[0m\n\u001b[1;32m     85\u001b[0m                                       \u001b[0mlayer_cache\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlayer_cache\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m                                       type=\"context\")\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeed_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmid\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/dl/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/OpenNMT-py/onmt/modules/position_ffn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     36\u001b[0m             \u001b[0moutput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_len\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel_dim\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \"\"\"\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0minter\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mw_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutput\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/dl/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    487\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 489\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    490\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/dl/lib/python3.6/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mweak_script_method\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/dl/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1352\u001b[0m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unwrap_optional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1353\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1354\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmatmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1355\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbias\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjit\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_unwrap_optional\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: CUDA out of memory. Tried to allocate 23.62 MiB (GPU 0; 5.76 GiB total capacity; 3.86 GiB already allocated; 35.06 MiB free; 228.79 MiB cached)"
     ]
    }
   ],
   "source": [
    "model_args = {\n",
    "    'model_dtype': 'fp32',\n",
    "    'encoder_type': 'transformer',\n",
    "    'decoder_type': 'transformer',\n",
    "    'layers': 6,\n",
    "    'position_encoding': None,\n",
    "    'max_generator_batches': 2,\n",
    "    'dropout': 0.1,\n",
    "    'enc_rnn_size': 512,\n",
    "    'dec_rnn_size': 512,\n",
    "    'src_word_vec_size': 512,\n",
    "    'tgt_word_vec_size': 512,\n",
    "    'accum_count': 4\n",
    "}\n",
    "train_args = {\n",
    "    'batch_size': 16,\n",
    "    'train_steps': 100000,\n",
    "    'optim': 'adam',\n",
    "    'learning_rate': 0.001,\n",
    "    'valid_step': 200,\n",
    "    'valid_batch': 16,\n",
    "    'early_stop_round': 10,\n",
    "    'early_stop_threshold': 0,\n",
    "    'report_every': 100\n",
    "}\n",
    "total_args = {}\n",
    "total_args.update(model_args)\n",
    "total_args.update(train_args)\n",
    "\n",
    "reverse_runner.run(total_args)\n",
    "reverse_runner.model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'__name__': '__main__',\n",
       " '__doc__': 'Automatically created module for IPython interactive environment',\n",
       " '__package__': None,\n",
       " '__loader__': None,\n",
       " '__spec__': None,\n",
       " '__builtin__': <module 'builtins' (built-in)>,\n",
       " '__builtins__': <module 'builtins' (built-in)>,\n",
       " '_ih': ['',\n",
       "  'import onmt\\nfrom onmt.trainer import Trainer\\nfrom onmt.utils.logging import logger\\nfrom onmt.utils.loss import build_loss_compute\\nimport onmt.opts as opts\\nfrom onmt.inputters.inputter import build_dataset_iter, \\\\\\n    load_old_vocab, old_style_vocab\\nfrom onmt.model_builder import build_model\\nfrom onmt.utils.optimizers import Optimizer\\nfrom onmt.utils.misc import set_random_seed\\nfrom onmt.models import build_model_saver\\nfrom onmt.utils.logging import init_logger, logger\\nfrom onmt.utils.misc import split_corpus\\nfrom onmt.translate.translator import build_translator\\n\\nimport itertools\\nimport configargparse\\nimport os\\nfrom itertools import chain\\nimport torch\\nimport subprocess, shlex, re, uuid, hashlib, json, gc, shutil, sys\\nfrom pprint import pprint\\nfrom copy import deepcopy',\n",
       "  \"class ArgumentHelper:\\n    def build(self, arg_dict):\\n        arg_total = self.base_arg_dict.copy()\\n        arg_total.update(arg_dict)\\n        return arg_total\\n    \\n    @staticmethod\\n    def save_args(arg_dict, filename):\\n        with open(filename + '.txt', 'w') as f:\\n            json.dump(arg_dict, f, indent=4)\\n    \\n    @staticmethod\\n    def convert_dict_to_arg_array(arg_dict):\\n        arr = []\\n        for k, v in arg_dict.items():\\n            arr.append('-' + k)\\n            if v is not None:\\n                arr.append(str(v))\\n        return arr\",\n",
       "  'def cleanup():\\n    torch.cuda.empty_cache()\\n    gc.collect()',\n",
       "  \"def build_translate_args(model_id):\\n    return {\\n        'src': 'data/nmt15/test_en',\\n        'tgt': 'data/nmt15/test_vi',\\n        'out': '/mnt/drive-2t/pred/opennmt_' + model_id + '.txt',\\n        'replace_unk': None,\\n        'gpu': '0',\\n        'batch_size': '16'\\n    }\\n\\ndef build_translate_args_reverse(model_id):\\n    return {\\n        'src': 'data/nmt15/test_vi',\\n        'tgt': 'data/nmt15/test_en',\\n        'out': '/mnt/drive-2t/pred/opennmt_' + model_id + '.txt',\\n        'replace_unk': None,\\n        'gpu': '0',\\n        'batch_size': '16'\\n    }\",\n",
       "  \"def clean_path_with_no_checkpoint(model_path, log_path):\\n    useless_file = []\\n    available_model = set()\\n    useless_log = []\\n    for root, dirs, files in os.walk(model_path):\\n        path = root.split(os.sep)\\n        for file in files:\\n            absolute_filename = os.path.join(root, file)\\n            model_id = re.findall(r'opennmt_(.+)_step_\\\\d+\\\\.pt', file)\\n            if len(model_id) != 0:\\n                available_model.add(model_id[0])\\n    \\n    for root, dirs, files in os.walk(model_path):\\n        path = root.split(os.sep)\\n        for file in files:\\n            absolute_filename = os.path.join(root, file)\\n            model_id = re.findall(r'opennmt_(.+).txt', file)\\n            if len(model_id) != 0 and model_id[0] not in available_model:\\n                useless_file.append(absolute_filename)\\n    \\n    dirs = os.listdir(log_path)\\n    for d in dirs:\\n        model_id = re.findall(r'opennmt_(.+)', d)\\n        if len(model_id) != 0 and model_id[0] not in available_model:\\n            useless_log.append(os.path.join(log_path, d))\\n    \\n#     print(available_model)\\n#     print(useless_file)\\n#     print(useless_log)\\n    for f in useless_file:\\n        os.remove(f)\\n    for f in useless_log:\\n        print(f)\\n        get_ipython().system('rm -rf $f')\\n\\n# clean_path_with_no_checkpoint('/mnt/drive-2t/model', '/mnt/drive-2t/log')\",\n",
       "  \"def evaluate_human(model_id):\\n    model_args = '/mnt/drive-2t/model/opennmt_' + model_id + '.txt'\\n    get_ipython().system('cat $model_args')\\n    pred = '/mnt/drive-2t/pred/opennmt_' + model_id + '.txt'\\n    ref = 'data/nmt15/test_en'\\n    print()\\n    print(bleu_evaluate(pred, ref), bleu_evaluate(pred, ref, '0.25 0.25 0.25 0.25'))\\n    pred_head = get_ipython().getoutput('head $pred')\\n    ref_head = get_ipython().getoutput('head $ref')\\n    pairs = list(zip(pred_head, ref_head))\\n    for pair in pairs:\\n        print(pair[0])\\n        print()\\n        print(pair[1])\\n        print('-------------')\",\n",
       "  \"python_interpreter = '/home/ray/.conda/envs/dl/bin/python'\",\n",
       "  \"bleu_script = '/home/ray/Smooth_BLEU/bleu.py'\\nnltk_script = '/home/ray/Smooth_BLEU/nltk_bleu.py'\\nbleu_pred = '/home/ray/OpenNMT-py/data/nmt15/pred.txt'\\nbleu_reference = '/home/ray/OpenNMT-py/data/nmt15/test_vi'\\nbleu_weight = '0.25 0.25 0.25'\",\n",
       "  \"def smooth_bleu_evaluate(pred, reference, weight=bleu_weight):\\n    p = subprocess.Popen([python_interpreter, bleu_script,\\n                          '-t', pred,\\n                          '-r', reference,\\n                          '-w', weight], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\\n    p.wait()\\n    out, err = p.communicate()\\n    if p.returncode != 0:\\n        print(err)\\n        raise Exception()\\n    out = out.decode()\\n    err = err.decode()\\n    bleu_score = re.findall(r'BLEU = ([1-9\\\\.]+)', err)\\n    return float(bleu_score[0])\\n\\ndef nltk_bleu_evaluate(pred, reference, weight=bleu_weight):\\n    p = subprocess.Popen([python_interpreter, nltk_script,\\n                          '-t', pred,\\n                          '-r', reference,\\n                          '-w', weight], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\\n    p.wait()\\n    out, err = p.communicate()\\n    if p.returncode != 0:\\n        print(err)\\n        raise Exception()\\n    out = out.decode()\\n    err = err.decode()\\n    bleu_score = re.findall(r'BLEU = ([1-9\\\\.]+)', err)\\n    return float(bleu_score[0])\\n\\ndef bleu_evaluate(pred, reference, weight=bleu_weight):\\n    smooth = smooth_bleu_evaluate(pred, reference, weight)\\n#     nltk_ = nltk_bleu_evaluate(pred, reference, weight)\\n#     return smooth, nltk_\\n    return smooth\",\n",
       "  \"# searchable hyperparameters\\nembedding_size = ['64', '128', '256', '512']\\nencoder_type = ['rnn', 'brnn', 'mean', 'transformer']\\ndecoder_type = encoder_type\\nlayers = ['1', '2', '3', '4', '5', '6']\\nrnn_size = ['64', '128', '256', '512']\\nrnn_type = ['LSTM', 'GRU', 'SRU']\\nglobal_attention = ['dot', 'general', 'mlp']\\nself_attention_type = ['scaled_dot', 'average']\\nheads = [2, 4, 6, 8, 10]\\ntransformer_ff = [1024, 2048, 4096]\\n\\nnormalization = ['sents', 'tokens']\\nvalid_steps = 1000\\ntrain_steps = 10000\\ndropout = 0.1\",\n",
       "  'class MyTrainer(Trainer):\\n    def train(self,\\n              train_iter,\\n              train_steps,\\n              save_checkpoint_steps=5000,\\n              valid_iter=None,\\n              valid_steps=10000,\\n              early_stop_round=50000,\\n              early_stop_threshold=1):\\n        self.last_model_saver = None\\n        self.last_step = None\\n        self.last_moving_average = None\\n        if valid_iter is None:\\n            logger.info(\\'Start training loop without validation...\\')\\n        else:\\n            logger.info(\\'Start training loop and validate every %d steps...\\',\\n                        valid_steps)\\n\\n        total_stats = onmt.utils.Statistics()\\n        report_stats = onmt.utils.Statistics()\\n        self._start_report_manager(start_time=total_stats.start_time)\\n        \\n        valid_early_stop_loss = []\\n        last_accent_valid_acc = float(\\'-inf\\')\\n        decent_round = 0\\n        for i, (batches, normalization) in enumerate(\\n                self._accum_batches(train_iter)):\\n#             if i == 0:\\n            print(\\'Testing GPU memory capability with batch shape:\\', batches[0].src[0].shape)\\n            step = self.optim.training_step\\n            if decent_round == 0:  # 如果没有处于下降过程，则保存前一个model，如果当前step下降，则一直保存这个model\\n                self.last_model_saver = deepcopy(self.model_saver)\\n                self.last_step = step\\n                self.last_moving_average = deepcopy(self.moving_average)\\n\\n            self._gradient_accumulation(\\n                batches, normalization, total_stats,\\n                report_stats)\\n\\n            if self.average_decay > 0 and i % self.average_every == 0:\\n                self._update_average(step)\\n                            \\n            # 向tensorboard输出loss\\n            if step % self.report_manager.report_every == 0:\\n                tensorboard_writer = self.report_manager.tensorboard_writer\\n                tensorboard_writer.add_scalar(\\'train/loss\\', report_stats.loss, step)\\n\\n            report_stats = self._maybe_report_training(\\n                step, train_steps,\\n                self.optim.learning_rate(),\\n                report_stats)\\n\\n            \\n            if valid_iter is not None and step % valid_steps == 0:\\n                print(\\'last accent valid acc:\\', last_accent_valid_acc)\\n                if step == valid_steps:\\n                    valid_stats = self.validate(\\n                        valid_iter, verbose=1, moving_average=self.moving_average)\\n                else:\\n                    valid_stats = self.validate(\\n                        valid_iter, moving_average=self.moving_average)\\n                valid_stats = self._maybe_gather_stats(valid_stats)\\n                tensorboard_writer.add_scalar(\\'valid/loss\\', valid_stats.loss, step)\\n#                 valid_acc = valid_stats.accuracy()\\n                valid_acc = -valid_stats.loss\\n                \\n#                 self._report_step(self.optim.learning_rate(),\\n#                                   step, valid_stats=valid_stats)\\n#                 if last_accent_valid_acc + early_stop_threshold > valid_acc:\\n#                     print(\\'meet early stop condition, prev best loss:\\', last_accent_valid_acc, \\'current loss:\\', valid_acc)\\n#                     if self.model_saver is not None:\\n#                         self.model_saver.save(step, moving_average=self.moving_average)\\n#                     return total_stats\\n                \\n#                 last_accent_valid_acc = valid_acc\\n                \\n                if valid_acc < last_accent_valid_acc + early_stop_threshold:  # 开始下降\\n                    if decent_round == 0:  # 保存最后一个上升model\\n                        print(\\'Save last accent model with acc:\\', last_accent_valid_acc)\\n                    decent_round += 1\\n                    print(\\'Decent round:\\', decent_round)\\n                else:\\n                    decent_round = 0\\n                    last_accent_valid_acc = valid_acc\\n\\n                self._report_step(self.optim.learning_rate(),\\n                                  step, valid_stats=valid_stats)\\n                if decent_round == early_stop_round:  # 停止\\n                    if self.last_model_saver is not None:\\n                        print(\\'meet early stop condition, prev best loss:\\', last_accent_valid_acc, \\'current loss:\\', valid_acc)\\n                        self.last_model_saver.save(self.last_step, moving_average=self.last_moving_average)\\n                    return total_stats\\n\\n#                 if len(valid_early_stop_loss) == early_stop_round:\\n#                     # 条件为当前valid loss要超过前n个valid loss最好的那个加threshold (X)\\n#                     # 应该允许valid loss暂时下降，但不能持续下降超过n个valid round\\n#                     # 如果持续下降，回到最好的model\\n#                     if valid_loss - max(valid_early_stop_loss) < early_stop_threshold:\\n#                         print(\\'meet early stop condition, prev best loss:\\', max(valid_early_stop_loss), \\'current loss:\\', valid_loss)\\n#                         if self.model_saver is not None:\\n#                             self.model_saver.save(step, moving_average=self.moving_average)\\n#                             self._report_step(self.optim.learning_rate(),\\n#                                               step, valid_stats=valid_stats)\\n#                         return total_stats\\n#                     else:\\n#                         valid_early_stop_loss.pop(0)\\n#                         valid_early_stop_loss.append(valid_loss)\\n\\n            if (self.model_saver is not None\\n                    and (save_checkpoint_steps != 0\\n                         and step % save_checkpoint_steps == 0)):\\n                self.model_saver.save(step, moving_average=self.moving_average)\\n\\n            if train_steps > 0 and step >= train_steps:\\n                break\\n\\n        if self.model_saver is not None:\\n            self.model_saver.save(step, moving_average=self.moving_average)\\n        return total_stats\\n    \\n    def validate(self, valid_iter, verbose=0, moving_average=None):\\n        if moving_average:\\n            valid_model = deepcopy(self.model)\\n            for avg, param in zip(self.moving_average,\\n                                  valid_model.parameters()):\\n                param.data = avg.data.half() if self.model_dtype == \"fp16\" \\\\\\n                    else avg.data\\n        else:\\n            valid_model = self.model\\n\\n        # Set model in validating mode.\\n        valid_model.eval()\\n\\n        with torch.no_grad():\\n            stats = onmt.utils.Statistics()\\n\\n            for batch in valid_iter:\\n                if verbose == 1:\\n                    print(\\'Testing GPU memory capability with batch shape:\\', batch[0].src[0].shape)\\n                src, src_lengths = batch.src if isinstance(batch.src, tuple) \\\\\\n                                   else (batch.src, None)\\n                tgt = batch.tgt\\n\\n                # F-prop through the model.\\n                outputs, attns = valid_model(src, tgt, src_lengths)\\n\\n                # Compute loss.\\n                _, batch_stats = self.valid_loss(batch, outputs, attns)\\n\\n                # Update statistics.\\n                stats.update(batch_stats)\\n\\n        if moving_average:\\n            del valid_model\\n        else:\\n            # Set model back to training mode.\\n            valid_model.train()\\n\\n        return stats',\n",
       "  \"def build_trainer(opt, device_id, model, fields, optim, model_saver=None):\\n    tgt_field = fields['tgt'][0][1].base_field\\n    train_loss = build_loss_compute(model, tgt_field, opt)\\n    valid_loss = build_loss_compute(\\n        model, tgt_field, opt, train=False)\\n\\n    trunc_size = opt.truncated_decoder  # Badly named...\\n    shard_size = opt.max_generator_batches if opt.model_dtype == 'fp32' else 0\\n    norm_method = opt.normalization\\n    grad_accum_count = opt.accum_count\\n    n_gpu = opt.world_size\\n    average_decay = opt.average_decay\\n    average_every = opt.average_every\\n    if device_id >= 0:\\n        gpu_rank = opt.gpu_ranks[device_id]\\n    else:\\n        gpu_rank = 0\\n        n_gpu = 0\\n    gpu_verbose_level = opt.gpu_verbose_level\\n\\n    report_manager = onmt.utils.build_report_manager(opt)\\n    trainer = MyTrainer(model, train_loss, valid_loss, optim, trunc_size,\\n                        shard_size, norm_method,\\n                        grad_accum_count, n_gpu, gpu_rank,\\n                        gpu_verbose_level, report_manager,\\n                        model_saver=model_saver if gpu_rank == 0 else None,\\n                        average_decay=average_decay,\\n                        average_every=average_every,\\n                        model_dtype=opt.model_dtype)\\n    return trainer\",\n",
       "  'def _check_save_model_path(opt):\\n    save_model_path = os.path.abspath(opt.save_model)\\n    model_dirname = os.path.dirname(save_model_path)\\n    if not os.path.exists(model_dirname):\\n        os.makedirs(model_dirname)\\n\\n\\ndef _tally_parameters(model):\\n    enc = 0\\n    dec = 0\\n    for name, param in model.named_parameters():\\n        if \\'encoder\\' in name:\\n            enc += param.nelement()\\n        else:\\n            dec += param.nelement()\\n    return enc + dec, enc, dec\\n\\n\\ndef training_opt_postprocessing(opt, device_id):\\n    if opt.word_vec_size != -1:\\n        opt.src_word_vec_size = opt.word_vec_size\\n        opt.tgt_word_vec_size = opt.word_vec_size\\n\\n    if opt.layers != -1:\\n        opt.enc_layers = opt.layers\\n        opt.dec_layers = opt.layers\\n\\n    if opt.rnn_size != -1:\\n        opt.enc_rnn_size = opt.rnn_size\\n        opt.dec_rnn_size = opt.rnn_size\\n\\n        # this check is here because audio allows the encoder and decoder to\\n        # be different sizes, but other model types do not yet\\n        same_size = opt.enc_rnn_size == opt.dec_rnn_size\\n        assert opt.model_type == \\'audio\\' or same_size, \\\\\\n            \"The encoder and decoder rnns must be the same size for now\"\\n\\n    opt.brnn = opt.encoder_type == \"brnn\"\\n\\n    assert opt.rnn_type != \"SRU\" or opt.gpu_ranks, \\\\\\n        \"Using SRU requires -gpu_ranks set.\"\\n\\n    if torch.cuda.is_available() and not opt.gpu_ranks:\\n        logger.info(\"WARNING: You have a CUDA device, \\\\\\n                    should run with -gpu_ranks\")\\n\\n    if device_id >= 0:\\n        torch.cuda.set_device(device_id)\\n    set_random_seed(opt.seed, device_id >= 0)\\n\\n    return opt\\n\\n\\ndef train_main(opt, device_id):\\n    opt = training_opt_postprocessing(opt, device_id)\\n    init_logger(opt.log_file)\\n    # Load checkpoint if we resume from a previous training.\\n    if opt.train_from:\\n        logger.info(\\'Loading checkpoint from %s\\' % opt.train_from)\\n        checkpoint = torch.load(opt.train_from,\\n                                map_location=lambda storage, loc: storage)\\n\\n        # Load default opts values then overwrite it with opts from\\n        # the checkpoint. It\\'s usefull in order to re-train a model\\n        # after adding a new option (not set in checkpoint)\\n        dummy_parser = configargparse.ArgumentParser()\\n        opts.model_opts(dummy_parser)\\n        default_opt = dummy_parser.parse_known_args([])[0]\\n\\n        model_opt = default_opt\\n        model_opt.__dict__.update(checkpoint[\\'opt\\'].__dict__)\\n        logger.info(\\'Loading vocab from checkpoint at %s.\\' % opt.train_from)\\n        vocab = checkpoint[\\'vocab\\']\\n    else:\\n        checkpoint = None\\n        model_opt = opt\\n        vocab = torch.load(opt.data + \\'.vocab.pt\\')\\n\\n    # check for code where vocab is saved instead of fields\\n    # (in the future this will be done in a smarter way)\\n    if old_style_vocab(vocab):\\n        data_type = opt.model_type\\n        fields = load_old_vocab(vocab, data_type, dynamic_dict=opt.copy_attn)\\n    else:\\n        fields = vocab\\n\\n    # Report src and tgt vocab sizes, including for features\\n    for side in [\\'src\\', \\'tgt\\']:\\n        for name, f in fields[side]:\\n            try:\\n                f_iter = iter(f)\\n            except TypeError:\\n                f_iter = [(name, f)]\\n            for sn, sf in f_iter:\\n                if sf.use_vocab:\\n                    logger.info(\\' * %s vocab size = %d\\' % (sn, len(sf.vocab)))\\n\\n    # Build model.\\n    model = build_model(model_opt, opt, fields, checkpoint)\\n    n_params, enc, dec = _tally_parameters(model)\\n    logger.info(\\'encoder: %d\\' % enc)\\n    logger.info(\\'decoder: %d\\' % dec)\\n    logger.info(\\'* number of parameters: %d\\' % n_params)\\n    _check_save_model_path(opt)\\n\\n    # Build optimizer.\\n    optim = Optimizer.from_opt(model, opt, checkpoint=checkpoint)\\n\\n    # Build model saver\\n    model_saver = build_model_saver(model_opt, opt, model, fields, optim)\\n\\n    trainer = build_trainer(\\n        opt, device_id, model, fields, optim, model_saver=model_saver)\\n\\n    # this line is kind of a temporary kludge because different objects expect\\n    # fields to have a different structure\\n    dataset_fields = dict(chain.from_iterable(fields.values()))\\n\\n    train_iter = build_dataset_iter(\"train\", dataset_fields, opt)\\n    valid_iter = build_dataset_iter(\\n        \"valid\", dataset_fields, opt, is_train=False)\\n\\n    if len(opt.gpu_ranks):\\n        logger.info(\\'Starting training on GPU: %s\\' % opt.gpu_ranks)\\n    else:\\n        logger.info(\\'Starting training on CPU, could be very slow\\')\\n    train_steps = opt.train_steps\\n    if opt.single_pass and train_steps > 0:\\n        logger.warning(\"Option single_pass is enabled, ignoring train_steps.\")\\n        train_steps = 0\\n    trainer.train(\\n        train_iter,\\n        train_steps,\\n        save_checkpoint_steps=opt.save_checkpoint_steps,\\n        valid_iter=valid_iter,\\n        valid_steps=opt.valid_steps,\\n        early_stop_round=opt.early_stop_round,\\n        early_stop_threshold=opt.early_stop_threshold)\\n\\n    if opt.tensorboard:\\n        trainer.report_manager.tensorboard_writer.close()',\n",
       "  'class BaseRunner:\\n    \\n    def __init__(self, data_path):\\n        self.data_path = data_path\\n        self.model_id = None\\n    \\n    def run(self, option, model_id=None, train_from=None):\\n        model_id = model_id if model_id is not None else BaseRunner.generate_id()\\n        self.model_id = model_id\\n        self.prepare_save_file()\\n        arg_dict = {\\n            \\'data\\': self.data_path,\\n            \\'save_model\\': self.model_path,\\n            \\'gpu_ranks\\': 0,\\n            \\'tensorboard\\': None,\\n            \\'tensorboard_log_dir\\': self.log_path + \\'/log\\'\\n        }\\n        arg_dict.update(option)\\n        if train_from is not None:\\n            arg_dict[\\'train_from\\'] = train_from\\n        ArgumentHelper.save_args(arg_dict, self.model_path)\\n        parser = configargparse.ArgumentParser(\\n            description=\\'train.py\\',\\n            formatter_class=configargparse.ArgumentDefaultsHelpFormatter)\\n\\n        opts.add_md_help_argument(parser)\\n        opts.model_opts(parser)\\n        opts.train_opts(parser)\\n        parser.add_argument(\\'-early_stop_round\\', type=int, default=10000, help=\\'Early stop round\\')\\n        parser.add_argument(\\'-early_stop_threshold\\', type=float, default=1, help=\\'Early stop threshold\\')\\n\\n        opt = parser.parse_args(ArgumentHelper.convert_dict_to_arg_array(arg_dict))\\n        train_main(opt, 0)\\n\\n    def translate_main(self, opt, logger, model_id):\\n        translator = build_translator(opt, report_score=True)\\n        src_shards = split_corpus(opt.src, opt.shard_size)\\n        tgt_shards = split_corpus(opt.tgt, opt.shard_size) \\\\\\n            if opt.tgt is not None else [None]*opt.shard_size\\n        shard_pairs = zip(src_shards, tgt_shards)\\n\\n        for i, (src_shard, tgt_shard) in enumerate(shard_pairs):\\n            logger.info(\"Translating shard %d.\" % i)\\n            translator.translate(\\n                src=src_shard,\\n                tgt=tgt_shard,\\n                src_dir=opt.src_dir,\\n                batch_size=opt.batch_size,\\n                attn_debug=opt.attn_debug\\n            )\\n\\n    def translate(self, args, model_id=None):\\n        model_id = model_id if model_id is not None else self.model_id\\n        if model_id is None:\\n            raise Exception()\\n        self.model_id = model_id\\n        self.prepare_save_file()\\n        args[\\'model\\'] = self.model_path + \\'_step_\\' + str(BaseRunner.get_latest_model(\\n                                                        self.model_dir,\\n                                                        self.model_prefix,\\n                                                        self.model_id)) + \\'.pt\\'\\n        parser = configargparse.ArgumentParser(\\n            description=\\'translate.py\\',\\n            config_file_parser_class=configargparse.YAMLConfigFileParser,\\n            formatter_class=configargparse.ArgumentDefaultsHelpFormatter)\\n        opts.config_opts(parser)\\n        opts.add_md_help_argument(parser)\\n        opts.translate_opts(parser)\\n\\n        opt = parser.parse_args(ArgumentHelper.convert_dict_to_arg_array(args))\\n        logger = init_logger(opt.log_file)\\n        self.translate_main(opt, logger, model_id)\\n\\n    @staticmethod\\n    def generate_id():\\n        return hashlib.md5(str(uuid.uuid4()).encode()).hexdigest()\\n    \\n    @staticmethod\\n    def get_latest_model(model_dir, model_prefix, model_id):\\n        all_steps = []\\n        for root, dirs, files in os.walk(model_dir):\\n            path = root.split(os.sep)\\n            for file in files:\\n                absolute_filename = os.path.join(root, file)\\n                if model_id in absolute_filename:\\n                    steps = re.findall(model_prefix + model_id + \\'_step_(\\\\d+)\\\\.pt\\', absolute_filename)\\n                    if len(steps) > 0:\\n                        all_steps.append(int(steps[0]))\\n        return max(all_steps)\\n\\n    def prepare_save_file(self):\\n        model_id = self.model_id\\n        model_dir = \\'/mnt/drive-2t/model/\\'\\n        if not os.path.exists(model_dir):\\n            os.makedirs(model_dir)\\n        model_prefix = \\'opennmt_\\'\\n        model_path = model_dir + model_prefix + model_id\\n        log_dir = \\'/mnt/drive-2t/log/\\'\\n        if not os.path.exists(log_dir):\\n            os.makedirs(log_dir)\\n        log_prefix = model_prefix\\n        log_path = log_dir + log_prefix + model_id\\n        if not os.path.exists(log_path):\\n            os.mkdir(log_path)\\n        self.model_path = model_path\\n        self.log_path = log_path\\n        self.model_prefix = model_prefix\\n        self.model_dir = model_dir\\n\\n\\nrunner = BaseRunner(\\'data/nmt15\\')',\n",
       "  \"reverse_runner = BaseRunner('data/nmt15-reverse')\",\n",
       "  \"model_args = {\\n    'model_dtype': 'fp32',\\n    'encoder_type': 'transformer',\\n    'decoder_type': 'transformer',\\n    'layers': 6,\\n    'position_encoding': None,\\n    'max_generator_batches': 2,\\n    'dropout': 0.1,\\n    'enc_rnn_size': 512,\\n    'dec_rnn_size': 512,\\n    'src_word_vec_size': 512,\\n    'tgt_word_vec_size': 512,\\n    'accum_count': 4\\n}\\ntrain_args = {\\n    'batch_size': 16,\\n    'train_steps': 100000,\\n    'optim': 'adam',\\n    'learning_rate': 0.001,\\n    'valid_step': 100,\\n    'valid_batch': 32,\\n    'early_stop_round': 5,\\n    'early_stop_threshold': 0.01,\\n    'report_every': 100\\n}\\ntotal_args = {}\\ntotal_args.update(model_args)\\ntotal_args.update(train_args)\\n\\nreverse_runner.run(total_args)\\nreverse_runner.model_id\",\n",
       "  'globals()'],\n",
       " '_oh': {},\n",
       " '_dh': ['/home/ray/OpenNMT-py'],\n",
       " 'In': ['',\n",
       "  'import onmt\\nfrom onmt.trainer import Trainer\\nfrom onmt.utils.logging import logger\\nfrom onmt.utils.loss import build_loss_compute\\nimport onmt.opts as opts\\nfrom onmt.inputters.inputter import build_dataset_iter, \\\\\\n    load_old_vocab, old_style_vocab\\nfrom onmt.model_builder import build_model\\nfrom onmt.utils.optimizers import Optimizer\\nfrom onmt.utils.misc import set_random_seed\\nfrom onmt.models import build_model_saver\\nfrom onmt.utils.logging import init_logger, logger\\nfrom onmt.utils.misc import split_corpus\\nfrom onmt.translate.translator import build_translator\\n\\nimport itertools\\nimport configargparse\\nimport os\\nfrom itertools import chain\\nimport torch\\nimport subprocess, shlex, re, uuid, hashlib, json, gc, shutil, sys\\nfrom pprint import pprint\\nfrom copy import deepcopy',\n",
       "  \"class ArgumentHelper:\\n    def build(self, arg_dict):\\n        arg_total = self.base_arg_dict.copy()\\n        arg_total.update(arg_dict)\\n        return arg_total\\n    \\n    @staticmethod\\n    def save_args(arg_dict, filename):\\n        with open(filename + '.txt', 'w') as f:\\n            json.dump(arg_dict, f, indent=4)\\n    \\n    @staticmethod\\n    def convert_dict_to_arg_array(arg_dict):\\n        arr = []\\n        for k, v in arg_dict.items():\\n            arr.append('-' + k)\\n            if v is not None:\\n                arr.append(str(v))\\n        return arr\",\n",
       "  'def cleanup():\\n    torch.cuda.empty_cache()\\n    gc.collect()',\n",
       "  \"def build_translate_args(model_id):\\n    return {\\n        'src': 'data/nmt15/test_en',\\n        'tgt': 'data/nmt15/test_vi',\\n        'out': '/mnt/drive-2t/pred/opennmt_' + model_id + '.txt',\\n        'replace_unk': None,\\n        'gpu': '0',\\n        'batch_size': '16'\\n    }\\n\\ndef build_translate_args_reverse(model_id):\\n    return {\\n        'src': 'data/nmt15/test_vi',\\n        'tgt': 'data/nmt15/test_en',\\n        'out': '/mnt/drive-2t/pred/opennmt_' + model_id + '.txt',\\n        'replace_unk': None,\\n        'gpu': '0',\\n        'batch_size': '16'\\n    }\",\n",
       "  \"def clean_path_with_no_checkpoint(model_path, log_path):\\n    useless_file = []\\n    available_model = set()\\n    useless_log = []\\n    for root, dirs, files in os.walk(model_path):\\n        path = root.split(os.sep)\\n        for file in files:\\n            absolute_filename = os.path.join(root, file)\\n            model_id = re.findall(r'opennmt_(.+)_step_\\\\d+\\\\.pt', file)\\n            if len(model_id) != 0:\\n                available_model.add(model_id[0])\\n    \\n    for root, dirs, files in os.walk(model_path):\\n        path = root.split(os.sep)\\n        for file in files:\\n            absolute_filename = os.path.join(root, file)\\n            model_id = re.findall(r'opennmt_(.+).txt', file)\\n            if len(model_id) != 0 and model_id[0] not in available_model:\\n                useless_file.append(absolute_filename)\\n    \\n    dirs = os.listdir(log_path)\\n    for d in dirs:\\n        model_id = re.findall(r'opennmt_(.+)', d)\\n        if len(model_id) != 0 and model_id[0] not in available_model:\\n            useless_log.append(os.path.join(log_path, d))\\n    \\n#     print(available_model)\\n#     print(useless_file)\\n#     print(useless_log)\\n    for f in useless_file:\\n        os.remove(f)\\n    for f in useless_log:\\n        print(f)\\n        get_ipython().system('rm -rf $f')\\n\\n# clean_path_with_no_checkpoint('/mnt/drive-2t/model', '/mnt/drive-2t/log')\",\n",
       "  \"def evaluate_human(model_id):\\n    model_args = '/mnt/drive-2t/model/opennmt_' + model_id + '.txt'\\n    get_ipython().system('cat $model_args')\\n    pred = '/mnt/drive-2t/pred/opennmt_' + model_id + '.txt'\\n    ref = 'data/nmt15/test_en'\\n    print()\\n    print(bleu_evaluate(pred, ref), bleu_evaluate(pred, ref, '0.25 0.25 0.25 0.25'))\\n    pred_head = get_ipython().getoutput('head $pred')\\n    ref_head = get_ipython().getoutput('head $ref')\\n    pairs = list(zip(pred_head, ref_head))\\n    for pair in pairs:\\n        print(pair[0])\\n        print()\\n        print(pair[1])\\n        print('-------------')\",\n",
       "  \"python_interpreter = '/home/ray/.conda/envs/dl/bin/python'\",\n",
       "  \"bleu_script = '/home/ray/Smooth_BLEU/bleu.py'\\nnltk_script = '/home/ray/Smooth_BLEU/nltk_bleu.py'\\nbleu_pred = '/home/ray/OpenNMT-py/data/nmt15/pred.txt'\\nbleu_reference = '/home/ray/OpenNMT-py/data/nmt15/test_vi'\\nbleu_weight = '0.25 0.25 0.25'\",\n",
       "  \"def smooth_bleu_evaluate(pred, reference, weight=bleu_weight):\\n    p = subprocess.Popen([python_interpreter, bleu_script,\\n                          '-t', pred,\\n                          '-r', reference,\\n                          '-w', weight], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\\n    p.wait()\\n    out, err = p.communicate()\\n    if p.returncode != 0:\\n        print(err)\\n        raise Exception()\\n    out = out.decode()\\n    err = err.decode()\\n    bleu_score = re.findall(r'BLEU = ([1-9\\\\.]+)', err)\\n    return float(bleu_score[0])\\n\\ndef nltk_bleu_evaluate(pred, reference, weight=bleu_weight):\\n    p = subprocess.Popen([python_interpreter, nltk_script,\\n                          '-t', pred,\\n                          '-r', reference,\\n                          '-w', weight], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\\n    p.wait()\\n    out, err = p.communicate()\\n    if p.returncode != 0:\\n        print(err)\\n        raise Exception()\\n    out = out.decode()\\n    err = err.decode()\\n    bleu_score = re.findall(r'BLEU = ([1-9\\\\.]+)', err)\\n    return float(bleu_score[0])\\n\\ndef bleu_evaluate(pred, reference, weight=bleu_weight):\\n    smooth = smooth_bleu_evaluate(pred, reference, weight)\\n#     nltk_ = nltk_bleu_evaluate(pred, reference, weight)\\n#     return smooth, nltk_\\n    return smooth\",\n",
       "  \"# searchable hyperparameters\\nembedding_size = ['64', '128', '256', '512']\\nencoder_type = ['rnn', 'brnn', 'mean', 'transformer']\\ndecoder_type = encoder_type\\nlayers = ['1', '2', '3', '4', '5', '6']\\nrnn_size = ['64', '128', '256', '512']\\nrnn_type = ['LSTM', 'GRU', 'SRU']\\nglobal_attention = ['dot', 'general', 'mlp']\\nself_attention_type = ['scaled_dot', 'average']\\nheads = [2, 4, 6, 8, 10]\\ntransformer_ff = [1024, 2048, 4096]\\n\\nnormalization = ['sents', 'tokens']\\nvalid_steps = 1000\\ntrain_steps = 10000\\ndropout = 0.1\",\n",
       "  'class MyTrainer(Trainer):\\n    def train(self,\\n              train_iter,\\n              train_steps,\\n              save_checkpoint_steps=5000,\\n              valid_iter=None,\\n              valid_steps=10000,\\n              early_stop_round=50000,\\n              early_stop_threshold=1):\\n        self.last_model_saver = None\\n        self.last_step = None\\n        self.last_moving_average = None\\n        if valid_iter is None:\\n            logger.info(\\'Start training loop without validation...\\')\\n        else:\\n            logger.info(\\'Start training loop and validate every %d steps...\\',\\n                        valid_steps)\\n\\n        total_stats = onmt.utils.Statistics()\\n        report_stats = onmt.utils.Statistics()\\n        self._start_report_manager(start_time=total_stats.start_time)\\n        \\n        valid_early_stop_loss = []\\n        last_accent_valid_acc = float(\\'-inf\\')\\n        decent_round = 0\\n        for i, (batches, normalization) in enumerate(\\n                self._accum_batches(train_iter)):\\n#             if i == 0:\\n            print(\\'Testing GPU memory capability with batch shape:\\', batches[0].src[0].shape)\\n            step = self.optim.training_step\\n            if decent_round == 0:  # 如果没有处于下降过程，则保存前一个model，如果当前step下降，则一直保存这个model\\n                self.last_model_saver = deepcopy(self.model_saver)\\n                self.last_step = step\\n                self.last_moving_average = deepcopy(self.moving_average)\\n\\n            self._gradient_accumulation(\\n                batches, normalization, total_stats,\\n                report_stats)\\n\\n            if self.average_decay > 0 and i % self.average_every == 0:\\n                self._update_average(step)\\n                            \\n            # 向tensorboard输出loss\\n            if step % self.report_manager.report_every == 0:\\n                tensorboard_writer = self.report_manager.tensorboard_writer\\n                tensorboard_writer.add_scalar(\\'train/loss\\', report_stats.loss, step)\\n\\n            report_stats = self._maybe_report_training(\\n                step, train_steps,\\n                self.optim.learning_rate(),\\n                report_stats)\\n\\n            \\n            if valid_iter is not None and step % valid_steps == 0:\\n                print(\\'last accent valid acc:\\', last_accent_valid_acc)\\n                if step == valid_steps:\\n                    valid_stats = self.validate(\\n                        valid_iter, verbose=1, moving_average=self.moving_average)\\n                else:\\n                    valid_stats = self.validate(\\n                        valid_iter, moving_average=self.moving_average)\\n                valid_stats = self._maybe_gather_stats(valid_stats)\\n                tensorboard_writer.add_scalar(\\'valid/loss\\', valid_stats.loss, step)\\n#                 valid_acc = valid_stats.accuracy()\\n                valid_acc = -valid_stats.loss\\n                \\n#                 self._report_step(self.optim.learning_rate(),\\n#                                   step, valid_stats=valid_stats)\\n#                 if last_accent_valid_acc + early_stop_threshold > valid_acc:\\n#                     print(\\'meet early stop condition, prev best loss:\\', last_accent_valid_acc, \\'current loss:\\', valid_acc)\\n#                     if self.model_saver is not None:\\n#                         self.model_saver.save(step, moving_average=self.moving_average)\\n#                     return total_stats\\n                \\n#                 last_accent_valid_acc = valid_acc\\n                \\n                if valid_acc < last_accent_valid_acc + early_stop_threshold:  # 开始下降\\n                    if decent_round == 0:  # 保存最后一个上升model\\n                        print(\\'Save last accent model with acc:\\', last_accent_valid_acc)\\n                    decent_round += 1\\n                    print(\\'Decent round:\\', decent_round)\\n                else:\\n                    decent_round = 0\\n                    last_accent_valid_acc = valid_acc\\n\\n                self._report_step(self.optim.learning_rate(),\\n                                  step, valid_stats=valid_stats)\\n                if decent_round == early_stop_round:  # 停止\\n                    if self.last_model_saver is not None:\\n                        print(\\'meet early stop condition, prev best loss:\\', last_accent_valid_acc, \\'current loss:\\', valid_acc)\\n                        self.last_model_saver.save(self.last_step, moving_average=self.last_moving_average)\\n                    return total_stats\\n\\n#                 if len(valid_early_stop_loss) == early_stop_round:\\n#                     # 条件为当前valid loss要超过前n个valid loss最好的那个加threshold (X)\\n#                     # 应该允许valid loss暂时下降，但不能持续下降超过n个valid round\\n#                     # 如果持续下降，回到最好的model\\n#                     if valid_loss - max(valid_early_stop_loss) < early_stop_threshold:\\n#                         print(\\'meet early stop condition, prev best loss:\\', max(valid_early_stop_loss), \\'current loss:\\', valid_loss)\\n#                         if self.model_saver is not None:\\n#                             self.model_saver.save(step, moving_average=self.moving_average)\\n#                             self._report_step(self.optim.learning_rate(),\\n#                                               step, valid_stats=valid_stats)\\n#                         return total_stats\\n#                     else:\\n#                         valid_early_stop_loss.pop(0)\\n#                         valid_early_stop_loss.append(valid_loss)\\n\\n            if (self.model_saver is not None\\n                    and (save_checkpoint_steps != 0\\n                         and step % save_checkpoint_steps == 0)):\\n                self.model_saver.save(step, moving_average=self.moving_average)\\n\\n            if train_steps > 0 and step >= train_steps:\\n                break\\n\\n        if self.model_saver is not None:\\n            self.model_saver.save(step, moving_average=self.moving_average)\\n        return total_stats\\n    \\n    def validate(self, valid_iter, verbose=0, moving_average=None):\\n        if moving_average:\\n            valid_model = deepcopy(self.model)\\n            for avg, param in zip(self.moving_average,\\n                                  valid_model.parameters()):\\n                param.data = avg.data.half() if self.model_dtype == \"fp16\" \\\\\\n                    else avg.data\\n        else:\\n            valid_model = self.model\\n\\n        # Set model in validating mode.\\n        valid_model.eval()\\n\\n        with torch.no_grad():\\n            stats = onmt.utils.Statistics()\\n\\n            for batch in valid_iter:\\n                if verbose == 1:\\n                    print(\\'Testing GPU memory capability with batch shape:\\', batch[0].src[0].shape)\\n                src, src_lengths = batch.src if isinstance(batch.src, tuple) \\\\\\n                                   else (batch.src, None)\\n                tgt = batch.tgt\\n\\n                # F-prop through the model.\\n                outputs, attns = valid_model(src, tgt, src_lengths)\\n\\n                # Compute loss.\\n                _, batch_stats = self.valid_loss(batch, outputs, attns)\\n\\n                # Update statistics.\\n                stats.update(batch_stats)\\n\\n        if moving_average:\\n            del valid_model\\n        else:\\n            # Set model back to training mode.\\n            valid_model.train()\\n\\n        return stats',\n",
       "  \"def build_trainer(opt, device_id, model, fields, optim, model_saver=None):\\n    tgt_field = fields['tgt'][0][1].base_field\\n    train_loss = build_loss_compute(model, tgt_field, opt)\\n    valid_loss = build_loss_compute(\\n        model, tgt_field, opt, train=False)\\n\\n    trunc_size = opt.truncated_decoder  # Badly named...\\n    shard_size = opt.max_generator_batches if opt.model_dtype == 'fp32' else 0\\n    norm_method = opt.normalization\\n    grad_accum_count = opt.accum_count\\n    n_gpu = opt.world_size\\n    average_decay = opt.average_decay\\n    average_every = opt.average_every\\n    if device_id >= 0:\\n        gpu_rank = opt.gpu_ranks[device_id]\\n    else:\\n        gpu_rank = 0\\n        n_gpu = 0\\n    gpu_verbose_level = opt.gpu_verbose_level\\n\\n    report_manager = onmt.utils.build_report_manager(opt)\\n    trainer = MyTrainer(model, train_loss, valid_loss, optim, trunc_size,\\n                        shard_size, norm_method,\\n                        grad_accum_count, n_gpu, gpu_rank,\\n                        gpu_verbose_level, report_manager,\\n                        model_saver=model_saver if gpu_rank == 0 else None,\\n                        average_decay=average_decay,\\n                        average_every=average_every,\\n                        model_dtype=opt.model_dtype)\\n    return trainer\",\n",
       "  'def _check_save_model_path(opt):\\n    save_model_path = os.path.abspath(opt.save_model)\\n    model_dirname = os.path.dirname(save_model_path)\\n    if not os.path.exists(model_dirname):\\n        os.makedirs(model_dirname)\\n\\n\\ndef _tally_parameters(model):\\n    enc = 0\\n    dec = 0\\n    for name, param in model.named_parameters():\\n        if \\'encoder\\' in name:\\n            enc += param.nelement()\\n        else:\\n            dec += param.nelement()\\n    return enc + dec, enc, dec\\n\\n\\ndef training_opt_postprocessing(opt, device_id):\\n    if opt.word_vec_size != -1:\\n        opt.src_word_vec_size = opt.word_vec_size\\n        opt.tgt_word_vec_size = opt.word_vec_size\\n\\n    if opt.layers != -1:\\n        opt.enc_layers = opt.layers\\n        opt.dec_layers = opt.layers\\n\\n    if opt.rnn_size != -1:\\n        opt.enc_rnn_size = opt.rnn_size\\n        opt.dec_rnn_size = opt.rnn_size\\n\\n        # this check is here because audio allows the encoder and decoder to\\n        # be different sizes, but other model types do not yet\\n        same_size = opt.enc_rnn_size == opt.dec_rnn_size\\n        assert opt.model_type == \\'audio\\' or same_size, \\\\\\n            \"The encoder and decoder rnns must be the same size for now\"\\n\\n    opt.brnn = opt.encoder_type == \"brnn\"\\n\\n    assert opt.rnn_type != \"SRU\" or opt.gpu_ranks, \\\\\\n        \"Using SRU requires -gpu_ranks set.\"\\n\\n    if torch.cuda.is_available() and not opt.gpu_ranks:\\n        logger.info(\"WARNING: You have a CUDA device, \\\\\\n                    should run with -gpu_ranks\")\\n\\n    if device_id >= 0:\\n        torch.cuda.set_device(device_id)\\n    set_random_seed(opt.seed, device_id >= 0)\\n\\n    return opt\\n\\n\\ndef train_main(opt, device_id):\\n    opt = training_opt_postprocessing(opt, device_id)\\n    init_logger(opt.log_file)\\n    # Load checkpoint if we resume from a previous training.\\n    if opt.train_from:\\n        logger.info(\\'Loading checkpoint from %s\\' % opt.train_from)\\n        checkpoint = torch.load(opt.train_from,\\n                                map_location=lambda storage, loc: storage)\\n\\n        # Load default opts values then overwrite it with opts from\\n        # the checkpoint. It\\'s usefull in order to re-train a model\\n        # after adding a new option (not set in checkpoint)\\n        dummy_parser = configargparse.ArgumentParser()\\n        opts.model_opts(dummy_parser)\\n        default_opt = dummy_parser.parse_known_args([])[0]\\n\\n        model_opt = default_opt\\n        model_opt.__dict__.update(checkpoint[\\'opt\\'].__dict__)\\n        logger.info(\\'Loading vocab from checkpoint at %s.\\' % opt.train_from)\\n        vocab = checkpoint[\\'vocab\\']\\n    else:\\n        checkpoint = None\\n        model_opt = opt\\n        vocab = torch.load(opt.data + \\'.vocab.pt\\')\\n\\n    # check for code where vocab is saved instead of fields\\n    # (in the future this will be done in a smarter way)\\n    if old_style_vocab(vocab):\\n        data_type = opt.model_type\\n        fields = load_old_vocab(vocab, data_type, dynamic_dict=opt.copy_attn)\\n    else:\\n        fields = vocab\\n\\n    # Report src and tgt vocab sizes, including for features\\n    for side in [\\'src\\', \\'tgt\\']:\\n        for name, f in fields[side]:\\n            try:\\n                f_iter = iter(f)\\n            except TypeError:\\n                f_iter = [(name, f)]\\n            for sn, sf in f_iter:\\n                if sf.use_vocab:\\n                    logger.info(\\' * %s vocab size = %d\\' % (sn, len(sf.vocab)))\\n\\n    # Build model.\\n    model = build_model(model_opt, opt, fields, checkpoint)\\n    n_params, enc, dec = _tally_parameters(model)\\n    logger.info(\\'encoder: %d\\' % enc)\\n    logger.info(\\'decoder: %d\\' % dec)\\n    logger.info(\\'* number of parameters: %d\\' % n_params)\\n    _check_save_model_path(opt)\\n\\n    # Build optimizer.\\n    optim = Optimizer.from_opt(model, opt, checkpoint=checkpoint)\\n\\n    # Build model saver\\n    model_saver = build_model_saver(model_opt, opt, model, fields, optim)\\n\\n    trainer = build_trainer(\\n        opt, device_id, model, fields, optim, model_saver=model_saver)\\n\\n    # this line is kind of a temporary kludge because different objects expect\\n    # fields to have a different structure\\n    dataset_fields = dict(chain.from_iterable(fields.values()))\\n\\n    train_iter = build_dataset_iter(\"train\", dataset_fields, opt)\\n    valid_iter = build_dataset_iter(\\n        \"valid\", dataset_fields, opt, is_train=False)\\n\\n    if len(opt.gpu_ranks):\\n        logger.info(\\'Starting training on GPU: %s\\' % opt.gpu_ranks)\\n    else:\\n        logger.info(\\'Starting training on CPU, could be very slow\\')\\n    train_steps = opt.train_steps\\n    if opt.single_pass and train_steps > 0:\\n        logger.warning(\"Option single_pass is enabled, ignoring train_steps.\")\\n        train_steps = 0\\n    trainer.train(\\n        train_iter,\\n        train_steps,\\n        save_checkpoint_steps=opt.save_checkpoint_steps,\\n        valid_iter=valid_iter,\\n        valid_steps=opt.valid_steps,\\n        early_stop_round=opt.early_stop_round,\\n        early_stop_threshold=opt.early_stop_threshold)\\n\\n    if opt.tensorboard:\\n        trainer.report_manager.tensorboard_writer.close()',\n",
       "  'class BaseRunner:\\n    \\n    def __init__(self, data_path):\\n        self.data_path = data_path\\n        self.model_id = None\\n    \\n    def run(self, option, model_id=None, train_from=None):\\n        model_id = model_id if model_id is not None else BaseRunner.generate_id()\\n        self.model_id = model_id\\n        self.prepare_save_file()\\n        arg_dict = {\\n            \\'data\\': self.data_path,\\n            \\'save_model\\': self.model_path,\\n            \\'gpu_ranks\\': 0,\\n            \\'tensorboard\\': None,\\n            \\'tensorboard_log_dir\\': self.log_path + \\'/log\\'\\n        }\\n        arg_dict.update(option)\\n        if train_from is not None:\\n            arg_dict[\\'train_from\\'] = train_from\\n        ArgumentHelper.save_args(arg_dict, self.model_path)\\n        parser = configargparse.ArgumentParser(\\n            description=\\'train.py\\',\\n            formatter_class=configargparse.ArgumentDefaultsHelpFormatter)\\n\\n        opts.add_md_help_argument(parser)\\n        opts.model_opts(parser)\\n        opts.train_opts(parser)\\n        parser.add_argument(\\'-early_stop_round\\', type=int, default=10000, help=\\'Early stop round\\')\\n        parser.add_argument(\\'-early_stop_threshold\\', type=float, default=1, help=\\'Early stop threshold\\')\\n\\n        opt = parser.parse_args(ArgumentHelper.convert_dict_to_arg_array(arg_dict))\\n        train_main(opt, 0)\\n\\n    def translate_main(self, opt, logger, model_id):\\n        translator = build_translator(opt, report_score=True)\\n        src_shards = split_corpus(opt.src, opt.shard_size)\\n        tgt_shards = split_corpus(opt.tgt, opt.shard_size) \\\\\\n            if opt.tgt is not None else [None]*opt.shard_size\\n        shard_pairs = zip(src_shards, tgt_shards)\\n\\n        for i, (src_shard, tgt_shard) in enumerate(shard_pairs):\\n            logger.info(\"Translating shard %d.\" % i)\\n            translator.translate(\\n                src=src_shard,\\n                tgt=tgt_shard,\\n                src_dir=opt.src_dir,\\n                batch_size=opt.batch_size,\\n                attn_debug=opt.attn_debug\\n            )\\n\\n    def translate(self, args, model_id=None):\\n        model_id = model_id if model_id is not None else self.model_id\\n        if model_id is None:\\n            raise Exception()\\n        self.model_id = model_id\\n        self.prepare_save_file()\\n        args[\\'model\\'] = self.model_path + \\'_step_\\' + str(BaseRunner.get_latest_model(\\n                                                        self.model_dir,\\n                                                        self.model_prefix,\\n                                                        self.model_id)) + \\'.pt\\'\\n        parser = configargparse.ArgumentParser(\\n            description=\\'translate.py\\',\\n            config_file_parser_class=configargparse.YAMLConfigFileParser,\\n            formatter_class=configargparse.ArgumentDefaultsHelpFormatter)\\n        opts.config_opts(parser)\\n        opts.add_md_help_argument(parser)\\n        opts.translate_opts(parser)\\n\\n        opt = parser.parse_args(ArgumentHelper.convert_dict_to_arg_array(args))\\n        logger = init_logger(opt.log_file)\\n        self.translate_main(opt, logger, model_id)\\n\\n    @staticmethod\\n    def generate_id():\\n        return hashlib.md5(str(uuid.uuid4()).encode()).hexdigest()\\n    \\n    @staticmethod\\n    def get_latest_model(model_dir, model_prefix, model_id):\\n        all_steps = []\\n        for root, dirs, files in os.walk(model_dir):\\n            path = root.split(os.sep)\\n            for file in files:\\n                absolute_filename = os.path.join(root, file)\\n                if model_id in absolute_filename:\\n                    steps = re.findall(model_prefix + model_id + \\'_step_(\\\\d+)\\\\.pt\\', absolute_filename)\\n                    if len(steps) > 0:\\n                        all_steps.append(int(steps[0]))\\n        return max(all_steps)\\n\\n    def prepare_save_file(self):\\n        model_id = self.model_id\\n        model_dir = \\'/mnt/drive-2t/model/\\'\\n        if not os.path.exists(model_dir):\\n            os.makedirs(model_dir)\\n        model_prefix = \\'opennmt_\\'\\n        model_path = model_dir + model_prefix + model_id\\n        log_dir = \\'/mnt/drive-2t/log/\\'\\n        if not os.path.exists(log_dir):\\n            os.makedirs(log_dir)\\n        log_prefix = model_prefix\\n        log_path = log_dir + log_prefix + model_id\\n        if not os.path.exists(log_path):\\n            os.mkdir(log_path)\\n        self.model_path = model_path\\n        self.log_path = log_path\\n        self.model_prefix = model_prefix\\n        self.model_dir = model_dir\\n\\n\\nrunner = BaseRunner(\\'data/nmt15\\')',\n",
       "  \"reverse_runner = BaseRunner('data/nmt15-reverse')\",\n",
       "  \"model_args = {\\n    'model_dtype': 'fp32',\\n    'encoder_type': 'transformer',\\n    'decoder_type': 'transformer',\\n    'layers': 6,\\n    'position_encoding': None,\\n    'max_generator_batches': 2,\\n    'dropout': 0.1,\\n    'enc_rnn_size': 512,\\n    'dec_rnn_size': 512,\\n    'src_word_vec_size': 512,\\n    'tgt_word_vec_size': 512,\\n    'accum_count': 4\\n}\\ntrain_args = {\\n    'batch_size': 16,\\n    'train_steps': 100000,\\n    'optim': 'adam',\\n    'learning_rate': 0.001,\\n    'valid_step': 100,\\n    'valid_batch': 32,\\n    'early_stop_round': 5,\\n    'early_stop_threshold': 0.01,\\n    'report_every': 100\\n}\\ntotal_args = {}\\ntotal_args.update(model_args)\\ntotal_args.update(train_args)\\n\\nreverse_runner.run(total_args)\\nreverse_runner.model_id\",\n",
       "  'globals()'],\n",
       " 'Out': {},\n",
       " 'get_ipython': <bound method InteractiveShell.get_ipython of <ipykernel.zmqshell.ZMQInteractiveShell object at 0x7f979abd48d0>>,\n",
       " 'exit': <IPython.core.autocall.ZMQExitAutocall at 0x7f97986e38d0>,\n",
       " 'quit': <IPython.core.autocall.ZMQExitAutocall at 0x7f97986e38d0>,\n",
       " '_': '',\n",
       " '__': '',\n",
       " '___': '',\n",
       " '_i': \"model_args = {\\n    'model_dtype': 'fp32',\\n    'encoder_type': 'transformer',\\n    'decoder_type': 'transformer',\\n    'layers': 6,\\n    'position_encoding': None,\\n    'max_generator_batches': 2,\\n    'dropout': 0.1,\\n    'enc_rnn_size': 512,\\n    'dec_rnn_size': 512,\\n    'src_word_vec_size': 512,\\n    'tgt_word_vec_size': 512,\\n    'accum_count': 4\\n}\\ntrain_args = {\\n    'batch_size': 16,\\n    'train_steps': 100000,\\n    'optim': 'adam',\\n    'learning_rate': 0.001,\\n    'valid_step': 100,\\n    'valid_batch': 32,\\n    'early_stop_round': 5,\\n    'early_stop_threshold': 0.01,\\n    'report_every': 100\\n}\\ntotal_args = {}\\ntotal_args.update(model_args)\\ntotal_args.update(train_args)\\n\\nreverse_runner.run(total_args)\\nreverse_runner.model_id\",\n",
       " '_ii': \"reverse_runner = BaseRunner('data/nmt15-reverse')\",\n",
       " '_iii': 'class BaseRunner:\\n    \\n    def __init__(self, data_path):\\n        self.data_path = data_path\\n        self.model_id = None\\n    \\n    def run(self, option, model_id=None, train_from=None):\\n        model_id = model_id if model_id is not None else BaseRunner.generate_id()\\n        self.model_id = model_id\\n        self.prepare_save_file()\\n        arg_dict = {\\n            \\'data\\': self.data_path,\\n            \\'save_model\\': self.model_path,\\n            \\'gpu_ranks\\': 0,\\n            \\'tensorboard\\': None,\\n            \\'tensorboard_log_dir\\': self.log_path + \\'/log\\'\\n        }\\n        arg_dict.update(option)\\n        if train_from is not None:\\n            arg_dict[\\'train_from\\'] = train_from\\n        ArgumentHelper.save_args(arg_dict, self.model_path)\\n        parser = configargparse.ArgumentParser(\\n            description=\\'train.py\\',\\n            formatter_class=configargparse.ArgumentDefaultsHelpFormatter)\\n\\n        opts.add_md_help_argument(parser)\\n        opts.model_opts(parser)\\n        opts.train_opts(parser)\\n        parser.add_argument(\\'-early_stop_round\\', type=int, default=10000, help=\\'Early stop round\\')\\n        parser.add_argument(\\'-early_stop_threshold\\', type=float, default=1, help=\\'Early stop threshold\\')\\n\\n        opt = parser.parse_args(ArgumentHelper.convert_dict_to_arg_array(arg_dict))\\n        train_main(opt, 0)\\n\\n    def translate_main(self, opt, logger, model_id):\\n        translator = build_translator(opt, report_score=True)\\n        src_shards = split_corpus(opt.src, opt.shard_size)\\n        tgt_shards = split_corpus(opt.tgt, opt.shard_size) \\\\\\n            if opt.tgt is not None else [None]*opt.shard_size\\n        shard_pairs = zip(src_shards, tgt_shards)\\n\\n        for i, (src_shard, tgt_shard) in enumerate(shard_pairs):\\n            logger.info(\"Translating shard %d.\" % i)\\n            translator.translate(\\n                src=src_shard,\\n                tgt=tgt_shard,\\n                src_dir=opt.src_dir,\\n                batch_size=opt.batch_size,\\n                attn_debug=opt.attn_debug\\n            )\\n\\n    def translate(self, args, model_id=None):\\n        model_id = model_id if model_id is not None else self.model_id\\n        if model_id is None:\\n            raise Exception()\\n        self.model_id = model_id\\n        self.prepare_save_file()\\n        args[\\'model\\'] = self.model_path + \\'_step_\\' + str(BaseRunner.get_latest_model(\\n                                                        self.model_dir,\\n                                                        self.model_prefix,\\n                                                        self.model_id)) + \\'.pt\\'\\n        parser = configargparse.ArgumentParser(\\n            description=\\'translate.py\\',\\n            config_file_parser_class=configargparse.YAMLConfigFileParser,\\n            formatter_class=configargparse.ArgumentDefaultsHelpFormatter)\\n        opts.config_opts(parser)\\n        opts.add_md_help_argument(parser)\\n        opts.translate_opts(parser)\\n\\n        opt = parser.parse_args(ArgumentHelper.convert_dict_to_arg_array(args))\\n        logger = init_logger(opt.log_file)\\n        self.translate_main(opt, logger, model_id)\\n\\n    @staticmethod\\n    def generate_id():\\n        return hashlib.md5(str(uuid.uuid4()).encode()).hexdigest()\\n    \\n    @staticmethod\\n    def get_latest_model(model_dir, model_prefix, model_id):\\n        all_steps = []\\n        for root, dirs, files in os.walk(model_dir):\\n            path = root.split(os.sep)\\n            for file in files:\\n                absolute_filename = os.path.join(root, file)\\n                if model_id in absolute_filename:\\n                    steps = re.findall(model_prefix + model_id + \\'_step_(\\\\d+)\\\\.pt\\', absolute_filename)\\n                    if len(steps) > 0:\\n                        all_steps.append(int(steps[0]))\\n        return max(all_steps)\\n\\n    def prepare_save_file(self):\\n        model_id = self.model_id\\n        model_dir = \\'/mnt/drive-2t/model/\\'\\n        if not os.path.exists(model_dir):\\n            os.makedirs(model_dir)\\n        model_prefix = \\'opennmt_\\'\\n        model_path = model_dir + model_prefix + model_id\\n        log_dir = \\'/mnt/drive-2t/log/\\'\\n        if not os.path.exists(log_dir):\\n            os.makedirs(log_dir)\\n        log_prefix = model_prefix\\n        log_path = log_dir + log_prefix + model_id\\n        if not os.path.exists(log_path):\\n            os.mkdir(log_path)\\n        self.model_path = model_path\\n        self.log_path = log_path\\n        self.model_prefix = model_prefix\\n        self.model_dir = model_dir\\n\\n\\nrunner = BaseRunner(\\'data/nmt15\\')',\n",
       " '_i1': 'import onmt\\nfrom onmt.trainer import Trainer\\nfrom onmt.utils.logging import logger\\nfrom onmt.utils.loss import build_loss_compute\\nimport onmt.opts as opts\\nfrom onmt.inputters.inputter import build_dataset_iter, \\\\\\n    load_old_vocab, old_style_vocab\\nfrom onmt.model_builder import build_model\\nfrom onmt.utils.optimizers import Optimizer\\nfrom onmt.utils.misc import set_random_seed\\nfrom onmt.models import build_model_saver\\nfrom onmt.utils.logging import init_logger, logger\\nfrom onmt.utils.misc import split_corpus\\nfrom onmt.translate.translator import build_translator\\n\\nimport itertools\\nimport configargparse\\nimport os\\nfrom itertools import chain\\nimport torch\\nimport subprocess, shlex, re, uuid, hashlib, json, gc, shutil, sys\\nfrom pprint import pprint\\nfrom copy import deepcopy',\n",
       " 'onmt': <module 'onmt' from '/home/ray/OpenNMT-py/onmt/__init__.py'>,\n",
       " 'Trainer': onmt.trainer.Trainer,\n",
       " 'logger': <RootLogger root (INFO)>,\n",
       " 'build_loss_compute': <function onmt.utils.loss.build_loss_compute(model, tgt_field, opt, train=True)>,\n",
       " 'opts': <module 'onmt.opts' from '/home/ray/OpenNMT-py/onmt/opts.py'>,\n",
       " 'build_dataset_iter': <function onmt.inputters.inputter.build_dataset_iter(corpus_type, fields, opt, is_train=True)>,\n",
       " 'load_old_vocab': <function onmt.inputters.inputter.load_old_vocab(vocab, data_type='text', dynamic_dict=False)>,\n",
       " 'old_style_vocab': <function onmt.inputters.inputter.old_style_vocab(vocab)>,\n",
       " 'build_model': <function onmt.model_builder.build_model(model_opt, opt, fields, checkpoint)>,\n",
       " 'Optimizer': onmt.utils.optimizers.Optimizer,\n",
       " 'set_random_seed': <function onmt.utils.misc.set_random_seed(seed, is_cuda)>,\n",
       " 'build_model_saver': <function onmt.models.model_saver.build_model_saver(model_opt, opt, model, fields, optim)>,\n",
       " 'init_logger': <function onmt.utils.logging.init_logger(log_file=None, log_file_level=0)>,\n",
       " 'split_corpus': <function onmt.utils.misc.split_corpus(path, shard_size)>,\n",
       " 'build_translator': <function onmt.translate.translator.build_translator(opt, report_score=True, logger=None, out_file=None)>,\n",
       " 'itertools': <module 'itertools' (built-in)>,\n",
       " 'configargparse': <module 'configargparse' from '/home/ray/.conda/envs/dl/lib/python3.6/site-packages/configargparse.py'>,\n",
       " 'os': <module 'os' from '/home/ray/.conda/envs/dl/lib/python3.6/os.py'>,\n",
       " 'chain': itertools.chain,\n",
       " 'torch': <module 'torch' from '/home/ray/.conda/envs/dl/lib/python3.6/site-packages/torch/__init__.py'>,\n",
       " 'subprocess': <module 'subprocess' from '/home/ray/.conda/envs/dl/lib/python3.6/subprocess.py'>,\n",
       " 'shlex': <module 'shlex' from '/home/ray/.conda/envs/dl/lib/python3.6/shlex.py'>,\n",
       " 're': <module 're' from '/home/ray/.conda/envs/dl/lib/python3.6/re.py'>,\n",
       " 'uuid': <module 'uuid' from '/home/ray/.conda/envs/dl/lib/python3.6/uuid.py'>,\n",
       " 'hashlib': <module 'hashlib' from '/home/ray/.conda/envs/dl/lib/python3.6/hashlib.py'>,\n",
       " 'json': <module 'json' from '/home/ray/.conda/envs/dl/lib/python3.6/json/__init__.py'>,\n",
       " 'gc': <module 'gc' (built-in)>,\n",
       " 'shutil': <module 'shutil' from '/home/ray/.conda/envs/dl/lib/python3.6/shutil.py'>,\n",
       " 'sys': <module 'sys' (built-in)>,\n",
       " 'pprint': <function pprint.pprint(object, stream=None, indent=1, width=80, depth=None, *, compact=False)>,\n",
       " 'deepcopy': <function copy.deepcopy(x, memo=None, _nil=[])>,\n",
       " '_i2': \"class ArgumentHelper:\\n    def build(self, arg_dict):\\n        arg_total = self.base_arg_dict.copy()\\n        arg_total.update(arg_dict)\\n        return arg_total\\n    \\n    @staticmethod\\n    def save_args(arg_dict, filename):\\n        with open(filename + '.txt', 'w') as f:\\n            json.dump(arg_dict, f, indent=4)\\n    \\n    @staticmethod\\n    def convert_dict_to_arg_array(arg_dict):\\n        arr = []\\n        for k, v in arg_dict.items():\\n            arr.append('-' + k)\\n            if v is not None:\\n                arr.append(str(v))\\n        return arr\",\n",
       " 'ArgumentHelper': __main__.ArgumentHelper,\n",
       " '_i3': 'def cleanup():\\n    torch.cuda.empty_cache()\\n    gc.collect()',\n",
       " 'cleanup': <function __main__.cleanup()>,\n",
       " '_i4': \"def build_translate_args(model_id):\\n    return {\\n        'src': 'data/nmt15/test_en',\\n        'tgt': 'data/nmt15/test_vi',\\n        'out': '/mnt/drive-2t/pred/opennmt_' + model_id + '.txt',\\n        'replace_unk': None,\\n        'gpu': '0',\\n        'batch_size': '16'\\n    }\\n\\ndef build_translate_args_reverse(model_id):\\n    return {\\n        'src': 'data/nmt15/test_vi',\\n        'tgt': 'data/nmt15/test_en',\\n        'out': '/mnt/drive-2t/pred/opennmt_' + model_id + '.txt',\\n        'replace_unk': None,\\n        'gpu': '0',\\n        'batch_size': '16'\\n    }\",\n",
       " 'build_translate_args': <function __main__.build_translate_args(model_id)>,\n",
       " 'build_translate_args_reverse': <function __main__.build_translate_args_reverse(model_id)>,\n",
       " '_i5': \"def clean_path_with_no_checkpoint(model_path, log_path):\\n    useless_file = []\\n    available_model = set()\\n    useless_log = []\\n    for root, dirs, files in os.walk(model_path):\\n        path = root.split(os.sep)\\n        for file in files:\\n            absolute_filename = os.path.join(root, file)\\n            model_id = re.findall(r'opennmt_(.+)_step_\\\\d+\\\\.pt', file)\\n            if len(model_id) != 0:\\n                available_model.add(model_id[0])\\n    \\n    for root, dirs, files in os.walk(model_path):\\n        path = root.split(os.sep)\\n        for file in files:\\n            absolute_filename = os.path.join(root, file)\\n            model_id = re.findall(r'opennmt_(.+).txt', file)\\n            if len(model_id) != 0 and model_id[0] not in available_model:\\n                useless_file.append(absolute_filename)\\n    \\n    dirs = os.listdir(log_path)\\n    for d in dirs:\\n        model_id = re.findall(r'opennmt_(.+)', d)\\n        if len(model_id) != 0 and model_id[0] not in available_model:\\n            useless_log.append(os.path.join(log_path, d))\\n    \\n#     print(available_model)\\n#     print(useless_file)\\n#     print(useless_log)\\n    for f in useless_file:\\n        os.remove(f)\\n    for f in useless_log:\\n        print(f)\\n        !rm -rf $f\\n\\n# clean_path_with_no_checkpoint('/mnt/drive-2t/model', '/mnt/drive-2t/log')\",\n",
       " 'clean_path_with_no_checkpoint': <function __main__.clean_path_with_no_checkpoint(model_path, log_path)>,\n",
       " '_i6': \"def evaluate_human(model_id):\\n    model_args = '/mnt/drive-2t/model/opennmt_' + model_id + '.txt'\\n    !cat $model_args\\n    pred = '/mnt/drive-2t/pred/opennmt_' + model_id + '.txt'\\n    ref = 'data/nmt15/test_en'\\n    print()\\n    print(bleu_evaluate(pred, ref), bleu_evaluate(pred, ref, '0.25 0.25 0.25 0.25'))\\n    pred_head = !head $pred\\n    ref_head = !head $ref\\n    pairs = list(zip(pred_head, ref_head))\\n    for pair in pairs:\\n        print(pair[0])\\n        print()\\n        print(pair[1])\\n        print('-------------')\",\n",
       " 'evaluate_human': <function __main__.evaluate_human(model_id)>,\n",
       " '_i7': \"python_interpreter = '/home/ray/.conda/envs/dl/bin/python'\",\n",
       " 'python_interpreter': '/home/ray/.conda/envs/dl/bin/python',\n",
       " '_i8': \"bleu_script = '/home/ray/Smooth_BLEU/bleu.py'\\nnltk_script = '/home/ray/Smooth_BLEU/nltk_bleu.py'\\nbleu_pred = '/home/ray/OpenNMT-py/data/nmt15/pred.txt'\\nbleu_reference = '/home/ray/OpenNMT-py/data/nmt15/test_vi'\\nbleu_weight = '0.25 0.25 0.25'\",\n",
       " 'bleu_script': '/home/ray/Smooth_BLEU/bleu.py',\n",
       " 'nltk_script': '/home/ray/Smooth_BLEU/nltk_bleu.py',\n",
       " 'bleu_pred': '/home/ray/OpenNMT-py/data/nmt15/pred.txt',\n",
       " 'bleu_reference': '/home/ray/OpenNMT-py/data/nmt15/test_vi',\n",
       " 'bleu_weight': '0.25 0.25 0.25',\n",
       " '_i9': \"def smooth_bleu_evaluate(pred, reference, weight=bleu_weight):\\n    p = subprocess.Popen([python_interpreter, bleu_script,\\n                          '-t', pred,\\n                          '-r', reference,\\n                          '-w', weight], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\\n    p.wait()\\n    out, err = p.communicate()\\n    if p.returncode != 0:\\n        print(err)\\n        raise Exception()\\n    out = out.decode()\\n    err = err.decode()\\n    bleu_score = re.findall(r'BLEU = ([1-9\\\\.]+)', err)\\n    return float(bleu_score[0])\\n\\ndef nltk_bleu_evaluate(pred, reference, weight=bleu_weight):\\n    p = subprocess.Popen([python_interpreter, nltk_script,\\n                          '-t', pred,\\n                          '-r', reference,\\n                          '-w', weight], stdout=subprocess.PIPE, stderr=subprocess.PIPE)\\n    p.wait()\\n    out, err = p.communicate()\\n    if p.returncode != 0:\\n        print(err)\\n        raise Exception()\\n    out = out.decode()\\n    err = err.decode()\\n    bleu_score = re.findall(r'BLEU = ([1-9\\\\.]+)', err)\\n    return float(bleu_score[0])\\n\\ndef bleu_evaluate(pred, reference, weight=bleu_weight):\\n    smooth = smooth_bleu_evaluate(pred, reference, weight)\\n#     nltk_ = nltk_bleu_evaluate(pred, reference, weight)\\n#     return smooth, nltk_\\n    return smooth\",\n",
       " 'smooth_bleu_evaluate': <function __main__.smooth_bleu_evaluate(pred, reference, weight='0.25 0.25 0.25')>,\n",
       " 'nltk_bleu_evaluate': <function __main__.nltk_bleu_evaluate(pred, reference, weight='0.25 0.25 0.25')>,\n",
       " 'bleu_evaluate': <function __main__.bleu_evaluate(pred, reference, weight='0.25 0.25 0.25')>,\n",
       " '_i10': \"# searchable hyperparameters\\nembedding_size = ['64', '128', '256', '512']\\nencoder_type = ['rnn', 'brnn', 'mean', 'transformer']\\ndecoder_type = encoder_type\\nlayers = ['1', '2', '3', '4', '5', '6']\\nrnn_size = ['64', '128', '256', '512']\\nrnn_type = ['LSTM', 'GRU', 'SRU']\\nglobal_attention = ['dot', 'general', 'mlp']\\nself_attention_type = ['scaled_dot', 'average']\\nheads = [2, 4, 6, 8, 10]\\ntransformer_ff = [1024, 2048, 4096]\\n\\nnormalization = ['sents', 'tokens']\\nvalid_steps = 1000\\ntrain_steps = 10000\\ndropout = 0.1\",\n",
       " 'embedding_size': ['64', '128', '256', '512'],\n",
       " 'encoder_type': ['rnn', 'brnn', 'mean', 'transformer'],\n",
       " 'decoder_type': ['rnn', 'brnn', 'mean', 'transformer'],\n",
       " 'layers': ['1', '2', '3', '4', '5', '6'],\n",
       " 'rnn_size': ['64', '128', '256', '512'],\n",
       " 'rnn_type': ['LSTM', 'GRU', 'SRU'],\n",
       " 'global_attention': ['dot', 'general', 'mlp'],\n",
       " 'self_attention_type': ['scaled_dot', 'average'],\n",
       " 'heads': [2, 4, 6, 8, 10],\n",
       " 'transformer_ff': [1024, 2048, 4096],\n",
       " 'normalization': ['sents', 'tokens'],\n",
       " 'valid_steps': 1000,\n",
       " 'train_steps': 10000,\n",
       " 'dropout': 0.1,\n",
       " '_i11': 'class MyTrainer(Trainer):\\n    def train(self,\\n              train_iter,\\n              train_steps,\\n              save_checkpoint_steps=5000,\\n              valid_iter=None,\\n              valid_steps=10000,\\n              early_stop_round=50000,\\n              early_stop_threshold=1):\\n        self.last_model_saver = None\\n        self.last_step = None\\n        self.last_moving_average = None\\n        if valid_iter is None:\\n            logger.info(\\'Start training loop without validation...\\')\\n        else:\\n            logger.info(\\'Start training loop and validate every %d steps...\\',\\n                        valid_steps)\\n\\n        total_stats = onmt.utils.Statistics()\\n        report_stats = onmt.utils.Statistics()\\n        self._start_report_manager(start_time=total_stats.start_time)\\n        \\n        valid_early_stop_loss = []\\n        last_accent_valid_acc = float(\\'-inf\\')\\n        decent_round = 0\\n        for i, (batches, normalization) in enumerate(\\n                self._accum_batches(train_iter)):\\n#             if i == 0:\\n            print(\\'Testing GPU memory capability with batch shape:\\', batches[0].src[0].shape)\\n            step = self.optim.training_step\\n            if decent_round == 0:  # 如果没有处于下降过程，则保存前一个model，如果当前step下降，则一直保存这个model\\n                self.last_model_saver = deepcopy(self.model_saver)\\n                self.last_step = step\\n                self.last_moving_average = deepcopy(self.moving_average)\\n\\n            self._gradient_accumulation(\\n                batches, normalization, total_stats,\\n                report_stats)\\n\\n            if self.average_decay > 0 and i % self.average_every == 0:\\n                self._update_average(step)\\n                            \\n            # 向tensorboard输出loss\\n            if step % self.report_manager.report_every == 0:\\n                tensorboard_writer = self.report_manager.tensorboard_writer\\n                tensorboard_writer.add_scalar(\\'train/loss\\', report_stats.loss, step)\\n\\n            report_stats = self._maybe_report_training(\\n                step, train_steps,\\n                self.optim.learning_rate(),\\n                report_stats)\\n\\n            \\n            if valid_iter is not None and step % valid_steps == 0:\\n                print(\\'last accent valid acc:\\', last_accent_valid_acc)\\n                if step == valid_steps:\\n                    valid_stats = self.validate(\\n                        valid_iter, verbose=1, moving_average=self.moving_average)\\n                else:\\n                    valid_stats = self.validate(\\n                        valid_iter, moving_average=self.moving_average)\\n                valid_stats = self._maybe_gather_stats(valid_stats)\\n                tensorboard_writer.add_scalar(\\'valid/loss\\', valid_stats.loss, step)\\n#                 valid_acc = valid_stats.accuracy()\\n                valid_acc = -valid_stats.loss\\n                \\n#                 self._report_step(self.optim.learning_rate(),\\n#                                   step, valid_stats=valid_stats)\\n#                 if last_accent_valid_acc + early_stop_threshold > valid_acc:\\n#                     print(\\'meet early stop condition, prev best loss:\\', last_accent_valid_acc, \\'current loss:\\', valid_acc)\\n#                     if self.model_saver is not None:\\n#                         self.model_saver.save(step, moving_average=self.moving_average)\\n#                     return total_stats\\n                \\n#                 last_accent_valid_acc = valid_acc\\n                \\n                if valid_acc < last_accent_valid_acc + early_stop_threshold:  # 开始下降\\n                    if decent_round == 0:  # 保存最后一个上升model\\n                        print(\\'Save last accent model with acc:\\', last_accent_valid_acc)\\n                    decent_round += 1\\n                    print(\\'Decent round:\\', decent_round)\\n                else:\\n                    decent_round = 0\\n                    last_accent_valid_acc = valid_acc\\n\\n                self._report_step(self.optim.learning_rate(),\\n                                  step, valid_stats=valid_stats)\\n                if decent_round == early_stop_round:  # 停止\\n                    if self.last_model_saver is not None:\\n                        print(\\'meet early stop condition, prev best loss:\\', last_accent_valid_acc, \\'current loss:\\', valid_acc)\\n                        self.last_model_saver.save(self.last_step, moving_average=self.last_moving_average)\\n                    return total_stats\\n\\n#                 if len(valid_early_stop_loss) == early_stop_round:\\n#                     # 条件为当前valid loss要超过前n个valid loss最好的那个加threshold (X)\\n#                     # 应该允许valid loss暂时下降，但不能持续下降超过n个valid round\\n#                     # 如果持续下降，回到最好的model\\n#                     if valid_loss - max(valid_early_stop_loss) < early_stop_threshold:\\n#                         print(\\'meet early stop condition, prev best loss:\\', max(valid_early_stop_loss), \\'current loss:\\', valid_loss)\\n#                         if self.model_saver is not None:\\n#                             self.model_saver.save(step, moving_average=self.moving_average)\\n#                             self._report_step(self.optim.learning_rate(),\\n#                                               step, valid_stats=valid_stats)\\n#                         return total_stats\\n#                     else:\\n#                         valid_early_stop_loss.pop(0)\\n#                         valid_early_stop_loss.append(valid_loss)\\n\\n            if (self.model_saver is not None\\n                    and (save_checkpoint_steps != 0\\n                         and step % save_checkpoint_steps == 0)):\\n                self.model_saver.save(step, moving_average=self.moving_average)\\n\\n            if train_steps > 0 and step >= train_steps:\\n                break\\n\\n        if self.model_saver is not None:\\n            self.model_saver.save(step, moving_average=self.moving_average)\\n        return total_stats\\n    \\n    def validate(self, valid_iter, verbose=0, moving_average=None):\\n        if moving_average:\\n            valid_model = deepcopy(self.model)\\n            for avg, param in zip(self.moving_average,\\n                                  valid_model.parameters()):\\n                param.data = avg.data.half() if self.model_dtype == \"fp16\" \\\\\\n                    else avg.data\\n        else:\\n            valid_model = self.model\\n\\n        # Set model in validating mode.\\n        valid_model.eval()\\n\\n        with torch.no_grad():\\n            stats = onmt.utils.Statistics()\\n\\n            for batch in valid_iter:\\n                if verbose == 1:\\n                    print(\\'Testing GPU memory capability with batch shape:\\', batch[0].src[0].shape)\\n                src, src_lengths = batch.src if isinstance(batch.src, tuple) \\\\\\n                                   else (batch.src, None)\\n                tgt = batch.tgt\\n\\n                # F-prop through the model.\\n                outputs, attns = valid_model(src, tgt, src_lengths)\\n\\n                # Compute loss.\\n                _, batch_stats = self.valid_loss(batch, outputs, attns)\\n\\n                # Update statistics.\\n                stats.update(batch_stats)\\n\\n        if moving_average:\\n            del valid_model\\n        else:\\n            # Set model back to training mode.\\n            valid_model.train()\\n\\n        return stats',\n",
       " 'MyTrainer': __main__.MyTrainer,\n",
       " '_i12': \"def build_trainer(opt, device_id, model, fields, optim, model_saver=None):\\n    tgt_field = fields['tgt'][0][1].base_field\\n    train_loss = build_loss_compute(model, tgt_field, opt)\\n    valid_loss = build_loss_compute(\\n        model, tgt_field, opt, train=False)\\n\\n    trunc_size = opt.truncated_decoder  # Badly named...\\n    shard_size = opt.max_generator_batches if opt.model_dtype == 'fp32' else 0\\n    norm_method = opt.normalization\\n    grad_accum_count = opt.accum_count\\n    n_gpu = opt.world_size\\n    average_decay = opt.average_decay\\n    average_every = opt.average_every\\n    if device_id >= 0:\\n        gpu_rank = opt.gpu_ranks[device_id]\\n    else:\\n        gpu_rank = 0\\n        n_gpu = 0\\n    gpu_verbose_level = opt.gpu_verbose_level\\n\\n    report_manager = onmt.utils.build_report_manager(opt)\\n    trainer = MyTrainer(model, train_loss, valid_loss, optim, trunc_size,\\n                        shard_size, norm_method,\\n                        grad_accum_count, n_gpu, gpu_rank,\\n                        gpu_verbose_level, report_manager,\\n                        model_saver=model_saver if gpu_rank == 0 else None,\\n                        average_decay=average_decay,\\n                        average_every=average_every,\\n                        model_dtype=opt.model_dtype)\\n    return trainer\",\n",
       " 'build_trainer': <function __main__.build_trainer(opt, device_id, model, fields, optim, model_saver=None)>,\n",
       " '_i13': 'def _check_save_model_path(opt):\\n    save_model_path = os.path.abspath(opt.save_model)\\n    model_dirname = os.path.dirname(save_model_path)\\n    if not os.path.exists(model_dirname):\\n        os.makedirs(model_dirname)\\n\\n\\ndef _tally_parameters(model):\\n    enc = 0\\n    dec = 0\\n    for name, param in model.named_parameters():\\n        if \\'encoder\\' in name:\\n            enc += param.nelement()\\n        else:\\n            dec += param.nelement()\\n    return enc + dec, enc, dec\\n\\n\\ndef training_opt_postprocessing(opt, device_id):\\n    if opt.word_vec_size != -1:\\n        opt.src_word_vec_size = opt.word_vec_size\\n        opt.tgt_word_vec_size = opt.word_vec_size\\n\\n    if opt.layers != -1:\\n        opt.enc_layers = opt.layers\\n        opt.dec_layers = opt.layers\\n\\n    if opt.rnn_size != -1:\\n        opt.enc_rnn_size = opt.rnn_size\\n        opt.dec_rnn_size = opt.rnn_size\\n\\n        # this check is here because audio allows the encoder and decoder to\\n        # be different sizes, but other model types do not yet\\n        same_size = opt.enc_rnn_size == opt.dec_rnn_size\\n        assert opt.model_type == \\'audio\\' or same_size, \\\\\\n            \"The encoder and decoder rnns must be the same size for now\"\\n\\n    opt.brnn = opt.encoder_type == \"brnn\"\\n\\n    assert opt.rnn_type != \"SRU\" or opt.gpu_ranks, \\\\\\n        \"Using SRU requires -gpu_ranks set.\"\\n\\n    if torch.cuda.is_available() and not opt.gpu_ranks:\\n        logger.info(\"WARNING: You have a CUDA device, \\\\\\n                    should run with -gpu_ranks\")\\n\\n    if device_id >= 0:\\n        torch.cuda.set_device(device_id)\\n    set_random_seed(opt.seed, device_id >= 0)\\n\\n    return opt\\n\\n\\ndef train_main(opt, device_id):\\n    opt = training_opt_postprocessing(opt, device_id)\\n    init_logger(opt.log_file)\\n    # Load checkpoint if we resume from a previous training.\\n    if opt.train_from:\\n        logger.info(\\'Loading checkpoint from %s\\' % opt.train_from)\\n        checkpoint = torch.load(opt.train_from,\\n                                map_location=lambda storage, loc: storage)\\n\\n        # Load default opts values then overwrite it with opts from\\n        # the checkpoint. It\\'s usefull in order to re-train a model\\n        # after adding a new option (not set in checkpoint)\\n        dummy_parser = configargparse.ArgumentParser()\\n        opts.model_opts(dummy_parser)\\n        default_opt = dummy_parser.parse_known_args([])[0]\\n\\n        model_opt = default_opt\\n        model_opt.__dict__.update(checkpoint[\\'opt\\'].__dict__)\\n        logger.info(\\'Loading vocab from checkpoint at %s.\\' % opt.train_from)\\n        vocab = checkpoint[\\'vocab\\']\\n    else:\\n        checkpoint = None\\n        model_opt = opt\\n        vocab = torch.load(opt.data + \\'.vocab.pt\\')\\n\\n    # check for code where vocab is saved instead of fields\\n    # (in the future this will be done in a smarter way)\\n    if old_style_vocab(vocab):\\n        data_type = opt.model_type\\n        fields = load_old_vocab(vocab, data_type, dynamic_dict=opt.copy_attn)\\n    else:\\n        fields = vocab\\n\\n    # Report src and tgt vocab sizes, including for features\\n    for side in [\\'src\\', \\'tgt\\']:\\n        for name, f in fields[side]:\\n            try:\\n                f_iter = iter(f)\\n            except TypeError:\\n                f_iter = [(name, f)]\\n            for sn, sf in f_iter:\\n                if sf.use_vocab:\\n                    logger.info(\\' * %s vocab size = %d\\' % (sn, len(sf.vocab)))\\n\\n    # Build model.\\n    model = build_model(model_opt, opt, fields, checkpoint)\\n    n_params, enc, dec = _tally_parameters(model)\\n    logger.info(\\'encoder: %d\\' % enc)\\n    logger.info(\\'decoder: %d\\' % dec)\\n    logger.info(\\'* number of parameters: %d\\' % n_params)\\n    _check_save_model_path(opt)\\n\\n    # Build optimizer.\\n    optim = Optimizer.from_opt(model, opt, checkpoint=checkpoint)\\n\\n    # Build model saver\\n    model_saver = build_model_saver(model_opt, opt, model, fields, optim)\\n\\n    trainer = build_trainer(\\n        opt, device_id, model, fields, optim, model_saver=model_saver)\\n\\n    # this line is kind of a temporary kludge because different objects expect\\n    # fields to have a different structure\\n    dataset_fields = dict(chain.from_iterable(fields.values()))\\n\\n    train_iter = build_dataset_iter(\"train\", dataset_fields, opt)\\n    valid_iter = build_dataset_iter(\\n        \"valid\", dataset_fields, opt, is_train=False)\\n\\n    if len(opt.gpu_ranks):\\n        logger.info(\\'Starting training on GPU: %s\\' % opt.gpu_ranks)\\n    else:\\n        logger.info(\\'Starting training on CPU, could be very slow\\')\\n    train_steps = opt.train_steps\\n    if opt.single_pass and train_steps > 0:\\n        logger.warning(\"Option single_pass is enabled, ignoring train_steps.\")\\n        train_steps = 0\\n    trainer.train(\\n        train_iter,\\n        train_steps,\\n        save_checkpoint_steps=opt.save_checkpoint_steps,\\n        valid_iter=valid_iter,\\n        valid_steps=opt.valid_steps,\\n        early_stop_round=opt.early_stop_round,\\n        early_stop_threshold=opt.early_stop_threshold)\\n\\n    if opt.tensorboard:\\n        trainer.report_manager.tensorboard_writer.close()',\n",
       " '_check_save_model_path': <function __main__._check_save_model_path(opt)>,\n",
       " '_tally_parameters': <function __main__._tally_parameters(model)>,\n",
       " 'training_opt_postprocessing': <function __main__.training_opt_postprocessing(opt, device_id)>,\n",
       " 'train_main': <function __main__.train_main(opt, device_id)>,\n",
       " '_i14': 'class BaseRunner:\\n    \\n    def __init__(self, data_path):\\n        self.data_path = data_path\\n        self.model_id = None\\n    \\n    def run(self, option, model_id=None, train_from=None):\\n        model_id = model_id if model_id is not None else BaseRunner.generate_id()\\n        self.model_id = model_id\\n        self.prepare_save_file()\\n        arg_dict = {\\n            \\'data\\': self.data_path,\\n            \\'save_model\\': self.model_path,\\n            \\'gpu_ranks\\': 0,\\n            \\'tensorboard\\': None,\\n            \\'tensorboard_log_dir\\': self.log_path + \\'/log\\'\\n        }\\n        arg_dict.update(option)\\n        if train_from is not None:\\n            arg_dict[\\'train_from\\'] = train_from\\n        ArgumentHelper.save_args(arg_dict, self.model_path)\\n        parser = configargparse.ArgumentParser(\\n            description=\\'train.py\\',\\n            formatter_class=configargparse.ArgumentDefaultsHelpFormatter)\\n\\n        opts.add_md_help_argument(parser)\\n        opts.model_opts(parser)\\n        opts.train_opts(parser)\\n        parser.add_argument(\\'-early_stop_round\\', type=int, default=10000, help=\\'Early stop round\\')\\n        parser.add_argument(\\'-early_stop_threshold\\', type=float, default=1, help=\\'Early stop threshold\\')\\n\\n        opt = parser.parse_args(ArgumentHelper.convert_dict_to_arg_array(arg_dict))\\n        train_main(opt, 0)\\n\\n    def translate_main(self, opt, logger, model_id):\\n        translator = build_translator(opt, report_score=True)\\n        src_shards = split_corpus(opt.src, opt.shard_size)\\n        tgt_shards = split_corpus(opt.tgt, opt.shard_size) \\\\\\n            if opt.tgt is not None else [None]*opt.shard_size\\n        shard_pairs = zip(src_shards, tgt_shards)\\n\\n        for i, (src_shard, tgt_shard) in enumerate(shard_pairs):\\n            logger.info(\"Translating shard %d.\" % i)\\n            translator.translate(\\n                src=src_shard,\\n                tgt=tgt_shard,\\n                src_dir=opt.src_dir,\\n                batch_size=opt.batch_size,\\n                attn_debug=opt.attn_debug\\n            )\\n\\n    def translate(self, args, model_id=None):\\n        model_id = model_id if model_id is not None else self.model_id\\n        if model_id is None:\\n            raise Exception()\\n        self.model_id = model_id\\n        self.prepare_save_file()\\n        args[\\'model\\'] = self.model_path + \\'_step_\\' + str(BaseRunner.get_latest_model(\\n                                                        self.model_dir,\\n                                                        self.model_prefix,\\n                                                        self.model_id)) + \\'.pt\\'\\n        parser = configargparse.ArgumentParser(\\n            description=\\'translate.py\\',\\n            config_file_parser_class=configargparse.YAMLConfigFileParser,\\n            formatter_class=configargparse.ArgumentDefaultsHelpFormatter)\\n        opts.config_opts(parser)\\n        opts.add_md_help_argument(parser)\\n        opts.translate_opts(parser)\\n\\n        opt = parser.parse_args(ArgumentHelper.convert_dict_to_arg_array(args))\\n        logger = init_logger(opt.log_file)\\n        self.translate_main(opt, logger, model_id)\\n\\n    @staticmethod\\n    def generate_id():\\n        return hashlib.md5(str(uuid.uuid4()).encode()).hexdigest()\\n    \\n    @staticmethod\\n    def get_latest_model(model_dir, model_prefix, model_id):\\n        all_steps = []\\n        for root, dirs, files in os.walk(model_dir):\\n            path = root.split(os.sep)\\n            for file in files:\\n                absolute_filename = os.path.join(root, file)\\n                if model_id in absolute_filename:\\n                    steps = re.findall(model_prefix + model_id + \\'_step_(\\\\d+)\\\\.pt\\', absolute_filename)\\n                    if len(steps) > 0:\\n                        all_steps.append(int(steps[0]))\\n        return max(all_steps)\\n\\n    def prepare_save_file(self):\\n        model_id = self.model_id\\n        model_dir = \\'/mnt/drive-2t/model/\\'\\n        if not os.path.exists(model_dir):\\n            os.makedirs(model_dir)\\n        model_prefix = \\'opennmt_\\'\\n        model_path = model_dir + model_prefix + model_id\\n        log_dir = \\'/mnt/drive-2t/log/\\'\\n        if not os.path.exists(log_dir):\\n            os.makedirs(log_dir)\\n        log_prefix = model_prefix\\n        log_path = log_dir + log_prefix + model_id\\n        if not os.path.exists(log_path):\\n            os.mkdir(log_path)\\n        self.model_path = model_path\\n        self.log_path = log_path\\n        self.model_prefix = model_prefix\\n        self.model_dir = model_dir\\n\\n\\nrunner = BaseRunner(\\'data/nmt15\\')',\n",
       " 'BaseRunner': __main__.BaseRunner,\n",
       " 'runner': <__main__.BaseRunner at 0x7f9722559ef0>,\n",
       " '_i15': \"reverse_runner = BaseRunner('data/nmt15-reverse')\",\n",
       " 'reverse_runner': <__main__.BaseRunner at 0x7f9722598748>,\n",
       " '_i16': \"model_args = {\\n    'model_dtype': 'fp32',\\n    'encoder_type': 'transformer',\\n    'decoder_type': 'transformer',\\n    'layers': 6,\\n    'position_encoding': None,\\n    'max_generator_batches': 2,\\n    'dropout': 0.1,\\n    'enc_rnn_size': 512,\\n    'dec_rnn_size': 512,\\n    'src_word_vec_size': 512,\\n    'tgt_word_vec_size': 512,\\n    'accum_count': 4\\n}\\ntrain_args = {\\n    'batch_size': 16,\\n    'train_steps': 100000,\\n    'optim': 'adam',\\n    'learning_rate': 0.001,\\n    'valid_step': 100,\\n    'valid_batch': 32,\\n    'early_stop_round': 5,\\n    'early_stop_threshold': 0.01,\\n    'report_every': 100\\n}\\ntotal_args = {}\\ntotal_args.update(model_args)\\ntotal_args.update(train_args)\\n\\nreverse_runner.run(total_args)\\nreverse_runner.model_id\",\n",
       " 'model_args': {'model_dtype': 'fp32',\n",
       "  'encoder_type': 'transformer',\n",
       "  'decoder_type': 'transformer',\n",
       "  'layers': 6,\n",
       "  'position_encoding': None,\n",
       "  'max_generator_batches': 2,\n",
       "  'dropout': 0.1,\n",
       "  'enc_rnn_size': 512,\n",
       "  'dec_rnn_size': 512,\n",
       "  'src_word_vec_size': 512,\n",
       "  'tgt_word_vec_size': 512,\n",
       "  'accum_count': 4},\n",
       " 'train_args': {'batch_size': 16,\n",
       "  'train_steps': 100000,\n",
       "  'optim': 'adam',\n",
       "  'learning_rate': 0.001,\n",
       "  'valid_step': 100,\n",
       "  'valid_batch': 32,\n",
       "  'early_stop_round': 5,\n",
       "  'early_stop_threshold': 0.01,\n",
       "  'report_every': 100},\n",
       " 'total_args': {'model_dtype': 'fp32',\n",
       "  'encoder_type': 'transformer',\n",
       "  'decoder_type': 'transformer',\n",
       "  'layers': 6,\n",
       "  'position_encoding': None,\n",
       "  'max_generator_batches': 2,\n",
       "  'dropout': 0.1,\n",
       "  'enc_rnn_size': 512,\n",
       "  'dec_rnn_size': 512,\n",
       "  'src_word_vec_size': 512,\n",
       "  'tgt_word_vec_size': 512,\n",
       "  'accum_count': 4,\n",
       "  'batch_size': 16,\n",
       "  'train_steps': 100000,\n",
       "  'optim': 'adam',\n",
       "  'learning_rate': 0.001,\n",
       "  'valid_step': 100,\n",
       "  'valid_batch': 32,\n",
       "  'early_stop_round': 5,\n",
       "  'early_stop_threshold': 0.01,\n",
       "  'report_every': 100},\n",
       " '_i17': 'globals()'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{k: v in globals().items() if k "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example #3: Make Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-15 17:15:34,546 WARNING] FP16 is experimental, the generated checkpoints may be incompatible with a future version\n",
      "[2019-02-15 17:15:34,564 INFO] Translating shard 0.\n",
      "[2019-02-15 17:22:55,574 INFO] Translating shard 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRED AVG SCORE: -0.6200, PRED PPL: 1.8589\n",
      "GOLD AVG SCORE: -10.4237, GOLD PPL: 33648.8349\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-15 17:30:52,361 INFO] Translating shard 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRED AVG SCORE: -0.6197, PRED PPL: 1.8584\n",
      "GOLD AVG SCORE: -10.4420, GOLD PPL: 34270.6062\n",
      "PRED AVG SCORE: -0.6319, PRED PPL: 1.8812\n",
      "GOLD AVG SCORE: -10.4421, GOLD PPL: 34270.9004\n"
     ]
    }
   ],
   "source": [
    "model_id = '832c1d8d66c693bce372445f692b0099'\n",
    "\n",
    "translate_args = {\n",
    "    'src': 'data/nmt15/test_en',\n",
    "    'tgt': 'data/nmt15/test_vi',\n",
    "    'out': '/mnt/drive-2t/pred/opennmt_' + model_id + '.txt',\n",
    "    'replace_unk': None,\n",
    "    'gpu': '0',\n",
    "    'batch_size': '16'\n",
    "}\n",
    "\n",
    "runner.translate(translate_args, model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(36.37, 2.0)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyTrainer(Trainer):\n",
    "    def train(self,\n",
    "              train_iter,\n",
    "              train_steps,\n",
    "              save_checkpoint_steps=5000,\n",
    "              valid_iter=None,\n",
    "              valid_steps=10000,\n",
    "              early_stop_round=50000,\n",
    "              early_stop_threshold=1):\n",
    "        if valid_iter is None:\n",
    "            logger.info('Start training loop without validation...')\n",
    "        else:\n",
    "            logger.info('Start training loop and validate every %d steps...',\n",
    "                        valid_steps)\n",
    "\n",
    "        total_stats = onmt.utils.Statistics()\n",
    "        report_stats = onmt.utils.Statistics()\n",
    "        self._start_report_manager(start_time=total_stats.start_time)\n",
    "        \n",
    "        valid_early_stop_loss = []\n",
    "        for i, (batches, normalization) in enumerate(\n",
    "                self._accum_batches(train_iter)):\n",
    "            step = self.optim.training_step\n",
    "\n",
    "            self._gradient_accumulation(\n",
    "                batches, normalization, total_stats,\n",
    "                report_stats)\n",
    "\n",
    "            if self.average_decay > 0 and i % self.average_every == 0:\n",
    "                self._update_average(step)\n",
    "\n",
    "            report_stats = self._maybe_report_training(\n",
    "                step, train_steps,\n",
    "                self.optim.learning_rate(),\n",
    "                report_stats)\n",
    "\n",
    "            if valid_iter is not None and step % valid_steps == 0:\n",
    "                valid_stats = self.validate(\n",
    "                    valid_iter, moving_average=self.moving_average)\n",
    "                valid_stats = self._maybe_gather_stats(valid_stats)\n",
    "                valid_loss = valid_stats.accuracy()\n",
    "#                 valid_early_stop_loss.append(valid_loss)\n",
    "                if len(valid_early_stop_loss) == early_stop_round:\n",
    "                    # 条件为当前valid loss要超过前n个valid loss最好的那个加threshold\n",
    "                    if valid_loss - max(valid_early_stop_loss) < early_stop_threshold:\n",
    "                        print('meet early stop condition, prev best loss:', max(valid_early_stop_loss), 'current loss:', valid_loss)\n",
    "                        if self.model_saver is not None:\n",
    "                            self.model_saver.save(step, moving_average=self.moving_average)\n",
    "                        return total_stats\n",
    "                    else:\n",
    "                        valid_early_stop_loss.pop(0)\n",
    "                        valid_early_stop_loss\n",
    "                self._report_step(self.optim.learning_rate(),\n",
    "                                  step, valid_stats=valid_stats)\n",
    "\n",
    "            if (self.model_saver is not None\n",
    "                    and (save_checkpoint_steps != 0\n",
    "                         and step % save_checkpoint_steps == 0)):\n",
    "                self.model_saver.save(step, moving_average=self.moving_average)\n",
    "\n",
    "            if train_steps > 0 and step >= train_steps:\n",
    "                break\n",
    "\n",
    "        if self.model_saver is not None:\n",
    "            self.model_saver.save(step, moving_average=self.moving_average)\n",
    "        return total_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "该model(transformer)的valid loss = 51.17859672702835, last_train_acc = 58.17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-15 19:46:30,266 INFO] Translating shard 0.\n",
      "[2019-02-15 19:48:39,104 INFO] Translating shard 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRED AVG SCORE: -0.4451, PRED PPL: 1.5607\n",
      "GOLD AVG SCORE: -11.8941, GOLD PPL: 146406.9002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-15 19:50:56,516 INFO] Translating shard 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRED AVG SCORE: -0.4401, PRED PPL: 1.5529\n",
      "GOLD AVG SCORE: -11.8323, GOLD PPL: 137626.8257\n",
      "PRED AVG SCORE: -0.4524, PRED PPL: 1.5722\n",
      "GOLD AVG SCORE: -11.7261, GOLD PPL: 123763.2465\n"
     ]
    }
   ],
   "source": [
    "model_id = '5cb980256df787cdd84b166f65503553'\n",
    "runner.translate(build_translate_args(model_id), model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4.0, 24.16)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = '/mnt/drive-2t/pred/opennmt_' + model_id + '.txt'\n",
    "ref = 'data/nmt15/test_vi'\n",
    "bleu_evaluate(pred, ref), bleu_evaluate(pred, ref, '0.25 0.25 0.25 0.25')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM model valid loss = 53.67019634197286, last_train_acc = 67.98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-15 20:47:26,898 INFO] Translating shard 0.\n",
      "[2019-02-15 20:48:58,846 INFO] Translating shard 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRED AVG SCORE: -0.6594, PRED PPL: 1.9336\n",
      "GOLD AVG SCORE: -11.0811, GOLD PPL: 64930.4421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-15 20:50:33,734 INFO] Translating shard 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRED AVG SCORE: -0.6708, PRED PPL: 1.9557\n",
      "GOLD AVG SCORE: -11.0883, GOLD PPL: 65401.4528\n",
      "PRED AVG SCORE: -0.6949, PRED PPL: 2.0034\n",
      "GOLD AVG SCORE: -11.0109, GOLD PPL: 60531.6975\n"
     ]
    }
   ],
   "source": [
    "model_id = '18afdcd77652c3d69836a58403953bac'\n",
    "args = build_translate_args_reverse(model_id)\n",
    "args['batch_size'] = 32\n",
    "reverse_runner.translate(args, model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(35.73, 21.21)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = '/mnt/drive-2t/pred/opennmt_' + model_id + '.txt'\n",
    "ref = 'data/nmt15/test_en'\n",
    "bleu_evaluate(pred, ref), bleu_evaluate(pred, ref, '0.25 0.25 0.25 0.25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now , it &apos;s time that I ended up , and as I mentioned before , of course , I &apos;ve got a lot of other data if you &apos;re interested , but I just want to give this basic idea about communication to its language , and the potential of that possibility .\n",
      "\n",
      "Now I just want to wrap up , and as I was mentioning earlier of course I have a lot of other data if you &apos;re interested , but I just wanted to give this sort of basic idea of being able to communicate with the brain in its language , and the potential power of being able to do that .\n",
      "-------------\n",
      "This is very different from the artificial device that you have to bridge from the brain to a device . Here we have to bridge the world from the external brain to a device .\n",
      "\n",
      "So it &apos;s different from the motor prosthetics where you &apos;re communicating from the brain to a device . Here we have to communicate from the outside world into the brain and be understood , and be understood by the brain .\n",
      "-------------\n",
      "And then the last thing I want to say , actually , I want to emphasize that this idea can be . .\n",
      "\n",
      "And then the last thing I wanted to say , really , is to emphasize that the idea generalizes .\n",
      "-------------\n",
      "The strategy that we use to find the code code , we can also use the code code for other parts , for example , for example , and the mechanics , to the blind and the loạn .\n",
      "\n",
      "So the same strategy that we used to find the code for the retina we can also use to find the code for other areas , for example , the auditory system and the motor system , so for treating deafness and for motor disorders .\n",
      "-------------\n",
      "So by the way we spent through the hỏng network of the retina to the original cells of the retina , we can also ignore the stocks in the cochlea to go to the auditory nerves , or take rid of the trong in the cortex , in the cortex , in the motor of motor , to vào in the space , in the cortex , in the motor , in the motor , in the motor , to vào .\n",
      "\n",
      "So just the same way that we were able to jump over the damaged circuitry in the retina to get to the retina &apos;s output cells , we can jump over the damaged circuitry in the cochlea to get the auditory nerve , or jump over damaged areas in the cortex , in the motor cortex , to bridge the gap produced by a stroke .\n",
      "-------------\n",
      "I just want to end with a simple message that understanding the code , which is really important , and if we can understand code , the language of the brain , what it looks like before not seem to be possible . Thank you .\n",
      "\n",
      "I just want to end with a simple message that understanding the code is really , really important , and if we can understand the code , the language of the brain , things become possible that didn &apos;t seem obviously possible before . Thank you .\n",
      "-------------\n",
      "Jonathan Harris : Mapping the visuals\n",
      "\n",
      "Jonathan Harris : The web as art\n",
      "-------------\n",
      "In December EG in 2007 , Jonathan Jonathan Drori talks his latest recent projects , including the thập of self , and the Internet , in which there are great projects , we &apos;re okay . &quot;\n",
      "\n",
      "At the EG conference in December 2007 , artist Jonathan Harris discusses his latest projects , which involve collecting stories : his own , strangers &apos; , and stories collected from the Internet , including his amazing &quot; We Feel Fine . &quot;\n",
      "-------------\n",
      "Today I &apos;m going to talk about the polls in some different ways .\n",
      "\n",
      "So I &apos;m going to talk today about collecting stories in some unconventional ways .\n",
      "-------------\n",
      "This is a picture of me -- a very awkward time .\n",
      "\n",
      "This is a picture of me from a very awkward stage in my life .\n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pred_head = !head $pred\n",
    "ref_head = !head $ref\n",
    "pairs = list(zip(pred_head, ref_head))\n",
    "for pair in pairs:\n",
    "    print(pair[0])\n",
    "    print()\n",
    "    print(pair[1])\n",
    "    print('-------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"data\": \"data/nmt15-reverse\",\n",
      "    \"save_model\": \"/mnt/drive-2t/model/opennmt_18afdcd77652c3d69836a58403953bac\",\n",
      "    \"gpu_ranks\": 0,\n",
      "    \"tensorboard\": null,\n",
      "    \"tensorboard_log_dir\": \"/mnt/drive-2t/log/opennmt_18afdcd77652c3d69836a58403953bac/log\",\n",
      "    \"batch_size\": 128,\n",
      "    \"train_steps\": 20000,\n",
      "    \"optim\": \"adam\",\n",
      "    \"learning_rate\": 0.001,\n",
      "    \"valid_step\": 500,\n",
      "    \"valid_batch\": 64,\n",
      "    \"early_stop_round\": 5,\n",
      "    \"early_stop_threshold\": 1,\n",
      "    \"report_every\": 100\n",
      "}35.73 21.21\n",
      "Now , it &apos;s time that I ended up , and as I mentioned before , of course , I &apos;ve got a lot of other data if you &apos;re interested , but I just want to give this basic idea about communication to its language , and the potential of that possibility .\n",
      "\n",
      "Now I just want to wrap up , and as I was mentioning earlier of course I have a lot of other data if you &apos;re interested , but I just wanted to give this sort of basic idea of being able to communicate with the brain in its language , and the potential power of being able to do that .\n",
      "-------------\n",
      "This is very different from the artificial device that you have to bridge from the brain to a device . Here we have to bridge the world from the external brain to a device .\n",
      "\n",
      "So it &apos;s different from the motor prosthetics where you &apos;re communicating from the brain to a device . Here we have to communicate from the outside world into the brain and be understood , and be understood by the brain .\n",
      "-------------\n",
      "And then the last thing I want to say , actually , I want to emphasize that this idea can be . .\n",
      "\n",
      "And then the last thing I wanted to say , really , is to emphasize that the idea generalizes .\n",
      "-------------\n",
      "The strategy that we use to find the code code , we can also use the code code for other parts , for example , for example , and the mechanics , to the blind and the loạn .\n",
      "\n",
      "So the same strategy that we used to find the code for the retina we can also use to find the code for other areas , for example , the auditory system and the motor system , so for treating deafness and for motor disorders .\n",
      "-------------\n",
      "So by the way we spent through the hỏng network of the retina to the original cells of the retina , we can also ignore the stocks in the cochlea to go to the auditory nerves , or take rid of the trong in the cortex , in the cortex , in the motor of motor , to vào in the space , in the cortex , in the motor , in the motor , in the motor , to vào .\n",
      "\n",
      "So just the same way that we were able to jump over the damaged circuitry in the retina to get to the retina &apos;s output cells , we can jump over the damaged circuitry in the cochlea to get the auditory nerve , or jump over damaged areas in the cortex , in the motor cortex , to bridge the gap produced by a stroke .\n",
      "-------------\n",
      "I just want to end with a simple message that understanding the code , which is really important , and if we can understand code , the language of the brain , what it looks like before not seem to be possible . Thank you .\n",
      "\n",
      "I just want to end with a simple message that understanding the code is really , really important , and if we can understand the code , the language of the brain , things become possible that didn &apos;t seem obviously possible before . Thank you .\n",
      "-------------\n",
      "Jonathan Harris : Mapping the visuals\n",
      "\n",
      "Jonathan Harris : The web as art\n",
      "-------------\n",
      "In December EG in 2007 , Jonathan Jonathan Drori talks his latest recent projects , including the thập of self , and the Internet , in which there are great projects , we &apos;re okay . &quot;\n",
      "\n",
      "At the EG conference in December 2007 , artist Jonathan Harris discusses his latest projects , which involve collecting stories : his own , strangers &apos; , and stories collected from the Internet , including his amazing &quot; We Feel Fine . &quot;\n",
      "-------------\n",
      "Today I &apos;m going to talk about the polls in some different ways .\n",
      "\n",
      "So I &apos;m going to talk today about collecting stories in some unconventional ways .\n",
      "-------------\n",
      "This is a picture of me -- a very awkward time .\n",
      "\n",
      "This is a picture of me from a very awkward stage in my life .\n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "evaluate_human('18afdcd77652c3d69836a58403953bac')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-16 11:08:55,819 WARNING] FP16 is experimental, the generated checkpoints may be incompatible with a future version\n",
      "[2019-02-16 11:08:55,847 INFO] Translating shard 0.\n",
      "[2019-02-16 11:17:00,743 INFO] Translating shard 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRED AVG SCORE: -0.8561, PRED PPL: 2.3540\n",
      "GOLD AVG SCORE: -10.7383, GOLD PPL: 46086.8161\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-16 11:24:55,976 INFO] Translating shard 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRED AVG SCORE: -0.8727, PRED PPL: 2.3933\n",
      "GOLD AVG SCORE: -10.7712, GOLD PPL: 47628.2838\n",
      "PRED AVG SCORE: -0.8991, PRED PPL: 2.4575\n",
      "GOLD AVG SCORE: -10.7426, GOLD PPL: 46286.4810\n"
     ]
    }
   ],
   "source": [
    "model_id = '5384edb1bec7646d9b0008074304d283'\n",
    "args = build_translate_args_reverse(model_id)\n",
    "args['batch_size'] = 16\n",
    "reverse_runner.translate(args, model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(31.22, 17.0)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = '/mnt/drive-2t/pred/opennmt_' + model_id + '.txt'\n",
    "ref = 'data/nmt15/test_en'\n",
    "bleu_evaluate(pred, ref), bleu_evaluate(pred, ref, '0.25 0.25 0.25 0.25')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Now , at the end of the end of the conclusion , I 've mentioned before , I 've got a lot of other data , but if you want to give me a sense of the idea of the brain , it 's ability to interact with the potential language , and now , it 's right now .\n",
      "\n",
      "Now I just want to wrap up , and as I was mentioning earlier of course I have a lot of other data if you 're interested , but I just wanted to give this sort of basic idea of being able to communicate with the brain in its language , and the potential power of being able to do that .\n",
      "-------------\n",
      "This is very different with artificial biology , and you have to communicate from the brain to the brain . Here we have to be able to communicate north through the world , and understand the brain .\n",
      "\n",
      "So it 's different from the motor prosthetics where you 're communicating from the brain to a device . Here we have to communicate from the outside world into the brain and be understood , and be understood by the brain .\n",
      "-------------\n",
      "And then finally , the last thing I want to say , well , I really want to emphasize that this idea could be hoá .\n",
      "\n",
      "And then the last thing I wanted to say , really , is to emphasize that the idea generalizes .\n",
      "-------------\n",
      "The strategy that we use to use them to find the retina chip , and we can also use different code to find other examples , such as psychiatrists and bones , the engine , the spinal cord injury , the spinal cord .\n",
      "\n",
      "So the same strategy that we used to find the code for the retina we can also use to find the code for other areas , for example , the auditory system and the motor system , so for treating deafness and for motor disorders .\n",
      "-------------\n",
      "In the way we removed the mạng mạng through the retina chip in the early retina to be the beginning of the retina chip , we can ignore the wireless network , the nerves , the nerves , the brain damage , the brain , the brain , the brain động , the brain damage to the brain , the brain damage to hỏng , to hỏng , to hỏng , to hỏng the brain damage .\n",
      "\n",
      "So just the same way that we were able to jump over the damaged circuitry in the retina to get to the retina 's output cells , we can jump over the damaged circuitry in the cochlea to get the auditory nerve , or jump over damaged areas in the cortex , in the motor cortex , to bridge the gap produced by a stroke .\n",
      "-------------\n",
      "I just want to end up with a message that simple marker of code , really important code , and if we can understand the code , the code of language , the brain may not be able to be able to be able to be able to imagine . Thank you .\n",
      "\n",
      "I just want to end with a simple message that understanding the code is really , really important , and if we can understand the code , the language of the brain , things become possible that didn 't seem obviously possible before . Thank you .\n",
      "-------------\n",
      "Jonathan Drori : Harris stories\n",
      "\n",
      "Jonathan Harris : The web as art\n",
      "-------------\n",
      "In December 2008 , Jonathan Harris , Jonathan Drori talks about his latest recent projects , including the recent stories of the story of others , and the Internet came from the Internet , &quot; We have a wonderful project . &quot;\n",
      "\n",
      "At the EG conference in December 2007 , artist Jonathan Harris discusses his latest projects , which involve collecting stories : his own , strangers ' , and stories collected from the Internet , including his amazing &quot; We Feel Fine . &quot;\n",
      "-------------\n",
      "Today I 'm going to talk to you today about a little bit skeptical .\n",
      "\n",
      "So I 'm going to talk today about collecting stories in some unconventional ways .\n",
      "-------------\n",
      "This is a picture of me -- a very awkward time .\n",
      "\n",
      "This is a picture of me from a very awkward stage in my life .\n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pred_head = !head $pred\n",
    "ref_head = !head $ref\n",
    "pairs = list(zip(pred_head, ref_head))\n",
    "for pair in pairs:\n",
    "    print(pair[0].replace('&apos;', '\\''))\n",
    "    print()\n",
    "    print(pair[1].replace('&apos;', '\\''))\n",
    "    print('-------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example #4: GRU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-16 13:22:25,039 INFO]  * src vocab size = 8870\n",
      "[2019-02-16 13:22:25,042 INFO]  * tgt vocab size = 20071\n",
      "[2019-02-16 13:22:25,042 INFO] Building model...\n",
      "[2019-02-16 13:22:28,630 INFO] NMTModel(\n",
      "  (encoder): RNNEncoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(8870, 512, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (rnn): GRU(512, 512, num_layers=3, dropout=0.1)\n",
      "  )\n",
      "  (decoder): InputFeedRNNDecoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(20071, 512, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.1)\n",
      "    (rnn): StackedGRU(\n",
      "      (dropout): Dropout(p=0.1)\n",
      "      (layers): ModuleList(\n",
      "        (0): GRUCell(1024, 512)\n",
      "        (1): GRUCell(512, 512)\n",
      "        (2): GRUCell(512, 512)\n",
      "      )\n",
      "    )\n",
      "    (attn): GlobalAttention(\n",
      "      (linear_in): Linear(in_features=512, out_features=512, bias=False)\n",
      "      (linear_out): Linear(in_features=1024, out_features=512, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (generator): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=20071, bias=True)\n",
      "    (1): Cast()\n",
      "    (2): LogSoftmax()\n",
      "  )\n",
      ")\n",
      "[2019-02-16 13:22:28,632 INFO] encoder: 9269248\n",
      "[2019-02-16 13:22:28,632 INFO] decoder: 26873447\n",
      "[2019-02-16 13:22:28,633 INFO] * number of parameters: 36142695\n",
      "[2019-02-16 13:22:28,788 INFO] Starting training on GPU: [0]\n",
      "[2019-02-16 13:22:28,789 INFO] Start training loop and validate every 500 steps...\n",
      "[2019-02-16 13:22:30,209 INFO] Loading dataset from data/nmt15-reverse.train.0.pt, number of examples: 95844\n",
      "[2019-02-16 13:22:44,546 INFO] Step 100/20000; acc:   7.67; ppl: 702.10; xent: 6.55; lr: 0.00100; 9977/8490 tok/s;     16 sec\n",
      "[2019-02-16 13:22:57,961 INFO] Step 200/20000; acc:  12.02; ppl: 463.61; xent: 6.14; lr: 0.00100; 11787/10064 tok/s;     29 sec\n",
      "[2019-02-16 13:23:11,875 INFO] Step 300/20000; acc:  17.59; ppl: 315.59; xent: 5.75; lr: 0.00100; 11550/9796 tok/s;     43 sec\n",
      "[2019-02-16 13:23:25,466 INFO] Step 400/20000; acc:  21.21; ppl: 219.63; xent: 5.39; lr: 0.00100; 11667/9897 tok/s;     57 sec\n",
      "[2019-02-16 13:23:39,206 INFO] Step 500/20000; acc:  23.55; ppl: 169.83; xent: 5.13; lr: 0.00100; 11509/9828 tok/s;     70 sec\n",
      "[2019-02-16 13:23:39,305 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n",
      "[2019-02-16 13:23:54,829 INFO] Validation perplexity: 137.143\n",
      "[2019-02-16 13:23:54,829 INFO] Validation accuracy: 24.4868\n",
      "[2019-02-16 13:24:08,731 INFO] Step 600/20000; acc:  25.26; ppl: 142.48; xent: 4.96; lr: 0.00100; 5397/4583 tok/s;    100 sec\n",
      "[2019-02-16 13:24:22,298 INFO] Step 700/20000; acc:  26.50; ppl: 125.29; xent: 4.83; lr: 0.00100; 11656/9952 tok/s;    114 sec\n",
      "[2019-02-16 13:24:35,945 INFO] Step 800/20000; acc:  27.71; ppl: 109.55; xent: 4.70; lr: 0.00100; 11481/9804 tok/s;    127 sec\n",
      "[2019-02-16 13:24:50,118 INFO] Step 900/20000; acc:  28.61; ppl: 99.39; xent: 4.60; lr: 0.00100; 11180/9497 tok/s;    141 sec\n",
      "[2019-02-16 13:25:03,767 INFO] Step 1000/20000; acc:  29.21; ppl: 90.55; xent: 4.51; lr: 0.00100; 11565/9863 tok/s;    155 sec\n",
      "[2019-02-16 13:25:03,864 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n",
      "[2019-02-16 13:25:19,876 INFO] Validation perplexity: 78.3542\n",
      "[2019-02-16 13:25:19,877 INFO] Validation accuracy: 30.3013\n",
      "[2019-02-16 13:25:33,691 INFO] Step 1100/20000; acc:  29.92; ppl: 84.91; xent: 4.44; lr: 0.00100; 5295/4520 tok/s;    185 sec\n",
      "[2019-02-16 13:25:47,696 INFO] Step 1200/20000; acc:  30.16; ppl: 81.21; xent: 4.40; lr: 0.00100; 11397/9649 tok/s;    199 sec\n",
      "[2019-02-16 13:26:01,985 INFO] Step 1300/20000; acc:  30.62; ppl: 78.28; xent: 4.36; lr: 0.00100; 11250/9591 tok/s;    213 sec\n",
      "[2019-02-16 13:26:15,545 INFO] Step 1400/20000; acc:  31.30; ppl: 72.24; xent: 4.28; lr: 0.00100; 11483/9826 tok/s;    227 sec\n",
      "[2019-02-16 13:26:30,738 INFO] Loading dataset from data/nmt15-reverse.train.0.pt, number of examples: 95844\n",
      "[2019-02-16 13:26:31,028 INFO] Step 1500/20000; acc:  32.02; ppl: 66.87; xent: 4.20; lr: 0.00100; 9973/8533 tok/s;    242 sec\n",
      "[2019-02-16 13:26:31,122 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n",
      "[2019-02-16 13:26:47,223 INFO] Validation perplexity: 61.8361\n",
      "[2019-02-16 13:26:47,223 INFO] Validation accuracy: 32.2966\n",
      "[2019-02-16 13:27:00,966 INFO] Step 1600/20000; acc:  32.34; ppl: 62.81; xent: 4.14; lr: 0.00100; 5368/4552 tok/s;    272 sec\n",
      "[2019-02-16 13:27:15,221 INFO] Step 1700/20000; acc:  32.64; ppl: 57.94; xent: 4.06; lr: 0.00100; 10997/9415 tok/s;    286 sec\n",
      "[2019-02-16 13:27:29,135 INFO] Step 1800/20000; acc:  32.65; ppl: 58.26; xent: 4.06; lr: 0.00100; 11421/9681 tok/s;    300 sec\n",
      "[2019-02-16 13:27:43,006 INFO] Step 1900/20000; acc:  33.06; ppl: 55.55; xent: 4.02; lr: 0.00100; 11642/9855 tok/s;    314 sec\n",
      "[2019-02-16 13:27:58,201 INFO] Step 2000/20000; acc:  33.56; ppl: 52.39; xent: 3.96; lr: 0.00100; 10414/8905 tok/s;    329 sec\n",
      "[2019-02-16 13:27:58,299 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n",
      "[2019-02-16 13:28:14,535 INFO] Validation perplexity: 49.6671\n",
      "[2019-02-16 13:28:14,535 INFO] Validation accuracy: 34.1693\n",
      "[2019-02-16 13:28:28,316 INFO] Step 2100/20000; acc:  33.86; ppl: 49.33; xent: 3.90; lr: 0.00100; 5221/4444 tok/s;    360 sec\n",
      "[2019-02-16 13:28:42,007 INFO] Step 2200/20000; acc:  34.17; ppl: 47.46; xent: 3.86; lr: 0.00100; 11607/9902 tok/s;    373 sec\n",
      "[2019-02-16 13:28:55,767 INFO] Step 2300/20000; acc:  34.69; ppl: 44.65; xent: 3.80; lr: 0.00100; 11463/9746 tok/s;    387 sec\n",
      "[2019-02-16 13:29:10,135 INFO] Step 2400/20000; acc:  34.70; ppl: 43.50; xent: 3.77; lr: 0.00100; 11102/9437 tok/s;    401 sec\n",
      "[2019-02-16 13:29:23,701 INFO] Step 2500/20000; acc:  35.09; ppl: 41.28; xent: 3.72; lr: 0.00100; 11488/9823 tok/s;    415 sec\n",
      "[2019-02-16 13:29:23,797 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n",
      "[2019-02-16 13:29:40,074 INFO] Validation perplexity: 45.1689\n",
      "[2019-02-16 13:29:40,075 INFO] Validation accuracy: 35.0392\n",
      "[2019-02-16 13:29:53,862 INFO] Step 2600/20000; acc:  35.61; ppl: 40.03; xent: 3.69; lr: 0.00100; 5223/4458 tok/s;    445 sec\n",
      "[2019-02-16 13:30:07,959 INFO] Step 2700/20000; acc:  35.31; ppl: 40.19; xent: 3.69; lr: 0.00100; 11272/9551 tok/s;    459 sec\n",
      "[2019-02-16 13:30:22,460 INFO] Step 2800/20000; acc:  34.91; ppl: 40.57; xent: 3.70; lr: 0.00100; 11297/9605 tok/s;    474 sec\n",
      "[2019-02-16 13:30:35,786 INFO] Step 2900/20000; acc:  36.27; ppl: 37.49; xent: 3.62; lr: 0.00100; 11408/9794 tok/s;    487 sec\n",
      "[2019-02-16 13:30:50,915 INFO] Loading dataset from data/nmt15-reverse.train.0.pt, number of examples: 95844\n",
      "[2019-02-16 13:30:51,354 INFO] Step 3000/20000; acc:  36.41; ppl: 36.09; xent: 3.59; lr: 0.00100; 9988/8524 tok/s;    503 sec\n",
      "[2019-02-16 13:30:51,447 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n",
      "[2019-02-16 13:31:07,531 INFO] Validation perplexity: 41.8491\n",
      "[2019-02-16 13:31:07,531 INFO] Validation accuracy: 35.7779\n",
      "[2019-02-16 13:31:21,480 INFO] Step 3100/20000; acc:  36.25; ppl: 35.47; xent: 3.57; lr: 0.00100; 5415/4600 tok/s;    533 sec\n",
      "[2019-02-16 13:31:34,942 INFO] Step 3200/20000; acc:  36.52; ppl: 33.46; xent: 3.51; lr: 0.00100; 11630/9938 tok/s;    546 sec\n",
      "[2019-02-16 13:31:48,750 INFO] Step 3300/20000; acc:  36.71; ppl: 33.77; xent: 3.52; lr: 0.00100; 11420/9680 tok/s;    560 sec\n",
      "[2019-02-16 13:32:02,494 INFO] Step 3400/20000; acc:  36.58; ppl: 33.34; xent: 3.51; lr: 0.00100; 11617/9868 tok/s;    574 sec\n",
      "[2019-02-16 13:32:16,367 INFO] Step 3500/20000; acc:  37.15; ppl: 31.91; xent: 3.46; lr: 0.00100; 11422/9741 tok/s;    588 sec\n",
      "[2019-02-16 13:32:16,463 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n",
      "[2019-02-16 13:32:32,724 INFO] Validation perplexity: 38.6914\n",
      "[2019-02-16 13:32:32,725 INFO] Validation accuracy: 36.3206\n",
      "[2019-02-16 13:32:46,690 INFO] Step 3600/20000; acc:  37.22; ppl: 31.12; xent: 3.44; lr: 0.00100; 5261/4478 tok/s;    618 sec\n",
      "[2019-02-16 13:33:00,584 INFO] Step 3700/20000; acc:  37.15; ppl: 30.92; xent: 3.43; lr: 0.00100; 11591/9872 tok/s;    632 sec\n",
      "[2019-02-16 13:33:14,154 INFO] Step 3800/20000; acc:  38.23; ppl: 27.92; xent: 3.33; lr: 0.00100; 11374/9691 tok/s;    645 sec\n",
      "[2019-02-16 13:33:28,797 INFO] Step 3900/20000; acc:  37.20; ppl: 29.95; xent: 3.40; lr: 0.00100; 11163/9458 tok/s;    660 sec\n",
      "[2019-02-16 13:33:42,057 INFO] Step 4000/20000; acc:  38.94; ppl: 26.59; xent: 3.28; lr: 0.00100; 11363/9762 tok/s;    673 sec\n",
      "[2019-02-16 13:33:42,154 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n",
      "[2019-02-16 13:33:58,444 INFO] Validation perplexity: 37.5653\n",
      "[2019-02-16 13:33:58,445 INFO] Validation accuracy: 36.7936\n",
      "[2019-02-16 13:34:12,330 INFO] Step 4100/20000; acc:  38.40; ppl: 27.22; xent: 3.30; lr: 0.00100; 5201/4432 tok/s;    704 sec\n",
      "[2019-02-16 13:34:26,494 INFO] Step 4200/20000; acc:  38.01; ppl: 27.77; xent: 3.32; lr: 0.00100; 11349/9612 tok/s;    718 sec\n",
      "[2019-02-16 13:34:41,312 INFO] Step 4300/20000; acc:  37.25; ppl: 28.72; xent: 3.36; lr: 0.00100; 11255/9560 tok/s;    733 sec\n",
      "[2019-02-16 13:34:54,350 INFO] Step 4400/20000; acc:  39.24; ppl: 25.38; xent: 3.23; lr: 0.00100; 11394/9807 tok/s;    746 sec\n",
      "[2019-02-16 13:35:09,336 INFO] Loading dataset from data/nmt15-reverse.train.0.pt, number of examples: 95844\n",
      "[2019-02-16 13:35:09,954 INFO] Step 4500/20000; acc:  39.00; ppl: 25.58; xent: 3.24; lr: 0.00100; 9937/8486 tok/s;    761 sec\n",
      "[2019-02-16 13:35:10,048 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n",
      "[2019-02-16 13:35:26,179 INFO] Validation perplexity: 36.5078\n",
      "[2019-02-16 13:35:26,180 INFO] Validation accuracy: 36.9972\n",
      "[2019-02-16 13:35:40,174 INFO] Step 4600/20000; acc:  38.51; ppl: 25.82; xent: 3.25; lr: 0.00100; 5411/4591 tok/s;    791 sec\n",
      "[2019-02-16 13:35:53,680 INFO] Step 4700/20000; acc:  39.25; ppl: 24.30; xent: 3.19; lr: 0.00100; 11604/9924 tok/s;    805 sec\n",
      "[2019-02-16 13:36:07,559 INFO] Step 4800/20000; acc:  39.15; ppl: 24.87; xent: 3.21; lr: 0.00100; 11465/9704 tok/s;    819 sec\n",
      "[2019-02-16 13:36:21,368 INFO] Step 4900/20000; acc:  39.09; ppl: 24.65; xent: 3.20; lr: 0.00100; 11568/9834 tok/s;    833 sec\n",
      "[2019-02-16 13:36:35,302 INFO] Step 5000/20000; acc:  39.44; ppl: 23.77; xent: 3.17; lr: 0.00100; 11419/9733 tok/s;    847 sec\n",
      "[2019-02-16 13:36:35,398 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n",
      "[2019-02-16 13:36:51,657 INFO] Validation perplexity: 34.9117\n",
      "[2019-02-16 13:36:51,658 INFO] Validation accuracy: 37.511\n",
      "[2019-02-16 13:36:51,659 INFO] Saving checkpoint /mnt/drive-2t/model/opennmt_990547e9b5cc3ef3ab8df6728e6f8669_step_5000.pt\n",
      "[2019-02-16 13:37:07,980 INFO] Step 5100/20000; acc:  39.68; ppl: 23.36; xent: 3.15; lr: 0.00100; 4871/4148 tok/s;    879 sec\n",
      "[2019-02-16 13:37:21,806 INFO] Step 5200/20000; acc:  39.59; ppl: 23.21; xent: 3.14; lr: 0.00100; 11499/9795 tok/s;    893 sec\n",
      "[2019-02-16 13:37:35,301 INFO] Step 5300/20000; acc:  40.53; ppl: 21.51; xent: 3.07; lr: 0.00100; 11390/9727 tok/s;    907 sec\n",
      "[2019-02-16 13:37:50,029 INFO] Step 5400/20000; acc:  39.45; ppl: 23.34; xent: 3.15; lr: 0.00100; 11171/9451 tok/s;    921 sec\n",
      "[2019-02-16 13:38:03,733 INFO] Step 5500/20000; acc:  40.28; ppl: 21.85; xent: 3.08; lr: 0.00100; 11429/9770 tok/s;    935 sec\n",
      "[2019-02-16 13:38:03,828 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n",
      "[2019-02-16 13:38:20,116 INFO] Validation perplexity: 34.0384\n",
      "[2019-02-16 13:38:20,117 INFO] Validation accuracy: 37.7897\n",
      "[2019-02-16 13:38:33,599 INFO] Step 5600/20000; acc:  41.25; ppl: 20.47; xent: 3.02; lr: 0.00100; 5134/4390 tok/s;    965 sec\n",
      "[2019-02-16 13:38:47,699 INFO] Step 5700/20000; acc:  40.38; ppl: 21.80; xent: 3.08; lr: 0.00100; 11382/9632 tok/s;    979 sec\n",
      "[2019-02-16 13:39:02,229 INFO] Step 5800/20000; acc:  39.89; ppl: 22.04; xent: 3.09; lr: 0.00100; 11237/9573 tok/s;    993 sec\n",
      "[2019-02-16 13:39:15,399 INFO] Step 5900/20000; acc:  41.29; ppl: 20.43; xent: 3.02; lr: 0.00100; 11507/9867 tok/s;   1007 sec\n",
      "[2019-02-16 13:39:30,115 INFO] Loading dataset from data/nmt15-reverse.train.0.pt, number of examples: 95844\n",
      "[2019-02-16 13:39:30,970 INFO] Step 6000/20000; acc:  41.27; ppl: 20.28; xent: 3.01; lr: 0.00100; 9888/8465 tok/s;   1022 sec\n",
      "[2019-02-16 13:39:31,065 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n",
      "[2019-02-16 13:39:47,267 INFO] Saving checkpoint /mnt/drive-2t/model/opennmt_990547e9b5cc3ef3ab8df6728e6f8669_step_6000.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meet early stop condition, prev loss: 36.793585036493106 current loss: 37.37898601616187\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'990547e9b5cc3ef3ab8df6728e6f8669'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_args = {\n",
    "    'model_dtype': 'fp32',\n",
    "    'encoder_type': 'rnn',\n",
    "    'decoder_type': 'rnn',\n",
    "    'rnn_type': 'GRU',\n",
    "    'layers': 3,\n",
    "    'max_generator_batches': 2,\n",
    "    'dropout': 0.1,\n",
    "    'enc_rnn_size': 512,\n",
    "    'dec_rnn_size': 512,\n",
    "    'src_word_vec_size': 512,\n",
    "    'tgt_word_vec_size': 512\n",
    "}\n",
    "train_args = {\n",
    "    'batch_size': 64,\n",
    "    'train_steps': 20000,\n",
    "    'optim': 'adam',\n",
    "    'learning_rate': 0.001,\n",
    "    'valid_step': 500,\n",
    "    'valid_batch': 64,\n",
    "    'early_stop_round': 5,\n",
    "    'early_stop_threshold': 0.1,\n",
    "    'report_every': 100\n",
    "}\n",
    "\n",
    "total_args = {}\n",
    "total_args.update(model_args)\n",
    "total_args.update(train_args)\n",
    "\n",
    "reverse_runner.run(total_args)\n",
    "reverse_runner.model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-16 13:42:42,908 INFO] Translating shard 0.\n",
      "[2019-02-16 13:44:19,325 INFO] Translating shard 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRED AVG SCORE: -1.1561, PRED PPL: 3.1776\n",
      "GOLD AVG SCORE: -10.0213, GOLD PPL: 22499.8106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-16 13:45:57,107 INFO] Translating shard 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRED AVG SCORE: -1.1661, PRED PPL: 3.2096\n",
      "GOLD AVG SCORE: -10.0651, GOLD PPL: 23508.4893\n",
      "PRED AVG SCORE: -1.1886, PRED PPL: 3.2823\n",
      "GOLD AVG SCORE: -10.0625, GOLD PPL: 23446.4471\n"
     ]
    }
   ],
   "source": [
    "model_id = '990547e9b5cc3ef3ab8df6728e6f8669'\n",
    "args = build_translate_args_reverse(model_id)\n",
    "args['batch_size'] = 32\n",
    "reverse_runner.translate(args, model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"data\": \"data/nmt15-reverse\",\n",
      "    \"save_model\": \"/mnt/drive-2t/model/opennmt_990547e9b5cc3ef3ab8df6728e6f8669\",\n",
      "    \"gpu_ranks\": 0,\n",
      "    \"tensorboard\": null,\n",
      "    \"tensorboard_log_dir\": \"/mnt/drive-2t/log/opennmt_990547e9b5cc3ef3ab8df6728e6f8669/log\",\n",
      "    \"model_dtype\": \"fp32\",\n",
      "    \"encoder_type\": \"rnn\",\n",
      "    \"decoder_type\": \"rnn\",\n",
      "    \"rnn_type\": \"GRU\",\n",
      "    \"layers\": 3,\n",
      "    \"max_generator_batches\": 2,\n",
      "    \"dropout\": 0.1,\n",
      "    \"enc_rnn_size\": 512,\n",
      "    \"dec_rnn_size\": 512,\n",
      "    \"src_word_vec_size\": 512,\n",
      "    \"tgt_word_vec_size\": 512,\n",
      "    \"batch_size\": 64,\n",
      "    \"train_steps\": 20000,\n",
      "    \"optim\": \"adam\",\n",
      "    \"learning_rate\": 0.001,\n",
      "    \"valid_step\": 500,\n",
      "    \"valid_batch\": 64,\n",
      "    \"early_stop_round\": 5,\n",
      "    \"early_stop_threshold\": 0.1,\n",
      "    \"report_every\": 100\n",
      "}\n",
      "23.3 11.89\n",
      "Now , before I go back to this time , I &apos;m going to show you , but I &apos;m going to talk about this , but it &apos;s a combination of time , and if you &apos;re talking about it , it &apos;s the idea of the human body .\n",
      "\n",
      "Now I just want to wrap up , and as I was mentioning earlier of course I have a lot of other data if you &apos;re interested , but I just wanted to give this sort of basic idea of being able to communicate with the brain in its language , and the potential power of being able to do that .\n",
      "-------------\n",
      "So this is a very important way to do with you , you have to speak in the brain .\n",
      "\n",
      "So it &apos;s different from the motor prosthetics where you &apos;re communicating from the brain to a device . Here we have to communicate from the outside world into the brain and be understood , and be understood by the brain .\n",
      "-------------\n",
      "And then , the last thing I want to say , however , I think that &apos;s going to be able to make it happen .\n",
      "\n",
      "And then the last thing I wanted to say , really , is to emphasize that the idea generalizes .\n",
      "-------------\n",
      "So that &apos;s the same combination of what we use is , for example , in the lab , for example , for the brain , for the brain , for the brain , and to be able to be able to control .\n",
      "\n",
      "So the same strategy that we used to find the code for the retina we can also use to find the code for other areas , for example , the auditory system and the motor system , so for treating deafness and for motor disorders .\n",
      "-------------\n",
      "In the way that we &apos;ve taken through the womb through the womb , and we &apos;re going to see the effects of the control of the locomotor vessels in the spinal cord , Chính Chính , Chính Chính .\n",
      "\n",
      "So just the same way that we were able to jump over the damaged circuitry in the retina to get to the retina &apos;s output cells , we can jump over the damaged circuitry in the cochlea to get the auditory nerve , or jump over damaged areas in the cortex , in the motor cortex , to bridge the gap produced by a stroke .\n",
      "-------------\n",
      "I just want to just want to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to be able to think about this , but it &apos;s really impossible to be able to be able to be able to be able to think .\n",
      "\n",
      "I just want to end with a simple message that understanding the code is really , really important , and if we can understand the code , the language of the brain , things become possible that didn &apos;t seem obviously possible before . Thank you .\n",
      "-------------\n",
      "Jonathan Drori : Jonathan a new story\n",
      "\n",
      "Jonathan Harris : The web as art\n",
      "-------------\n",
      "In the September February of September 2010 , the CEO of the TED Prize , is the sum of the story of TED , and the story is , &quot; Tại . &quot;\n",
      "\n",
      "At the EG conference in December 2007 , artist Jonathan Harris discusses his latest projects , which involve collecting stories : his own , strangers &apos; , and stories collected from the Internet , including his amazing &quot; We Feel Fine . &quot;\n",
      "-------------\n",
      "I &apos;m going to talk about briefly briefly briefly briefly .\n",
      "\n",
      "So I &apos;m going to talk today about collecting stories in some unconventional ways .\n",
      "-------------\n",
      "This is my first time -- a very long time .\n",
      "\n",
      "This is a picture of me from a very awkward stage in my life .\n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "evaluate_human('990547e9b5cc3ef3ab8df6728e6f8669')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-16 13:57:27,389 INFO]  * src vocab size = 8870\n",
      "[2019-02-16 13:57:27,390 INFO]  * tgt vocab size = 20071\n",
      "[2019-02-16 13:57:27,392 INFO] Building model...\n",
      "[2019-02-16 13:57:30,951 INFO] NMTModel(\n",
      "  (encoder): RNNEncoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(8870, 500, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (rnn): GRU(500, 500, num_layers=2, dropout=0.3)\n",
      "  )\n",
      "  (decoder): InputFeedRNNDecoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(20071, 500, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.3)\n",
      "    (rnn): StackedGRU(\n",
      "      (dropout): Dropout(p=0.3)\n",
      "      (layers): ModuleList(\n",
      "        (0): GRUCell(1000, 500)\n",
      "        (1): GRUCell(500, 500)\n",
      "      )\n",
      "    )\n",
      "    (attn): GlobalAttention(\n",
      "      (linear_in): Linear(in_features=500, out_features=500, bias=False)\n",
      "      (linear_out): Linear(in_features=1000, out_features=500, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (generator): Sequential(\n",
      "    (0): Linear(in_features=500, out_features=20071, bias=True)\n",
      "    (1): Cast()\n",
      "    (2): LogSoftmax()\n",
      "  )\n",
      ")\n",
      "[2019-02-16 13:57:30,952 INFO] encoder: 7441000\n",
      "[2019-02-16 13:57:30,953 INFO] decoder: 24597071\n",
      "[2019-02-16 13:57:30,953 INFO] * number of parameters: 32038071\n",
      "[2019-02-16 13:57:31,114 INFO] Starting training on GPU: [0]\n",
      "[2019-02-16 13:57:31,114 INFO] Start training loop and validate every 500 steps...\n",
      "[2019-02-16 13:57:32,579 INFO] Loading dataset from data/nmt15-reverse.train.0.pt, number of examples: 95844\n",
      "[2019-02-16 13:57:42,880 INFO] Step 100/20000; acc:   7.95; ppl: 911.43; xent: 6.82; lr: 0.00100; 13485/11478 tok/s;     12 sec\n",
      "[2019-02-16 13:57:52,799 INFO] Step 200/20000; acc:  17.67; ppl: 327.54; xent: 5.79; lr: 0.00100; 15970/13642 tok/s;     22 sec\n",
      "[2019-02-16 13:58:02,627 INFO] Step 300/20000; acc:  23.06; ppl: 193.01; xent: 5.26; lr: 0.00100; 15835/13538 tok/s;     32 sec\n",
      "[2019-02-16 13:58:12,742 INFO] Step 400/20000; acc:  26.71; ppl: 139.00; xent: 4.93; lr: 0.00100; 15785/13466 tok/s;     42 sec\n",
      "[2019-02-16 13:58:23,007 INFO] Step 500/20000; acc:  28.93; ppl: 110.80; xent: 4.71; lr: 0.00100; 15583/13208 tok/s;     52 sec\n",
      "[2019-02-16 13:58:23,111 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n",
      "[2019-02-16 13:58:37,708 INFO] Validation perplexity: 86.135\n",
      "[2019-02-16 13:58:37,708 INFO] Validation accuracy: 30.7071\n",
      "[2019-02-16 13:58:47,614 INFO] Step 600/20000; acc:  30.74; ppl: 94.17; xent: 4.55; lr: 0.00100; 6373/5435 tok/s;     76 sec\n",
      "[2019-02-16 13:58:57,490 INFO] Step 700/20000; acc:  32.27; ppl: 81.22; xent: 4.40; lr: 0.00100; 16067/13690 tok/s;     86 sec\n",
      "[2019-02-16 13:59:07,642 INFO] Step 800/20000; acc:  33.16; ppl: 73.83; xent: 4.30; lr: 0.00100; 15510/13185 tok/s;     97 sec\n",
      "[2019-02-16 13:59:17,664 INFO] Step 900/20000; acc:  34.30; ppl: 65.04; xent: 4.17; lr: 0.00100; 15804/13428 tok/s;    107 sec\n",
      "[2019-02-16 13:59:27,692 INFO] Step 1000/20000; acc:  34.69; ppl: 61.24; xent: 4.11; lr: 0.00100; 15805/13493 tok/s;    117 sec\n",
      "[2019-02-16 13:59:27,795 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n",
      "[2019-02-16 13:59:42,772 INFO] Validation perplexity: 46.0243\n",
      "[2019-02-16 13:59:42,773 INFO] Validation accuracy: 37.667\n",
      "[2019-02-16 13:59:52,903 INFO] Step 1100/20000; acc:  34.94; ppl: 59.35; xent: 4.08; lr: 0.00100; 6327/5368 tok/s;    142 sec\n",
      "[2019-02-16 14:00:02,899 INFO] Step 1200/20000; acc:  36.10; ppl: 54.29; xent: 3.99; lr: 0.00100; 15939/13528 tok/s;    152 sec\n",
      "[2019-02-16 14:00:13,275 INFO] Step 1300/20000; acc:  36.95; ppl: 49.95; xent: 3.91; lr: 0.00100; 15438/13161 tok/s;    162 sec\n",
      "[2019-02-16 14:00:23,340 INFO] Step 1400/20000; acc:  37.15; ppl: 47.85; xent: 3.87; lr: 0.00100; 15595/13322 tok/s;    172 sec\n",
      "[2019-02-16 14:00:34,696 INFO] Loading dataset from data/nmt15-reverse.train.0.pt, number of examples: 95844\n",
      "[2019-02-16 14:00:35,215 INFO] Step 1500/20000; acc:  37.25; ppl: 45.85; xent: 3.83; lr: 0.00100; 13392/11397 tok/s;    184 sec\n",
      "[2019-02-16 14:00:35,317 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n",
      "[2019-02-16 14:00:50,385 INFO] Validation perplexity: 34.798\n",
      "[2019-02-16 14:00:50,386 INFO] Validation accuracy: 40.0317\n",
      "[2019-02-16 14:01:00,358 INFO] Step 1600/20000; acc:  38.41; ppl: 40.95; xent: 3.71; lr: 0.00100; 6260/5334 tok/s;    209 sec\n",
      "[2019-02-16 14:01:10,339 INFO] Step 1700/20000; acc:  38.31; ppl: 40.51; xent: 3.70; lr: 0.00100; 15776/13489 tok/s;    219 sec\n",
      "[2019-02-16 14:01:20,115 INFO] Step 1800/20000; acc:  39.50; ppl: 36.39; xent: 3.59; lr: 0.00100; 15663/13432 tok/s;    229 sec\n",
      "[2019-02-16 14:01:30,376 INFO] Step 1900/20000; acc:  39.02; ppl: 37.10; xent: 3.61; lr: 0.00100; 15696/13366 tok/s;    239 sec\n",
      "[2019-02-16 14:01:40,658 INFO] Step 2000/20000; acc:  39.75; ppl: 33.77; xent: 3.52; lr: 0.00100; 15435/13074 tok/s;    250 sec\n",
      "[2019-02-16 14:01:40,753 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n",
      "[2019-02-16 14:01:56,052 INFO] Validation perplexity: 31.0773\n",
      "[2019-02-16 14:01:56,053 INFO] Validation accuracy: 40.8754\n",
      "[2019-02-16 14:02:06,107 INFO] Step 2100/20000; acc:  40.10; ppl: 33.33; xent: 3.51; lr: 0.00100; 6262/5333 tok/s;    275 sec\n",
      "[2019-02-16 14:02:15,954 INFO] Step 2200/20000; acc:  40.74; ppl: 31.12; xent: 3.44; lr: 0.00100; 15863/13533 tok/s;    285 sec\n",
      "[2019-02-16 14:02:26,293 INFO] Step 2300/20000; acc:  40.32; ppl: 31.90; xent: 3.46; lr: 0.00100; 15467/13155 tok/s;    295 sec\n",
      "[2019-02-16 14:02:36,274 INFO] Step 2400/20000; acc:  41.35; ppl: 28.55; xent: 3.35; lr: 0.00100; 15705/13360 tok/s;    305 sec\n",
      "[2019-02-16 14:02:46,278 INFO] Step 2500/20000; acc:  41.16; ppl: 28.77; xent: 3.36; lr: 0.00100; 15826/13489 tok/s;    315 sec\n",
      "[2019-02-16 14:02:46,375 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n",
      "[2019-02-16 14:03:01,657 INFO] Validation perplexity: 26.0131\n",
      "[2019-02-16 14:03:01,658 INFO] Validation accuracy: 42.9494\n",
      "[2019-02-16 14:03:11,999 INFO] Step 2600/20000; acc:  41.07; ppl: 29.26; xent: 3.38; lr: 0.00100; 6324/5359 tok/s;    341 sec\n",
      "[2019-02-16 14:03:21,821 INFO] Step 2700/20000; acc:  41.63; ppl: 27.52; xent: 3.31; lr: 0.00100; 15811/13455 tok/s;    351 sec\n",
      "[2019-02-16 14:03:32,376 INFO] Step 2800/20000; acc:  41.59; ppl: 27.49; xent: 3.31; lr: 0.00100; 15397/13100 tok/s;    361 sec\n",
      "[2019-02-16 14:03:42,399 INFO] Step 2900/20000; acc:  42.08; ppl: 26.56; xent: 3.28; lr: 0.00100; 15455/13212 tok/s;    371 sec\n",
      "[2019-02-16 14:03:53,831 INFO] Loading dataset from data/nmt15-reverse.train.0.pt, number of examples: 95844\n",
      "[2019-02-16 14:03:54,523 INFO] Step 3000/20000; acc:  41.73; ppl: 26.89; xent: 3.29; lr: 0.00100; 13239/11265 tok/s;    383 sec\n",
      "[2019-02-16 14:03:54,620 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n",
      "[2019-02-16 14:04:09,705 INFO] Validation perplexity: 22.9626\n",
      "[2019-02-16 14:04:09,705 INFO] Validation accuracy: 44.1904\n",
      "[2019-02-16 14:04:19,789 INFO] Step 3100/20000; acc:  42.63; ppl: 24.87; xent: 3.21; lr: 0.00100; 6257/5333 tok/s;    409 sec\n",
      "[2019-02-16 14:04:29,996 INFO] Step 3200/20000; acc:  41.95; ppl: 25.90; xent: 3.25; lr: 0.00100; 15723/13406 tok/s;    419 sec\n",
      "[2019-02-16 14:04:39,633 INFO] Step 3300/20000; acc:  43.59; ppl: 22.75; xent: 3.12; lr: 0.00100; 15504/13305 tok/s;    429 sec\n",
      "[2019-02-16 14:04:50,083 INFO] Step 3400/20000; acc:  43.03; ppl: 23.65; xent: 3.16; lr: 0.00100; 15635/13324 tok/s;    439 sec\n",
      "[2019-02-16 14:05:00,363 INFO] Step 3500/20000; acc:  43.01; ppl: 23.27; xent: 3.15; lr: 0.00100; 15323/12986 tok/s;    449 sec\n",
      "[2019-02-16 14:05:00,460 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n",
      "[2019-02-16 14:05:15,760 INFO] Saving checkpoint /mnt/drive-2t/model/opennmt_f40ce12cd75f5edc1a96a6e26f46ed4e_step_3500.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meet early stop condition, prev best loss: 44.19043978415284 current loss: 40.965622694177995\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'f40ce12cd75f5edc1a96a6e26f46ed4e'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_args = {\n",
    "    'model_dtype': 'fp32',\n",
    "    'encoder_type': 'rnn',\n",
    "    'decoder_type': 'rnn',\n",
    "    'rnn_type': 'GRU',\n",
    "#     'layers': 3,\n",
    "#     'max_generator_batches': 2,\n",
    "    'dropout': 0.3,\n",
    "#     'enc_rnn_size': 512,\n",
    "#     'dec_rnn_size': 512,\n",
    "#     'src_word_vec_size': 512,\n",
    "#     'tgt_word_vec_size': 512\n",
    "}\n",
    "train_args = {\n",
    "    'batch_size': 64,\n",
    "    'train_steps': 20000,\n",
    "    'optim': 'adam',\n",
    "    'learning_rate': 0.001,\n",
    "    'valid_step': 500,\n",
    "    'valid_batch': 64,\n",
    "    'early_stop_round': 5,\n",
    "    'early_stop_threshold': 0.1,\n",
    "    'report_every': 100\n",
    "}\n",
    "\n",
    "total_args = {}\n",
    "total_args.update(model_args)\n",
    "total_args.update(train_args)\n",
    "\n",
    "reverse_runner.run(total_args)\n",
    "reverse_runner.model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-16 14:09:21,216 INFO] Translating shard 0.\n",
      "[2019-02-16 14:10:34,134 INFO] Translating shard 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRED AVG SCORE: -1.0952, PRED PPL: 2.9897\n",
      "GOLD AVG SCORE: -9.4220, GOLD PPL: 12357.4480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-16 14:11:48,342 INFO] Translating shard 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRED AVG SCORE: -1.1152, PRED PPL: 3.0503\n",
      "GOLD AVG SCORE: -9.4861, GOLD PPL: 13175.3264\n",
      "PRED AVG SCORE: -1.1401, PRED PPL: 3.1272\n",
      "GOLD AVG SCORE: -9.5160, GOLD PPL: 13575.0565\n"
     ]
    }
   ],
   "source": [
    "model_id = 'f40ce12cd75f5edc1a96a6e26f46ed4e'\n",
    "args = build_translate_args_reverse(model_id)\n",
    "args['batch_size'] = 32\n",
    "reverse_runner.translate(args, model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"data\": \"data/nmt15-reverse\",\n",
      "    \"save_model\": \"/mnt/drive-2t/model/opennmt_f40ce12cd75f5edc1a96a6e26f46ed4e\",\n",
      "    \"gpu_ranks\": 0,\n",
      "    \"tensorboard\": null,\n",
      "    \"tensorboard_log_dir\": \"/mnt/drive-2t/log/opennmt_f40ce12cd75f5edc1a96a6e26f46ed4e/log\",\n",
      "    \"model_dtype\": \"fp32\",\n",
      "    \"encoder_type\": \"rnn\",\n",
      "    \"decoder_type\": \"rnn\",\n",
      "    \"rnn_type\": \"GRU\",\n",
      "    \"dropout\": 0.3,\n",
      "    \"batch_size\": 64,\n",
      "    \"train_steps\": 20000,\n",
      "    \"optim\": \"adam\",\n",
      "    \"learning_rate\": 0.001,\n",
      "    \"valid_step\": 500,\n",
      "    \"valid_batch\": 64,\n",
      "    \"early_stop_round\": 5,\n",
      "    \"early_stop_threshold\": 0.1,\n",
      "    \"report_every\": 100\n",
      "}\n",
      "21.52 12.44\n",
      "Now , this time I ended up , and I mentioned earlier , of course , of course , but I wanted to give this idea of this idea about its body .\n",
      "\n",
      "Now I just want to wrap up , and as I was mentioning earlier of course I have a lot of other data if you &apos;re interested , but I just wanted to give this sort of basic idea of being able to communicate with the brain in its language , and the potential power of being able to do that .\n",
      "-------------\n",
      "This is a very different thing from that you have to protect from the brain from the brain to the brain , and we have to have to bridge .\n",
      "\n",
      "So it &apos;s different from the motor prosthetics where you &apos;re communicating from the brain to a device . Here we have to communicate from the outside world into the brain and be understood , and be understood by the brain .\n",
      "-------------\n",
      "And then , the last thing I wanted to say , well , I want to stress that this idea can be saved .\n",
      "\n",
      "And then the last thing I wanted to say , really , is to emphasize that the idea generalizes .\n",
      "-------------\n",
      "The strategy we use the code for the code , we can also use the code for other parts , for example , and to treat your blind habits .\n",
      "\n",
      "So the same strategy that we used to find the code for the retina we can also use to find the code for other areas , for example , the auditory system and the motor system , so for treating deafness and for motor disorders .\n",
      "-------------\n",
      "In the way we ignore the site through the retina to the retina that the retina of the retina , we can ignore through the metals in order to be able to stick in the environment .\n",
      "\n",
      "So just the same way that we were able to jump over the damaged circuitry in the retina to get to the retina &apos;s output cells , we can jump over the damaged circuitry in the cochlea to get the auditory nerve , or jump over damaged areas in the cortex , in the motor cortex , to bridge the gap produced by a stroke .\n",
      "-------------\n",
      "I just want to finish by a simple message message message , really important , and if we can understand the code , the language of the brain . Thank you .\n",
      "\n",
      "I just want to end with a simple message that understanding the code is really , really important , and if we can understand the code , the language of the brain , things become possible that didn &apos;t seem obviously possible before . Thank you .\n",
      "-------------\n",
      "Jonathan Drori : Stories\n",
      "\n",
      "Jonathan Harris : The web as art\n",
      "-------------\n",
      "In the October of June 2007 , the artist bàn his best project , including his identity , and from the Internet , there &apos;s a great project . &quot;\n",
      "\n",
      "At the EG conference in December 2007 , artist Jonathan Harris discusses his latest projects , which involve collecting stories : his own , strangers &apos; , and stories collected from the Internet , including his amazing &quot; We Feel Fine . &quot;\n",
      "-------------\n",
      "Today , I &apos;m going to talk about the assumptions of different ways .\n",
      "\n",
      "So I &apos;m going to talk today about collecting stories in some unconventional ways .\n",
      "-------------\n",
      "This is a aerial picture -- I &apos;m very excited .\n",
      "\n",
      "This is a picture of me from a very awkward stage in my life .\n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "evaluate_human('f40ce12cd75f5edc1a96a6e26f46ed4e')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-16 14:23:42,127 INFO]  * src vocab size = 8870\n",
      "[2019-02-16 14:23:42,128 INFO]  * tgt vocab size = 20071\n",
      "[2019-02-16 14:23:42,130 INFO] Building model...\n",
      "[2019-02-16 14:23:45,681 INFO] NMTModel(\n",
      "  (encoder): RNNEncoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(8870, 500, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (rnn): LSTM(500, 500, num_layers=2, dropout=0.3)\n",
      "  )\n",
      "  (decoder): InputFeedRNNDecoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(20071, 500, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.3)\n",
      "    (rnn): StackedLSTM(\n",
      "      (dropout): Dropout(p=0.3)\n",
      "      (layers): ModuleList(\n",
      "        (0): LSTMCell(1000, 500)\n",
      "        (1): LSTMCell(500, 500)\n",
      "      )\n",
      "    )\n",
      "    (attn): GlobalAttention(\n",
      "      (linear_in): Linear(in_features=500, out_features=500, bias=False)\n",
      "      (linear_out): Linear(in_features=1000, out_features=500, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (generator): Sequential(\n",
      "    (0): Linear(in_features=500, out_features=20071, bias=True)\n",
      "    (1): Cast()\n",
      "    (2): LogSoftmax()\n",
      "  )\n",
      ")\n",
      "[2019-02-16 14:23:45,683 INFO] encoder: 8443000\n",
      "[2019-02-16 14:23:45,683 INFO] decoder: 25849071\n",
      "[2019-02-16 14:23:45,683 INFO] * number of parameters: 34292071\n",
      "[2019-02-16 14:23:45,836 INFO] Starting training on GPU: [0]\n",
      "[2019-02-16 14:23:45,836 INFO] Start training loop and validate every 500 steps...\n",
      "[2019-02-16 14:23:47,279 INFO] Loading dataset from data/nmt15-reverse.train.0.pt, number of examples: 95844\n",
      "[2019-02-16 14:23:57,872 INFO] Step 100/20000; acc:   9.18; ppl: 700.25; xent: 6.55; lr: 0.00100; 13159/11196 tok/s;     12 sec\n",
      "[2019-02-16 14:24:08,750 INFO] Step 200/20000; acc:  16.46; ppl: 311.52; xent: 5.74; lr: 0.00100; 14453/12298 tok/s;     23 sec\n",
      "[2019-02-16 14:24:19,491 INFO] Step 300/20000; acc:  21.88; ppl: 193.85; xent: 5.27; lr: 0.00100; 14748/12569 tok/s;     34 sec\n",
      "[2019-02-16 14:24:30,381 INFO] Step 400/20000; acc:  25.11; ppl: 149.56; xent: 5.01; lr: 0.00100; 14419/12333 tok/s;     45 sec\n",
      "[2019-02-16 14:24:41,061 INFO] Step 500/20000; acc:  27.30; ppl: 127.38; xent: 4.85; lr: 0.00100; 14648/12499 tok/s;     55 sec\n",
      "[2019-02-16 14:24:41,168 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n",
      "[2019-02-16 14:24:57,445 INFO] Validation perplexity: 99.5566\n",
      "[2019-02-16 14:24:57,446 INFO] Validation accuracy: 29.4483\n",
      "[2019-02-16 14:25:08,074 INFO] Step 600/20000; acc:  29.83; ppl: 105.10; xent: 4.65; lr: 0.00100; 5892/5021 tok/s;     82 sec\n",
      "[2019-02-16 14:25:19,075 INFO] Step 700/20000; acc:  31.80; ppl: 89.00; xent: 4.49; lr: 0.00100; 14513/12369 tok/s;     93 sec\n",
      "[2019-02-16 14:25:30,059 INFO] Step 800/20000; acc:  33.68; ppl: 75.74; xent: 4.33; lr: 0.00100; 14304/12212 tok/s;    104 sec\n",
      "[2019-02-16 14:25:41,186 INFO] Step 900/20000; acc:  34.86; ppl: 68.20; xent: 4.22; lr: 0.00100; 14440/12335 tok/s;    115 sec\n",
      "[2019-02-16 14:25:52,004 INFO] Step 1000/20000; acc:  36.53; ppl: 58.96; xent: 4.08; lr: 0.00100; 14707/12450 tok/s;    126 sec\n",
      "[2019-02-16 14:25:52,106 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n",
      "[2019-02-16 14:26:09,584 INFO] Validation perplexity: 46.6118\n",
      "[2019-02-16 14:26:09,585 INFO] Validation accuracy: 38.7578\n",
      "[2019-02-16 14:26:20,585 INFO] Step 1100/20000; acc:  37.61; ppl: 53.87; xent: 3.99; lr: 0.00100; 5603/4760 tok/s;    155 sec\n",
      "[2019-02-16 14:26:31,309 INFO] Step 1200/20000; acc:  39.16; ppl: 47.45; xent: 3.86; lr: 0.00100; 14883/12685 tok/s;    165 sec\n",
      "[2019-02-16 14:26:41,658 INFO] Step 1300/20000; acc:  39.73; ppl: 43.99; xent: 3.78; lr: 0.00100; 15073/12857 tok/s;    176 sec\n",
      "[2019-02-16 14:26:52,259 INFO] Step 1400/20000; acc:  41.02; ppl: 39.95; xent: 3.69; lr: 0.00100; 14824/12607 tok/s;    186 sec\n",
      "[2019-02-16 14:27:04,569 INFO] Loading dataset from data/nmt15-reverse.train.0.pt, number of examples: 95844\n",
      "[2019-02-16 14:27:04,845 INFO] Step 1500/20000; acc:  41.80; ppl: 37.22; xent: 3.62; lr: 0.00100; 12432/10584 tok/s;    199 sec\n",
      "[2019-02-16 14:27:04,939 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n",
      "[2019-02-16 14:27:21,563 INFO] Validation perplexity: 30.5045\n",
      "[2019-02-16 14:27:21,564 INFO] Validation accuracy: 43.7726\n",
      "[2019-02-16 14:27:32,184 INFO] Step 1600/20000; acc:  42.23; ppl: 34.80; xent: 3.55; lr: 0.00100; 5835/4955 tok/s;    226 sec\n",
      "[2019-02-16 14:27:43,447 INFO] Step 1700/20000; acc:  42.66; ppl: 33.44; xent: 3.51; lr: 0.00100; 14473/12247 tok/s;    238 sec\n",
      "[2019-02-16 14:27:54,135 INFO] Step 1800/20000; acc:  44.24; ppl: 28.88; xent: 3.36; lr: 0.00100; 14444/12387 tok/s;    248 sec\n",
      "[2019-02-16 14:28:04,768 INFO] Step 1900/20000; acc:  44.57; ppl: 27.77; xent: 3.32; lr: 0.00100; 14637/12515 tok/s;    259 sec\n",
      "[2019-02-16 14:28:15,534 INFO] Step 2000/20000; acc:  44.80; ppl: 27.25; xent: 3.31; lr: 0.00100; 14376/12276 tok/s;    270 sec\n",
      "[2019-02-16 14:28:15,631 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n",
      "[2019-02-16 14:28:32,847 INFO] Validation perplexity: 23.7529\n",
      "[2019-02-16 14:28:32,848 INFO] Validation accuracy: 46.5494\n",
      "[2019-02-16 14:28:43,502 INFO] Step 2100/20000; acc:  45.17; ppl: 26.09; xent: 3.26; lr: 0.00100; 5716/4867 tok/s;    298 sec\n",
      "[2019-02-16 14:28:54,386 INFO] Step 2200/20000; acc:  45.51; ppl: 25.25; xent: 3.23; lr: 0.00100; 14711/12536 tok/s;    309 sec\n",
      "[2019-02-16 14:29:05,370 INFO] Step 2300/20000; acc:  46.08; ppl: 23.53; xent: 3.16; lr: 0.00100; 14235/12167 tok/s;    320 sec\n",
      "[2019-02-16 14:29:16,251 INFO] Step 2400/20000; acc:  46.12; ppl: 23.40; xent: 3.15; lr: 0.00100; 14753/12589 tok/s;    330 sec\n",
      "[2019-02-16 14:29:27,007 INFO] Step 2500/20000; acc:  46.98; ppl: 21.72; xent: 3.08; lr: 0.00100; 14804/12534 tok/s;    341 sec\n",
      "[2019-02-16 14:29:27,104 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n",
      "[2019-02-16 14:29:44,006 INFO] Validation perplexity: 19.8926\n",
      "[2019-02-16 14:29:44,007 INFO] Validation accuracy: 48.2576\n",
      "[2019-02-16 14:29:55,162 INFO] Step 2600/20000; acc:  47.02; ppl: 21.39; xent: 3.06; lr: 0.00100; 5681/4825 tok/s;    369 sec\n",
      "[2019-02-16 14:30:06,014 INFO] Step 2700/20000; acc:  47.66; ppl: 20.11; xent: 3.00; lr: 0.00100; 14815/12625 tok/s;    380 sec\n",
      "[2019-02-16 14:30:16,497 INFO] Step 2800/20000; acc:  47.96; ppl: 19.63; xent: 2.98; lr: 0.00100; 14893/12710 tok/s;    391 sec\n",
      "[2019-02-16 14:30:27,189 INFO] Step 2900/20000; acc:  48.52; ppl: 18.83; xent: 2.94; lr: 0.00100; 14698/12491 tok/s;    401 sec\n",
      "[2019-02-16 14:30:39,448 INFO] Loading dataset from data/nmt15-reverse.train.0.pt, number of examples: 95844\n",
      "[2019-02-16 14:30:40,067 INFO] Step 3000/20000; acc:  48.52; ppl: 18.40; xent: 2.91; lr: 0.00100; 12378/10516 tok/s;    414 sec\n",
      "[2019-02-16 14:30:40,162 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n",
      "[2019-02-16 14:30:56,910 INFO] Validation perplexity: 17.6572\n",
      "[2019-02-16 14:30:56,911 INFO] Validation accuracy: 49.5213\n",
      "[2019-02-16 14:31:07,615 INFO] Step 3100/20000; acc:  48.70; ppl: 17.85; xent: 2.88; lr: 0.00100; 5744/4889 tok/s;    442 sec\n",
      "[2019-02-16 14:31:18,627 INFO] Step 3200/20000; acc:  48.75; ppl: 17.71; xent: 2.87; lr: 0.00100; 14558/12319 tok/s;    453 sec\n",
      "[2019-02-16 14:31:28,989 INFO] Step 3300/20000; acc:  50.08; ppl: 15.72; xent: 2.75; lr: 0.00100; 14906/12808 tok/s;    463 sec\n",
      "[2019-02-16 14:31:39,770 INFO] Step 3400/20000; acc:  50.11; ppl: 15.87; xent: 2.76; lr: 0.00100; 14669/12509 tok/s;    474 sec\n",
      "[2019-02-16 14:31:50,415 INFO] Step 3500/20000; acc:  50.00; ppl: 15.84; xent: 2.76; lr: 0.00100; 14452/12370 tok/s;    485 sec\n",
      "[2019-02-16 14:31:50,512 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n",
      "[2019-02-16 14:32:07,532 INFO] Validation perplexity: 16.1593\n",
      "[2019-02-16 14:32:07,533 INFO] Validation accuracy: 50.4303\n",
      "[2019-02-16 14:32:18,139 INFO] Step 3600/20000; acc:  50.42; ppl: 15.55; xent: 2.74; lr: 0.00100; 5709/4866 tok/s;    512 sec\n",
      "[2019-02-16 14:32:28,956 INFO] Step 3700/20000; acc:  50.36; ppl: 15.39; xent: 2.73; lr: 0.00100; 14839/12624 tok/s;    523 sec\n",
      "[2019-02-16 14:32:39,796 INFO] Step 3800/20000; acc:  50.65; ppl: 14.79; xent: 2.69; lr: 0.00100; 14517/12388 tok/s;    534 sec\n",
      "[2019-02-16 14:32:50,604 INFO] Step 3900/20000; acc:  50.78; ppl: 14.82; xent: 2.70; lr: 0.00100; 14744/12614 tok/s;    545 sec\n",
      "[2019-02-16 14:33:01,326 INFO] Step 4000/20000; acc:  51.17; ppl: 14.13; xent: 2.65; lr: 0.00100; 14808/12512 tok/s;    555 sec\n",
      "[2019-02-16 14:33:01,426 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n",
      "[2019-02-16 14:33:18,380 INFO] Validation perplexity: 15.1622\n",
      "[2019-02-16 14:33:18,381 INFO] Validation accuracy: 51.153\n",
      "[2019-02-16 14:33:29,580 INFO] Step 4100/20000; acc:  50.96; ppl: 14.18; xent: 2.65; lr: 0.00100; 5681/4827 tok/s;    584 sec\n",
      "[2019-02-16 14:33:40,497 INFO] Step 4200/20000; acc:  51.34; ppl: 13.72; xent: 2.62; lr: 0.00100; 14726/12548 tok/s;    595 sec\n",
      "[2019-02-16 14:33:51,053 INFO] Step 4300/20000; acc:  51.77; ppl: 13.37; xent: 2.59; lr: 0.00100; 14861/12683 tok/s;    605 sec\n",
      "[2019-02-16 14:34:01,781 INFO] Step 4400/20000; acc:  52.13; ppl: 13.02; xent: 2.57; lr: 0.00100; 14676/12490 tok/s;    616 sec\n",
      "[2019-02-16 14:34:13,886 INFO] Loading dataset from data/nmt15-reverse.train.0.pt, number of examples: 95844\n",
      "[2019-02-16 14:34:14,632 INFO] Step 4500/20000; acc:  52.33; ppl: 12.83; xent: 2.55; lr: 0.00100; 12294/10440 tok/s;    629 sec\n",
      "[2019-02-16 14:34:14,729 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n",
      "[2019-02-16 14:34:31,705 INFO] Validation perplexity: 14.3939\n",
      "[2019-02-16 14:34:31,706 INFO] Validation accuracy: 51.8758\n",
      "[2019-02-16 14:34:42,356 INFO] Step 4600/20000; acc:  52.21; ppl: 12.74; xent: 2.54; lr: 0.00100; 5703/4845 tok/s;    657 sec\n",
      "[2019-02-16 14:34:53,454 INFO] Step 4700/20000; acc:  52.12; ppl: 12.80; xent: 2.55; lr: 0.00100; 14558/12346 tok/s;    668 sec\n",
      "[2019-02-16 14:35:03,929 INFO] Step 4800/20000; acc:  53.30; ppl: 11.59; xent: 2.45; lr: 0.00100; 14680/12583 tok/s;    678 sec\n",
      "[2019-02-16 14:35:15,062 INFO] Step 4900/20000; acc:  53.21; ppl: 11.72; xent: 2.46; lr: 0.00100; 14425/12284 tok/s;    689 sec\n",
      "[2019-02-16 14:35:25,920 INFO] Step 5000/20000; acc:  53.02; ppl: 11.71; xent: 2.46; lr: 0.00100; 14045/12043 tok/s;    700 sec\n",
      "[2019-02-16 14:35:26,017 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n",
      "[2019-02-16 14:35:42,754 INFO] Validation perplexity: 13.8041\n",
      "[2019-02-16 14:35:42,754 INFO] Validation accuracy: 52.3136\n",
      "[2019-02-16 14:35:42,755 INFO] Saving checkpoint /mnt/drive-2t/model/opennmt_22846dcbc32b2f51a395010cde5d7f85_step_5000.pt\n",
      "[2019-02-16 14:35:55,560 INFO] Step 5100/20000; acc:  53.26; ppl: 11.75; xent: 2.46; lr: 0.00100; 5342/4557 tok/s;    730 sec\n",
      "[2019-02-16 14:36:06,209 INFO] Step 5200/20000; acc:  53.33; ppl: 11.57; xent: 2.45; lr: 0.00100; 14909/12690 tok/s;    740 sec\n",
      "[2019-02-16 14:36:16,942 INFO] Step 5300/20000; acc:  53.54; ppl: 11.26; xent: 2.42; lr: 0.00100; 14735/12577 tok/s;    751 sec\n",
      "[2019-02-16 14:36:27,600 INFO] Step 5400/20000; acc:  53.48; ppl: 11.41; xent: 2.43; lr: 0.00100; 14863/12702 tok/s;    762 sec\n",
      "[2019-02-16 14:36:38,531 INFO] Step 5500/20000; acc:  53.45; ppl: 11.21; xent: 2.42; lr: 0.00100; 14945/12617 tok/s;    773 sec\n",
      "[2019-02-16 14:36:38,630 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n",
      "[2019-02-16 14:36:55,319 INFO] Validation perplexity: 13.434\n",
      "[2019-02-16 14:36:55,320 INFO] Validation accuracy: 52.5421\n",
      "[2019-02-16 14:37:06,156 INFO] Step 5600/20000; acc:  53.85; ppl: 10.96; xent: 2.39; lr: 0.00100; 5647/4816 tok/s;    800 sec\n",
      "[2019-02-16 14:37:17,026 INFO] Step 5700/20000; acc:  54.15; ppl: 10.64; xent: 2.37; lr: 0.00100; 14854/12624 tok/s;    811 sec\n",
      "[2019-02-16 14:37:27,432 INFO] Step 5800/20000; acc:  54.16; ppl: 10.57; xent: 2.36; lr: 0.00100; 15024/12824 tok/s;    822 sec\n",
      "[2019-02-16 14:37:38,165 INFO] Step 5900/20000; acc:  54.44; ppl: 10.52; xent: 2.35; lr: 0.00100; 14888/12653 tok/s;    832 sec\n",
      "[2019-02-16 14:37:49,978 INFO] Loading dataset from data/nmt15-reverse.train.0.pt, number of examples: 95844\n",
      "[2019-02-16 14:37:50,908 INFO] Step 6000/20000; acc:  54.46; ppl: 10.32; xent: 2.33; lr: 0.00100; 12299/10463 tok/s;    845 sec\n",
      "[2019-02-16 14:37:51,037 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n",
      "[2019-02-16 14:38:07,623 INFO] Validation perplexity: 13.0838\n",
      "[2019-02-16 14:38:07,623 INFO] Validation accuracy: 52.9737\n",
      "[2019-02-16 14:38:18,131 INFO] Step 6100/20000; acc:  54.59; ppl: 10.21; xent: 2.32; lr: 0.00100; 5794/4908 tok/s;    872 sec\n",
      "[2019-02-16 14:38:29,203 INFO] Step 6200/20000; acc:  54.45; ppl: 10.33; xent: 2.34; lr: 0.00100; 14602/12422 tok/s;    883 sec\n",
      "[2019-02-16 14:38:39,435 INFO] Step 6300/20000; acc:  55.66; ppl:  9.45; xent: 2.25; lr: 0.00100; 14971/12824 tok/s;    894 sec\n",
      "[2019-02-16 14:38:50,233 INFO] Step 6400/20000; acc:  55.51; ppl:  9.60; xent: 2.26; lr: 0.00100; 14982/12744 tok/s;    904 sec\n",
      "[2019-02-16 14:39:01,047 INFO] Step 6500/20000; acc:  54.68; ppl:  9.95; xent: 2.30; lr: 0.00100; 14583/12475 tok/s;    915 sec\n",
      "[2019-02-16 14:39:01,144 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n",
      "[2019-02-16 14:39:17,916 INFO] Validation perplexity: 12.7305\n",
      "[2019-02-16 14:39:17,917 INFO] Validation accuracy: 53.2826\n",
      "[2019-02-16 14:39:28,346 INFO] Step 6600/20000; acc:  55.42; ppl:  9.61; xent: 2.26; lr: 0.00100; 5714/4879 tok/s;    943 sec\n",
      "[2019-02-16 14:39:38,731 INFO] Step 6700/20000; acc:  55.85; ppl:  9.38; xent: 2.24; lr: 0.00100; 14888/12702 tok/s;    953 sec\n",
      "[2019-02-16 14:39:50,087 INFO] Step 6800/20000; acc:  55.52; ppl:  9.46; xent: 2.25; lr: 0.00100; 14161/12094 tok/s;    964 sec\n",
      "[2019-02-16 14:40:01,404 INFO] Step 6900/20000; acc:  55.36; ppl:  9.61; xent: 2.26; lr: 0.00100; 14050/11984 tok/s;    976 sec\n",
      "[2019-02-16 14:40:12,871 INFO] Step 7000/20000; acc:  55.66; ppl:  9.30; xent: 2.23; lr: 0.00100; 14001/11843 tok/s;    987 sec\n",
      "[2019-02-16 14:40:12,968 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n",
      "[2019-02-16 14:40:30,115 INFO] Saving checkpoint /mnt/drive-2t/model/opennmt_22846dcbc32b2f51a395010cde5d7f85_step_7000.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meet early stop condition, prev best loss: 53.28260154506743 current loss: 53.23859646003538\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'22846dcbc32b2f51a395010cde5d7f85'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_args = {\n",
    "    'model_dtype': 'fp32',\n",
    "    'encoder_type': 'rnn',\n",
    "    'decoder_type': 'rnn',\n",
    "#     'rnn_type': 'GRU',\n",
    "#     'layers': 3,\n",
    "#     'max_generator_batches': 2,\n",
    "    'dropout': 0.3,\n",
    "#     'enc_rnn_size': 512,\n",
    "#     'dec_rnn_size': 512,\n",
    "#     'src_word_vec_size': 512,\n",
    "#     'tgt_word_vec_size': 512\n",
    "}\n",
    "train_args = {\n",
    "    'batch_size': 64,\n",
    "    'train_steps': 20000,\n",
    "    'optim': 'adam',\n",
    "    'learning_rate': 0.001,\n",
    "    'valid_step': 500,\n",
    "    'valid_batch': 64,\n",
    "    'early_stop_round': 5,\n",
    "    'early_stop_threshold': 0.1,\n",
    "    'report_every': 100\n",
    "}\n",
    "\n",
    "total_args = {}\n",
    "total_args.update(model_args)\n",
    "total_args.update(train_args)\n",
    "\n",
    "reverse_runner.run(total_args)\n",
    "reverse_runner.model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-16 14:44:18,390 INFO] Translating shard 0.\n",
      "[2019-02-16 14:45:53,548 INFO] Translating shard 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRED AVG SCORE: -0.7247, PRED PPL: 2.0640\n",
      "GOLD AVG SCORE: -10.3031, GOLD PPL: 29825.0073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-16 14:47:31,194 INFO] Translating shard 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRED AVG SCORE: -0.7346, PRED PPL: 2.0846\n",
      "GOLD AVG SCORE: -10.2928, GOLD PPL: 29518.2979\n",
      "PRED AVG SCORE: -0.7631, PRED PPL: 2.1449\n",
      "GOLD AVG SCORE: -10.2362, GOLD PPL: 27895.0392\n"
     ]
    }
   ],
   "source": [
    "model_id = '22846dcbc32b2f51a395010cde5d7f85'\n",
    "args = build_translate_args_reverse(model_id)\n",
    "args['batch_size'] = 32\n",
    "reverse_runner.translate(args, model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"data\": \"data/nmt15-reverse\",\n",
      "    \"save_model\": \"/mnt/drive-2t/model/opennmt_22846dcbc32b2f51a395010cde5d7f85\",\n",
      "    \"gpu_ranks\": 0,\n",
      "    \"tensorboard\": null,\n",
      "    \"tensorboard_log_dir\": \"/mnt/drive-2t/log/opennmt_22846dcbc32b2f51a395010cde5d7f85/log\",\n",
      "    \"model_dtype\": \"fp32\",\n",
      "    \"encoder_type\": \"rnn\",\n",
      "    \"decoder_type\": \"rnn\",\n",
      "    \"dropout\": 0.3,\n",
      "    \"batch_size\": 64,\n",
      "    \"train_steps\": 20000,\n",
      "    \"optim\": \"adam\",\n",
      "    \"learning_rate\": 0.001,\n",
      "    \"valid_step\": 500,\n",
      "    \"valid_batch\": 64,\n",
      "    \"early_stop_round\": 5,\n",
      "    \"early_stop_threshold\": 0.1,\n",
      "    \"report_every\": 100\n",
      "}\n",
      "35.56 21.0\n",
      "Now , it &apos;s time that I ended up , and as I mentioned earlier , I had a lot of other data if you &apos;re interested in , but I just wanted to give this basic idea about communication with the brain of it , and the potential of that ability to be the potential of that ability .\n",
      "\n",
      "Now I just want to wrap up , and as I was mentioning earlier of course I have a lot of other data if you &apos;re interested , but I just wanted to give this sort of basic idea of being able to communicate with the brain in its language , and the potential power of being able to do that .\n",
      "-------------\n",
      "This is very different from the artificial device that you have to bridge the brain from the brain to a device . Here we have to be bắc from the world outside in the brain , and understand , brains understand .\n",
      "\n",
      "So it &apos;s different from the motor prosthetics where you &apos;re communicating from the brain to a device . Here we have to communicate from the outside world into the brain and be understood , and be understood by the brain .\n",
      "-------------\n",
      "And then , the last thing I &apos;d like to say , well , I want to highlight that idea can contribute .\n",
      "\n",
      "And then the last thing I wanted to say , really , is to emphasize that the idea generalizes .\n",
      "-------------\n",
      "The strategy that we &apos;re using to look at the code for the retina , we can also use the code for other areas , like the hearing and the skeleton , to treat hearing messaging and chuyển .\n",
      "\n",
      "So the same strategy that we used to find the code for the retina we can also use to find the code for other areas , for example , the auditory system and the motor system , so for treating deafness and for motor disorders .\n",
      "-------------\n",
      "It &apos;s by the way we go through the system that &apos;s broken in the retina to get the stem cells of the retina , and we can also go through the circuits that are broken in the cochlea to the auditory nerves , or qua through suburbia areas , in the prefrontal cortex , in the prefrontal cortex , to trám in the hở of the quỵ .\n",
      "\n",
      "So just the same way that we were able to jump over the damaged circuitry in the retina to get to the retina &apos;s output cells , we can jump over the damaged circuitry in the cochlea to get the auditory nerve , or jump over damaged areas in the cortex , in the motor cortex , to bridge the gap produced by a stroke .\n",
      "-------------\n",
      "I just wanted to finish with a simple message that understanding that code is really important , and if we can understand the code , the language of the brain , what the brain is in front of it seems to be possible . Thank you .\n",
      "\n",
      "I just want to end with a simple message that understanding the code is really , really important , and if we can understand the code , the language of the brain , things become possible that didn &apos;t seem obviously possible before . Thank you .\n",
      "-------------\n",
      "Jonathan Harris : tầm stories\n",
      "\n",
      "Jonathan Harris : The web as art\n",
      "-------------\n",
      "At the 12th EG EG EG Conference , artist Kim bàn his latest stories , including the chuyện stories , and from the Internet , and from the Internet , in which there is a great project . &quot;\n",
      "\n",
      "At the EG conference in December 2007 , artist Jonathan Harris discusses his latest projects , which involve collecting stories : his own , strangers &apos; , and stories collected from the Internet , including his amazing &quot; We Feel Fine . &quot;\n",
      "-------------\n",
      "Today I &apos;m going to talk about plugging into some different ways .\n",
      "\n",
      "So I &apos;m going to talk today about collecting stories in some unconventional ways .\n",
      "-------------\n",
      "This is a photograph of me -- a very awkward moment about .\n",
      "\n",
      "This is a picture of me from a very awkward stage in my life .\n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "evaluate_human('22846dcbc32b2f51a395010cde5d7f85')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-16 14:53:42,839 INFO]  * src vocab size = 8870\n",
      "[2019-02-16 14:53:42,840 INFO]  * tgt vocab size = 20071\n",
      "[2019-02-16 14:53:42,842 INFO] Building model...\n",
      "[2019-02-16 14:53:47,064 INFO] NMTModel(\n",
      "  (encoder): RNNEncoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(8870, 500, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (rnn): LSTM(500, 250, num_layers=2, dropout=0.3, bidirectional=True)\n",
      "  )\n",
      "  (decoder): InputFeedRNNDecoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(20071, 500, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.3)\n",
      "    (rnn): StackedLSTM(\n",
      "      (dropout): Dropout(p=0.3)\n",
      "      (layers): ModuleList(\n",
      "        (0): LSTMCell(1000, 500)\n",
      "        (1): LSTMCell(500, 500)\n",
      "      )\n",
      "    )\n",
      "    (attn): GlobalAttention(\n",
      "      (linear_in): Linear(in_features=500, out_features=500, bias=False)\n",
      "      (linear_out): Linear(in_features=1000, out_features=500, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (generator): Sequential(\n",
      "    (0): Linear(in_features=500, out_features=20071, bias=True)\n",
      "    (1): Cast()\n",
      "    (2): LogSoftmax()\n",
      "  )\n",
      ")\n",
      "[2019-02-16 14:53:47,066 INFO] encoder: 7443000\n",
      "[2019-02-16 14:53:47,067 INFO] decoder: 25849071\n",
      "[2019-02-16 14:53:47,069 INFO] * number of parameters: 33292071\n",
      "[2019-02-16 14:53:47,212 INFO] Starting training on GPU: [0]\n",
      "[2019-02-16 14:53:47,213 INFO] Start training loop and validate every 500 steps...\n",
      "[2019-02-16 14:53:48,639 INFO] Loading dataset from data/nmt15-reverse.train.0.pt, number of examples: 95844\n",
      "[2019-02-16 14:54:05,436 INFO] Step 100/20000; acc:   9.08; ppl: 719.49; xent: 6.58; lr: 0.00100; 17567/14945 tok/s;     18 sec\n",
      "[2019-02-16 14:54:21,371 INFO] Step 200/20000; acc:  20.21; ppl: 235.21; xent: 5.46; lr: 0.00100; 19910/16931 tok/s;     34 sec\n",
      "[2019-02-16 14:54:37,824 INFO] Step 300/20000; acc:  27.78; ppl: 127.33; xent: 4.85; lr: 0.00100; 19098/16285 tok/s;     51 sec\n",
      "[2019-02-16 14:54:54,117 INFO] Step 400/20000; acc:  31.56; ppl: 90.52; xent: 4.51; lr: 0.00100; 19439/16586 tok/s;     67 sec\n",
      "[2019-02-16 14:55:10,188 INFO] Step 500/20000; acc:  35.50; ppl: 65.37; xent: 4.18; lr: 0.00100; 19551/16670 tok/s;     83 sec\n",
      "[2019-02-16 14:55:10,291 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n",
      "[2019-02-16 14:55:26,389 INFO] Validation perplexity: 49.2713\n",
      "[2019-02-16 14:55:26,390 INFO] Validation accuracy: 38.26\n",
      "[2019-02-16 14:55:42,895 INFO] Step 600/20000; acc:  37.25; ppl: 54.95; xent: 4.01; lr: 0.00100; 9804/8331 tok/s;    116 sec\n",
      "[2019-02-16 14:55:58,982 INFO] Step 700/20000; acc:  39.50; ppl: 44.73; xent: 3.80; lr: 0.00100; 19658/16745 tok/s;    132 sec\n",
      "[2019-02-16 14:56:08,961 INFO] Loading dataset from data/nmt15-reverse.train.0.pt, number of examples: 95844\n",
      "[2019-02-16 14:56:19,270 INFO] Step 800/20000; acc:  40.51; ppl: 39.81; xent: 3.68; lr: 0.00100; 16949/14302 tok/s;    152 sec\n",
      "[2019-02-16 14:56:33,927 INFO] Step 900/20000; acc:  43.26; ppl: 31.51; xent: 3.45; lr: 0.00100; 19363/16663 tok/s;    167 sec\n",
      "[2019-02-16 14:56:50,900 INFO] Step 1000/20000; acc:  43.53; ppl: 29.81; xent: 3.39; lr: 0.00100; 19183/16293 tok/s;    184 sec\n",
      "[2019-02-16 14:56:50,996 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n",
      "[2019-02-16 14:57:07,626 INFO] Validation perplexity: 24.0268\n",
      "[2019-02-16 14:57:07,627 INFO] Validation accuracy: 46.092\n",
      "[2019-02-16 14:57:23,919 INFO] Step 1100/20000; acc:  44.84; ppl: 25.94; xent: 3.26; lr: 0.00100; 9698/8301 tok/s;    217 sec\n",
      "[2019-02-16 14:57:40,193 INFO] Step 1200/20000; acc:  45.52; ppl: 24.44; xent: 3.20; lr: 0.00100; 19391/16401 tok/s;    233 sec\n",
      "[2019-02-16 14:57:56,027 INFO] Step 1300/20000; acc:  47.01; ppl: 20.98; xent: 3.04; lr: 0.00100; 20109/17286 tok/s;    249 sec\n",
      "[2019-02-16 14:58:12,332 INFO] Step 1400/20000; acc:  47.27; ppl: 20.40; xent: 3.02; lr: 0.00100; 19557/16557 tok/s;    265 sec\n",
      "[2019-02-16 14:58:30,088 INFO] Loading dataset from data/nmt15-reverse.train.0.pt, number of examples: 95844\n",
      "[2019-02-16 14:58:30,664 INFO] Step 1500/20000; acc:  48.06; ppl: 19.01; xent: 2.94; lr: 0.00100; 16553/14140 tok/s;    283 sec\n",
      "[2019-02-16 14:58:30,761 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n",
      "[2019-02-16 14:58:48,729 INFO] Validation perplexity: 18.0557\n",
      "[2019-02-16 14:58:48,730 INFO] Validation accuracy: 48.8901\n",
      "[2019-02-16 14:59:05,458 INFO] Step 1600/20000; acc:  48.63; ppl: 17.72; xent: 2.87; lr: 0.00100; 9104/7762 tok/s;    318 sec\n",
      "[2019-02-16 14:59:22,500 INFO] Step 1700/20000; acc:  48.96; ppl: 17.13; xent: 2.84; lr: 0.00100; 18864/16037 tok/s;    335 sec\n",
      "[2019-02-16 14:59:39,005 INFO] Step 1800/20000; acc:  49.87; ppl: 15.80; xent: 2.76; lr: 0.00100; 19008/16155 tok/s;    352 sec\n",
      "[2019-02-16 14:59:55,424 INFO] Step 1900/20000; acc:  50.00; ppl: 15.39; xent: 2.73; lr: 0.00100; 19435/16619 tok/s;    368 sec\n",
      "[2019-02-16 15:00:11,452 INFO] Step 2000/20000; acc:  51.37; ppl: 13.92; xent: 2.63; lr: 0.00100; 19358/16498 tok/s;    384 sec\n",
      "[2019-02-16 15:00:11,570 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n",
      "[2019-02-16 15:00:29,062 INFO] Validation perplexity: 15.0575\n",
      "[2019-02-16 15:00:29,063 INFO] Validation accuracy: 50.9837\n",
      "[2019-02-16 15:00:45,491 INFO] Step 2100/20000; acc:  50.96; ppl: 14.07; xent: 2.64; lr: 0.00100; 9357/7961 tok/s;    418 sec\n",
      "[2019-02-16 15:01:01,880 INFO] Step 2200/20000; acc:  51.44; ppl: 13.51; xent: 2.60; lr: 0.00100; 19469/16579 tok/s;    435 sec\n",
      "[2019-02-16 15:01:11,584 INFO] Loading dataset from data/nmt15-reverse.train.0.pt, number of examples: 95844\n",
      "[2019-02-16 15:01:22,115 INFO] Step 2300/20000; acc:  51.56; ppl: 13.31; xent: 2.59; lr: 0.00100; 16862/14235 tok/s;    455 sec\n",
      "[2019-02-16 15:01:36,836 INFO] Step 2400/20000; acc:  52.93; ppl: 11.92; xent: 2.48; lr: 0.00100; 19289/16576 tok/s;    470 sec\n",
      "[2019-02-16 15:01:53,880 INFO] Step 2500/20000; acc:  52.38; ppl: 12.30; xent: 2.51; lr: 0.00100; 19225/16342 tok/s;    487 sec\n",
      "[2019-02-16 15:01:53,980 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n",
      "[2019-02-16 15:02:10,545 INFO] Validation perplexity: 13.7059\n",
      "[2019-02-16 15:02:10,546 INFO] Validation accuracy: 51.9091\n",
      "[2019-02-16 15:02:26,919 INFO] Step 2600/20000; acc:  52.78; ppl: 11.74; xent: 2.46; lr: 0.00100; 9772/8355 tok/s;    520 sec\n",
      "[2019-02-16 15:02:43,058 INFO] Step 2700/20000; acc:  53.13; ppl: 11.51; xent: 2.44; lr: 0.00100; 19279/16316 tok/s;    536 sec\n",
      "[2019-02-16 15:02:58,947 INFO] Step 2800/20000; acc:  54.27; ppl: 10.55; xent: 2.36; lr: 0.00100; 20117/17274 tok/s;    552 sec\n",
      "[2019-02-16 15:03:16,154 INFO] Step 2900/20000; acc:  53.68; ppl: 11.10; xent: 2.41; lr: 0.00100; 19082/16105 tok/s;    569 sec\n",
      "[2019-02-16 15:03:32,949 INFO] Loading dataset from data/nmt15-reverse.train.0.pt, number of examples: 95844\n",
      "[2019-02-16 15:03:33,965 INFO] Step 3000/20000; acc:  54.88; ppl: 10.00; xent: 2.30; lr: 0.00100; 16724/14322 tok/s;    587 sec\n",
      "[2019-02-16 15:03:34,067 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n",
      "[2019-02-16 15:03:50,724 INFO] Validation perplexity: 13.026\n",
      "[2019-02-16 15:03:50,725 INFO] Validation accuracy: 52.8079\n",
      "[2019-02-16 15:04:07,092 INFO] Step 3100/20000; acc:  54.72; ppl: 10.07; xent: 2.31; lr: 0.00100; 9426/8070 tok/s;    620 sec\n",
      "[2019-02-16 15:04:24,425 INFO] Step 3200/20000; acc:  54.03; ppl: 10.70; xent: 2.37; lr: 0.00100; 19138/16160 tok/s;    637 sec\n",
      "[2019-02-16 15:04:39,959 INFO] Step 3300/20000; acc:  55.50; ppl:  9.46; xent: 2.25; lr: 0.00100; 19833/16931 tok/s;    653 sec\n",
      "[2019-02-16 15:04:55,997 INFO] Step 3400/20000; acc:  55.21; ppl:  9.63; xent: 2.26; lr: 0.00100; 19437/16666 tok/s;    669 sec\n",
      "[2019-02-16 15:05:12,163 INFO] Step 3500/20000; acc:  56.04; ppl:  9.14; xent: 2.21; lr: 0.00100; 19429/16555 tok/s;    685 sec\n",
      "[2019-02-16 15:05:12,271 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n",
      "[2019-02-16 15:05:29,115 INFO] Validation perplexity: 12.4293\n",
      "[2019-02-16 15:05:29,116 INFO] Validation accuracy: 53.2857\n",
      "[2019-02-16 15:05:45,463 INFO] Step 3600/20000; acc:  55.83; ppl:  9.16; xent: 2.21; lr: 0.00100; 9506/8079 tok/s;    718 sec\n",
      "[2019-02-16 15:06:01,997 INFO] Step 3700/20000; acc:  55.76; ppl:  9.22; xent: 2.22; lr: 0.00100; 19378/16495 tok/s;    735 sec\n",
      "[2019-02-16 15:06:11,464 INFO] Loading dataset from data/nmt15-reverse.train.0.pt, number of examples: 95844\n",
      "[2019-02-16 15:06:22,183 INFO] Step 3800/20000; acc:  55.86; ppl:  9.20; xent: 2.22; lr: 0.00100; 16777/14165 tok/s;    755 sec\n",
      "[2019-02-16 15:06:37,025 INFO] Step 3900/20000; acc:  57.21; ppl:  8.29; xent: 2.12; lr: 0.00100; 19334/16619 tok/s;    770 sec\n",
      "[2019-02-16 15:06:54,044 INFO] Step 4000/20000; acc:  56.34; ppl:  8.83; xent: 2.18; lr: 0.00100; 19211/16309 tok/s;    787 sec\n",
      "[2019-02-16 15:06:54,142 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n",
      "[2019-02-16 15:07:10,834 INFO] Saving checkpoint /mnt/drive-2t/model/opennmt_2a82e28e8fdf56498eba4ee34eee9ac0_step_4000.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meet early stop condition, prev best loss: 53.285713015726266 current loss: 53.31460524327256\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'2a82e28e8fdf56498eba4ee34eee9ac0'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_args = {\n",
    "    'model_dtype': 'fp32',\n",
    "    'encoder_type': 'brnn',\n",
    "    'decoder_type': 'rnn',\n",
    "#     'rnn_type': 'GRU',\n",
    "#     'layers': 3,\n",
    "#     'max_generator_batches': 2,\n",
    "    'dropout': 0.3,\n",
    "#     'enc_rnn_size': 512,\n",
    "#     'dec_rnn_size': 512,\n",
    "#     'src_word_vec_size': 512,\n",
    "#     'tgt_word_vec_size': 512\n",
    "}\n",
    "train_args = {\n",
    "    'batch_size': 128,\n",
    "    'train_steps': 20000,\n",
    "    'optim': 'adam',\n",
    "    'learning_rate': 0.001,\n",
    "    'valid_step': 500,\n",
    "    'valid_batch': 64,\n",
    "    'early_stop_round': 5,\n",
    "    'early_stop_threshold': 0.1,\n",
    "    'report_every': 100\n",
    "}\n",
    "\n",
    "total_args = {}\n",
    "total_args.update(model_args)\n",
    "total_args.update(train_args)\n",
    "\n",
    "reverse_runner.run(total_args)\n",
    "reverse_runner.model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-16 15:11:53,568 INFO] Translating shard 0.\n",
      "[2019-02-16 15:13:34,420 INFO] Translating shard 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRED AVG SCORE: -0.7467, PRED PPL: 2.1100\n",
      "GOLD AVG SCORE: -10.3315, GOLD PPL: 30683.9296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-16 15:15:21,707 INFO] Translating shard 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRED AVG SCORE: -0.7608, PRED PPL: 2.1400\n",
      "GOLD AVG SCORE: -10.3291, GOLD PPL: 30610.4227\n",
      "PRED AVG SCORE: -0.7843, PRED PPL: 2.1910\n",
      "GOLD AVG SCORE: -10.2622, GOLD PPL: 28629.7671\n"
     ]
    }
   ],
   "source": [
    "model_id = '2a82e28e8fdf56498eba4ee34eee9ac0'\n",
    "args = build_translate_args_reverse(model_id)\n",
    "args['batch_size'] = 32\n",
    "reverse_runner.translate(args, model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"data\": \"data/nmt15-reverse\",\n",
      "    \"save_model\": \"/mnt/drive-2t/model/opennmt_2a82e28e8fdf56498eba4ee34eee9ac0\",\n",
      "    \"gpu_ranks\": 0,\n",
      "    \"tensorboard\": null,\n",
      "    \"tensorboard_log_dir\": \"/mnt/drive-2t/log/opennmt_2a82e28e8fdf56498eba4ee34eee9ac0/log\",\n",
      "    \"model_dtype\": \"fp32\",\n",
      "    \"encoder_type\": \"brnn\",\n",
      "    \"decoder_type\": \"rnn\",\n",
      "    \"dropout\": 0.3,\n",
      "    \"batch_size\": 128,\n",
      "    \"train_steps\": 20000,\n",
      "    \"optim\": \"adam\",\n",
      "    \"learning_rate\": 0.001,\n",
      "    \"valid_step\": 500,\n",
      "    \"valid_batch\": 64,\n",
      "    \"early_stop_round\": 5,\n",
      "    \"early_stop_threshold\": 0.1,\n",
      "    \"report_every\": 100\n",
      "}\n",
      "38.0 22.36\n",
      "Now , it &apos;s time to end , and as I mentioned before , of course , I have a lot of different data if you &apos;re interested , but I just had a lot of different data if you care , but I just wanted to show this basic idea about communication communication with the brain with its language , and the potential of the potential of it .\n",
      "\n",
      "Now I just want to wrap up , and as I was mentioning earlier of course I have a lot of other data if you &apos;re interested , but I just wanted to give this sort of basic idea of being able to communicate with the brain in its language , and the potential power of being able to do that .\n",
      "-------------\n",
      "It &apos;s very different with a mechanical device with which you have to northern communications from the brain to a device , and here we have to northern communications from the brain outside into the brain inside the brain , and understand , the brain understands .\n",
      "\n",
      "So it &apos;s different from the motor prosthetics where you &apos;re communicating from the brain to a device . Here we have to communicate from the outside world into the brain and be understood , and be understood by the brain .\n",
      "-------------\n",
      "And then the last thing I would like to say , well , I want to highlight that this idea is biodegradable .\n",
      "\n",
      "And then the last thing I wanted to say , really , is to emphasize that the idea generalizes .\n",
      "-------------\n",
      "The strategy that we use to find the code for the retina , we can also use to find code for other areas , for example , like the auditory and the body system , to treat hearing and disorder .\n",
      "\n",
      "So the same strategy that we used to find the code for the retina we can also use to find the code for other areas , for example , the auditory system and the motor system , so for treating deafness and for motor disorders .\n",
      "-------------\n",
      "It &apos;s by how we remove the mạch of the retina to come into the first cells in the retina to come to the first cells of the retina , and we can ignore the circuits that are buried in the cochlea to reach the hearing , or remove the hư of the cortex in the cerebral cortex , in the brain cortex , to trám in the cracks of the cerebral cortex , in the brain cortex , to trám in the cracks of the cerebral cortex , in the brain cortex , to trám the stroke of the\n",
      "\n",
      "So just the same way that we were able to jump over the damaged circuitry in the retina to get to the retina &apos;s output cells , we can jump over the damaged circuitry in the cochlea to get the auditory nerve , or jump over damaged areas in the cortex , in the motor cortex , to bridge the gap produced by a stroke .\n",
      "-------------\n",
      "I just want to end with a simple message that understand the real code , which is really important , and if we can understand the code , the language of the brain , which is that we don &apos;t seem to be able to be possible . Thank you .\n",
      "\n",
      "I just want to end with a simple message that understanding the code is really , really important , and if we can understand the code , the language of the brain , things become possible that didn &apos;t seem obviously possible before . Thank you .\n",
      "-------------\n",
      "Jonathan Harris : Mapping stories of stories\n",
      "\n",
      "Jonathan Harris : The web as art\n",
      "-------------\n",
      "At the early EG Conference of December 2007 , artists Jonathan Jonathan Harris talks about the most recent projects of you , including gathering stories : stories of self , of strangers , and the Internet , in which there &apos;s a wonderful project , &quot; We &apos;re okay . &quot;\n",
      "\n",
      "At the EG conference in December 2007 , artist Jonathan Harris discusses his latest projects , which involve collecting stories : his own , strangers &apos; , and stories collected from the Internet , including his amazing &quot; We Feel Fine . &quot;\n",
      "-------------\n",
      "Today I &apos;m going to talk about the collection of a few different ways .\n",
      "\n",
      "So I &apos;m going to talk today about collecting stories in some unconventional ways .\n",
      "-------------\n",
      "This is a photograph of me -- a lot of awkward .\n",
      "\n",
      "This is a picture of me from a very awkward stage in my life .\n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "evaluate_human('2a82e28e8fdf56498eba4ee34eee9ac0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-16 15:25:23,776 INFO]  * src vocab size = 8870\n",
      "[2019-02-16 15:25:23,779 INFO]  * tgt vocab size = 20071\n",
      "[2019-02-16 15:25:23,779 INFO] Building model...\n",
      "[2019-02-16 15:25:27,407 INFO] NMTModel(\n",
      "  (encoder): RNNEncoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(8870, 512, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (rnn): LSTM(512, 256, num_layers=3, dropout=0.3, bidirectional=True)\n",
      "  )\n",
      "  (decoder): InputFeedRNNDecoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(20071, 512, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.3)\n",
      "    (rnn): StackedLSTM(\n",
      "      (dropout): Dropout(p=0.3)\n",
      "      (layers): ModuleList(\n",
      "        (0): LSTMCell(1024, 512)\n",
      "        (1): LSTMCell(512, 512)\n",
      "        (2): LSTMCell(512, 512)\n",
      "      )\n",
      "    )\n",
      "    (attn): GlobalAttention(\n",
      "      (linear_in): Linear(in_features=512, out_features=512, bias=False)\n",
      "      (linear_out): Linear(in_features=1024, out_features=512, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (generator): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=20071, bias=True)\n",
      "    (1): Cast()\n",
      "    (2): LogSoftmax()\n",
      "  )\n",
      ")\n",
      "[2019-02-16 15:25:27,408 INFO] encoder: 9272320\n",
      "[2019-02-16 15:25:27,409 INFO] decoder: 28711527\n",
      "[2019-02-16 15:25:27,409 INFO] * number of parameters: 37983847\n",
      "[2019-02-16 15:25:27,538 INFO] Starting training on GPU: [0]\n",
      "[2019-02-16 15:25:27,539 INFO] Start training loop and validate every 500 steps...\n",
      "[2019-02-16 15:25:28,963 INFO] Loading dataset from data/nmt15-reverse.train.0.pt, number of examples: 95844\n",
      "[2019-02-16 15:25:48,147 INFO] Step 100/20000; acc:   8.19; ppl: 682.57; xent: 6.53; lr: 0.00100; 15369/13100 tok/s;     21 sec\n",
      "[2019-02-16 15:26:08,308 INFO] Step 200/20000; acc:  16.97; ppl: 287.01; xent: 5.66; lr: 0.00100; 15690/13371 tok/s;     41 sec\n",
      "[2019-02-16 15:26:27,529 INFO] Step 300/20000; acc:  21.91; ppl: 188.13; xent: 5.24; lr: 0.00100; 16477/14039 tok/s;     60 sec\n",
      "[2019-02-16 15:26:46,115 INFO] Step 400/20000; acc:  25.32; ppl: 138.24; xent: 4.93; lr: 0.00100; 16988/14446 tok/s;     79 sec\n",
      "[2019-02-16 15:27:05,244 INFO] Step 500/20000; acc:  28.74; ppl: 103.57; xent: 4.64; lr: 0.00100; 16619/14133 tok/s;     98 sec\n",
      "[2019-02-16 15:27:05,347 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n",
      "[2019-02-16 15:27:22,997 INFO] Validation perplexity: 77.8619\n",
      "[2019-02-16 15:27:22,998 INFO] Validation accuracy: 31.3592\n",
      "[2019-02-16 15:27:41,775 INFO] Step 600/20000; acc:  31.17; ppl: 82.48; xent: 4.41; lr: 0.00100; 8625/7340 tok/s;    134 sec\n",
      "[2019-02-16 15:28:00,210 INFO] Step 700/20000; acc:  33.99; ppl: 66.32; xent: 4.19; lr: 0.00100; 17154/14674 tok/s;    153 sec\n",
      "[2019-02-16 15:28:11,674 INFO] Loading dataset from data/nmt15-reverse.train.0.pt, number of examples: 95844\n",
      "[2019-02-16 15:28:20,847 INFO] Step 800/20000; acc:  36.22; ppl: 54.67; xent: 4.00; lr: 0.00100; 15054/12870 tok/s;    173 sec\n",
      "[2019-02-16 15:28:40,510 INFO] Step 900/20000; acc:  37.27; ppl: 49.30; xent: 3.90; lr: 0.00100; 16776/14297 tok/s;    193 sec\n",
      "[2019-02-16 15:28:59,552 INFO] Step 1000/20000; acc:  38.80; ppl: 42.74; xent: 3.76; lr: 0.00100; 16060/13560 tok/s;    212 sec\n",
      "[2019-02-16 15:28:59,649 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n",
      "[2019-02-16 15:29:17,926 INFO] Validation perplexity: 33.5001\n",
      "[2019-02-16 15:29:17,927 INFO] Validation accuracy: 41.4808\n",
      "[2019-02-16 15:29:37,622 INFO] Step 1100/20000; acc:  40.20; ppl: 37.11; xent: 3.61; lr: 0.00100; 8495/7264 tok/s;    250 sec\n",
      "[2019-02-16 15:29:56,925 INFO] Step 1200/20000; acc:  42.09; ppl: 31.94; xent: 3.46; lr: 0.00100; 15929/13517 tok/s;    269 sec\n",
      "[2019-02-16 15:30:19,161 INFO] Step 1300/20000; acc:  42.33; ppl: 30.60; xent: 3.42; lr: 0.00100; 14167/12118 tok/s;    292 sec\n",
      "[2019-02-16 15:30:39,299 INFO] Step 1400/20000; acc:  43.14; ppl: 27.88; xent: 3.33; lr: 0.00100; 16514/13976 tok/s;    312 sec\n",
      "[2019-02-16 15:31:00,541 INFO] Loading dataset from data/nmt15-reverse.train.0.pt, number of examples: 95844\n",
      "[2019-02-16 15:31:01,349 INFO] Step 1500/20000; acc:  44.52; ppl: 25.32; xent: 3.23; lr: 0.00100; 14240/12151 tok/s;    334 sec\n",
      "[2019-02-16 15:31:01,445 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n",
      "[2019-02-16 15:31:19,783 INFO] Validation perplexity: 21.8454\n",
      "[2019-02-16 15:31:19,784 INFO] Validation accuracy: 46.837\n",
      "[2019-02-16 15:31:39,101 INFO] Step 1600/20000; acc:  45.31; ppl: 23.47; xent: 3.16; lr: 0.00100; 8395/7168 tok/s;    372 sec\n",
      "[2019-02-16 15:31:57,455 INFO] Step 1700/20000; acc:  45.96; ppl: 21.83; xent: 3.08; lr: 0.00100; 16834/14364 tok/s;    390 sec\n",
      "[2019-02-16 15:32:16,623 INFO] Step 1800/20000; acc:  46.37; ppl: 20.82; xent: 3.04; lr: 0.00100; 16714/14230 tok/s;    409 sec\n",
      "[2019-02-16 15:32:35,372 INFO] Step 1900/20000; acc:  47.24; ppl: 19.23; xent: 2.96; lr: 0.00100; 16951/14396 tok/s;    428 sec\n",
      "[2019-02-16 15:32:54,197 INFO] Step 2000/20000; acc:  47.94; ppl: 18.10; xent: 2.90; lr: 0.00100; 16602/14169 tok/s;    447 sec\n",
      "[2019-02-16 15:32:54,298 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n",
      "[2019-02-16 15:33:12,699 INFO] Validation perplexity: 18.378\n",
      "[2019-02-16 15:33:12,700 INFO] Validation accuracy: 48.675\n",
      "[2019-02-16 15:33:31,694 INFO] Step 2100/20000; acc:  48.28; ppl: 17.47; xent: 2.86; lr: 0.00100; 8465/7189 tok/s;    484 sec\n",
      "[2019-02-16 15:33:50,095 INFO] Step 2200/20000; acc:  49.07; ppl: 16.31; xent: 2.79; lr: 0.00100; 17093/14635 tok/s;    503 sec\n",
      "[2019-02-16 15:34:01,356 INFO] Loading dataset from data/nmt15-reverse.train.0.pt, number of examples: 95844\n",
      "[2019-02-16 15:34:11,039 INFO] Step 2300/20000; acc:  49.48; ppl: 15.64; xent: 2.75; lr: 0.00100; 15035/12844 tok/s;    523 sec\n",
      "[2019-02-16 15:34:31,785 INFO] Step 2400/20000; acc:  49.26; ppl: 15.79; xent: 2.76; lr: 0.00100; 15750/13412 tok/s;    544 sec\n",
      "[2019-02-16 15:34:52,184 INFO] Step 2500/20000; acc:  50.09; ppl: 14.88; xent: 2.70; lr: 0.00100; 14980/12662 tok/s;    565 sec\n",
      "[2019-02-16 15:34:52,290 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n",
      "[2019-02-16 15:35:14,177 INFO] Validation perplexity: 15.2559\n",
      "[2019-02-16 15:35:14,178 INFO] Validation accuracy: 50.7041\n",
      "[2019-02-16 15:35:34,478 INFO] Step 2600/20000; acc:  50.54; ppl: 14.20; xent: 2.65; lr: 0.00100; 7613/6517 tok/s;    607 sec\n",
      "[2019-02-16 15:35:53,071 INFO] Step 2700/20000; acc:  51.58; ppl: 13.06; xent: 2.57; lr: 0.00100; 16706/14155 tok/s;    626 sec\n",
      "[2019-02-16 15:36:13,220 INFO] Step 2800/20000; acc:  51.25; ppl: 13.33; xent: 2.59; lr: 0.00100; 15518/13278 tok/s;    646 sec\n",
      "[2019-02-16 15:36:33,307 INFO] Step 2900/20000; acc:  51.43; ppl: 12.96; xent: 2.56; lr: 0.00100; 16595/14036 tok/s;    666 sec\n",
      "[2019-02-16 15:36:54,261 INFO] Loading dataset from data/nmt15-reverse.train.0.pt, number of examples: 95844\n",
      "[2019-02-16 15:36:55,452 INFO] Step 3000/20000; acc:  52.06; ppl: 12.46; xent: 2.52; lr: 0.00100; 14136/12073 tok/s;    688 sec\n",
      "[2019-02-16 15:36:55,553 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n",
      "[2019-02-16 15:37:15,376 INFO] Validation perplexity: 14.0114\n",
      "[2019-02-16 15:37:15,377 INFO] Validation accuracy: 51.994\n",
      "[2019-02-16 15:37:35,691 INFO] Step 3100/20000; acc:  52.28; ppl: 12.18; xent: 2.50; lr: 0.00100; 7918/6768 tok/s;    728 sec\n",
      "[2019-02-16 15:37:55,046 INFO] Step 3200/20000; acc:  52.76; ppl: 11.78; xent: 2.47; lr: 0.00100; 16005/13650 tok/s;    748 sec\n",
      "[2019-02-16 15:38:15,105 INFO] Step 3300/20000; acc:  52.88; ppl: 11.54; xent: 2.45; lr: 0.00100; 15897/13533 tok/s;    768 sec\n",
      "[2019-02-16 15:38:34,711 INFO] Step 3400/20000; acc:  53.42; ppl: 11.13; xent: 2.41; lr: 0.00100; 16204/13746 tok/s;    787 sec\n",
      "[2019-02-16 15:38:54,335 INFO] Step 3500/20000; acc:  53.80; ppl: 10.77; xent: 2.38; lr: 0.00100; 15753/13455 tok/s;    807 sec\n",
      "[2019-02-16 15:38:54,439 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n",
      "[2019-02-16 15:39:14,378 INFO] Validation perplexity: 13.3542\n",
      "[2019-02-16 15:39:14,379 INFO] Validation accuracy: 52.6052\n",
      "[2019-02-16 15:39:34,522 INFO] Step 3600/20000; acc:  53.76; ppl: 10.78; xent: 2.38; lr: 0.00100; 8008/6802 tok/s;    847 sec\n",
      "[2019-02-16 15:39:54,591 INFO] Step 3700/20000; acc:  53.90; ppl: 10.60; xent: 2.36; lr: 0.00100; 16066/13699 tok/s;    867 sec\n",
      "[2019-02-16 15:40:05,199 INFO] Loading dataset from data/nmt15-reverse.train.0.pt, number of examples: 95844\n",
      "[2019-02-16 15:40:15,607 INFO] Step 3800/20000; acc:  55.11; ppl:  9.70; xent: 2.27; lr: 0.00100; 14450/12391 tok/s;    888 sec\n",
      "[2019-02-16 15:40:36,184 INFO] Step 3900/20000; acc:  54.16; ppl: 10.39; xent: 2.34; lr: 0.00100; 15915/13554 tok/s;    909 sec\n",
      "[2019-02-16 15:40:56,039 INFO] Step 4000/20000; acc:  54.56; ppl:  9.94; xent: 2.30; lr: 0.00100; 15403/13058 tok/s;    928 sec\n",
      "[2019-02-16 15:40:56,142 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n",
      "[2019-02-16 15:41:15,774 INFO] Validation perplexity: 12.9542\n",
      "[2019-02-16 15:41:15,775 INFO] Validation accuracy: 52.8603\n",
      "[2019-02-16 15:41:36,120 INFO] Step 4100/20000; acc:  55.05; ppl:  9.70; xent: 2.27; lr: 0.00100; 8091/6904 tok/s;    969 sec\n",
      "[2019-02-16 15:41:54,673 INFO] Step 4200/20000; acc:  55.93; ppl:  9.11; xent: 2.21; lr: 0.00100; 16562/14056 tok/s;    987 sec\n",
      "[2019-02-16 15:42:15,102 INFO] Step 4300/20000; acc:  55.43; ppl:  9.50; xent: 2.25; lr: 0.00100; 15392/13155 tok/s;   1008 sec\n",
      "[2019-02-16 15:42:35,126 INFO] Step 4400/20000; acc:  55.57; ppl:  9.23; xent: 2.22; lr: 0.00100; 16487/13952 tok/s;   1028 sec\n",
      "[2019-02-16 15:42:55,803 INFO] Loading dataset from data/nmt15-reverse.train.0.pt, number of examples: 95844\n",
      "[2019-02-16 15:42:57,245 INFO] Step 4500/20000; acc:  55.82; ppl:  9.05; xent: 2.20; lr: 0.00100; 14201/12148 tok/s;   1050 sec\n",
      "[2019-02-16 15:42:57,344 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n",
      "[2019-02-16 15:43:17,143 INFO] Validation perplexity: 12.2962\n",
      "[2019-02-16 15:43:17,144 INFO] Validation accuracy: 53.7111\n",
      "[2019-02-16 15:43:37,774 INFO] Step 4600/20000; acc:  55.85; ppl:  9.10; xent: 2.21; lr: 0.00100; 8070/6859 tok/s;   1090 sec\n",
      "[2019-02-16 15:43:56,671 INFO] Step 4700/20000; acc:  56.66; ppl:  8.49; xent: 2.14; lr: 0.00100; 15927/13644 tok/s;   1109 sec\n",
      "[2019-02-16 15:44:16,688 INFO] Step 4800/20000; acc:  56.45; ppl:  8.71; xent: 2.16; lr: 0.00100; 15998/13620 tok/s;   1129 sec\n",
      "[2019-02-16 15:44:36,738 INFO] Step 4900/20000; acc:  56.65; ppl:  8.53; xent: 2.14; lr: 0.00100; 16116/13641 tok/s;   1149 sec\n",
      "[2019-02-16 15:44:55,942 INFO] Step 5000/20000; acc:  57.27; ppl:  8.17; xent: 2.10; lr: 0.00100; 15814/13539 tok/s;   1168 sec\n",
      "[2019-02-16 15:44:56,049 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n",
      "[2019-02-16 15:45:15,931 INFO] Validation perplexity: 12.1603\n",
      "[2019-02-16 15:45:15,932 INFO] Validation accuracy: 53.9658\n",
      "[2019-02-16 15:45:15,933 INFO] Saving checkpoint /mnt/drive-2t/model/opennmt_6082f2c966d9ea8dbb7afb565d719193_step_5000.pt\n",
      "[2019-02-16 15:45:38,810 INFO] Step 5100/20000; acc:  57.03; ppl:  8.35; xent: 2.12; lr: 0.00100; 7515/6375 tok/s;   1211 sec\n",
      "[2019-02-16 15:45:58,938 INFO] Step 5200/20000; acc:  56.95; ppl:  8.32; xent: 2.12; lr: 0.00100; 16139/13758 tok/s;   1231 sec\n",
      "[2019-02-16 15:46:09,129 INFO] Loading dataset from data/nmt15-reverse.train.0.pt, number of examples: 95844\n",
      "[2019-02-16 15:46:19,735 INFO] Step 5300/20000; acc:  58.39; ppl:  7.50; xent: 2.02; lr: 0.00100; 14471/12419 tok/s;   1252 sec\n",
      "[2019-02-16 15:46:40,510 INFO] Step 5400/20000; acc:  57.10; ppl:  8.25; xent: 2.11; lr: 0.00100; 15945/13541 tok/s;   1273 sec\n",
      "[2019-02-16 15:47:00,510 INFO] Step 5500/20000; acc:  57.82; ppl:  7.86; xent: 2.06; lr: 0.00100; 15302/13019 tok/s;   1293 sec\n",
      "[2019-02-16 15:47:00,630 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n",
      "[2019-02-16 15:47:20,428 INFO] Validation perplexity: 12.0925\n",
      "[2019-02-16 15:47:20,428 INFO] Validation accuracy: 54.0489\n",
      "[2019-02-16 15:47:40,670 INFO] Step 5600/20000; acc:  57.78; ppl:  7.85; xent: 2.06; lr: 0.00100; 8087/6891 tok/s;   1333 sec\n",
      "[2019-02-16 15:47:58,865 INFO] Step 5700/20000; acc:  58.92; ppl:  7.26; xent: 1.98; lr: 0.00100; 16571/14073 tok/s;   1351 sec\n",
      "[2019-02-16 15:48:19,525 INFO] Step 5800/20000; acc:  57.94; ppl:  7.82; xent: 2.06; lr: 0.00100; 15613/13314 tok/s;   1372 sec\n",
      "[2019-02-16 15:48:39,106 INFO] Step 5900/20000; acc:  58.52; ppl:  7.41; xent: 2.00; lr: 0.00100; 16500/14006 tok/s;   1392 sec\n",
      "[2019-02-16 15:48:59,587 INFO] Loading dataset from data/nmt15-reverse.train.0.pt, number of examples: 95844\n",
      "[2019-02-16 15:49:01,745 INFO] Step 6000/20000; acc:  58.41; ppl:  7.50; xent: 2.01; lr: 0.00100; 14100/12040 tok/s;   1414 sec\n",
      "[2019-02-16 15:49:01,861 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n",
      "[2019-02-16 15:49:21,636 INFO] Validation perplexity: 11.722\n",
      "[2019-02-16 15:49:21,637 INFO] Validation accuracy: 54.5965\n",
      "[2019-02-16 15:49:42,068 INFO] Step 6100/20000; acc:  58.52; ppl:  7.42; xent: 2.00; lr: 0.00100; 8023/6822 tok/s;   1455 sec\n",
      "[2019-02-16 15:50:01,397 INFO] Step 6200/20000; acc:  59.20; ppl:  7.05; xent: 1.95; lr: 0.00100; 15746/13463 tok/s;   1474 sec\n",
      "[2019-02-16 15:50:21,375 INFO] Step 6300/20000; acc:  59.06; ppl:  7.17; xent: 1.97; lr: 0.00100; 15989/13636 tok/s;   1494 sec\n",
      "[2019-02-16 15:50:41,246 INFO] Step 6400/20000; acc:  59.31; ppl:  7.07; xent: 1.96; lr: 0.00100; 16170/13693 tok/s;   1514 sec\n",
      "[2019-02-16 15:51:00,649 INFO] Step 6500/20000; acc:  59.69; ppl:  6.83; xent: 1.92; lr: 0.00100; 15610/13362 tok/s;   1533 sec\n",
      "[2019-02-16 15:51:00,769 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n",
      "[2019-02-16 15:51:20,638 INFO] Saving checkpoint /mnt/drive-2t/model/opennmt_6082f2c966d9ea8dbb7afb565d719193_step_6500.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "meet early stop condition, prev best loss: 54.596531154711215 current loss: 54.21470925529172\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'6082f2c966d9ea8dbb7afb565d719193'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_args = {\n",
    "    'model_dtype': 'fp32',\n",
    "    'encoder_type': 'brnn',\n",
    "    'decoder_type': 'rnn',\n",
    "#     'rnn_type': 'GRU',\n",
    "    'layers': 3,\n",
    "#     'max_generator_batches': 2,\n",
    "    'dropout': 0.3,\n",
    "    'enc_rnn_size': 512,\n",
    "    'dec_rnn_size': 512,\n",
    "    'src_word_vec_size': 512,\n",
    "    'tgt_word_vec_size': 512\n",
    "}\n",
    "train_args = {\n",
    "    'batch_size': 128,\n",
    "    'train_steps': 20000,\n",
    "    'optim': 'adam',\n",
    "    'learning_rate': 0.001,\n",
    "    'valid_step': 500,\n",
    "    'valid_batch': 64,\n",
    "    'early_stop_round': 5,\n",
    "    'early_stop_threshold': 0.01,\n",
    "    'report_every': 100\n",
    "}\n",
    "\n",
    "total_args = {}\n",
    "total_args.update(model_args)\n",
    "total_args.update(train_args)\n",
    "\n",
    "reverse_runner.run(total_args)\n",
    "reverse_runner.model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-16 15:52:50,313 INFO] Translating shard 0.\n",
      "[2019-02-16 15:54:43,488 INFO] Translating shard 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRED AVG SCORE: -0.6527, PRED PPL: 1.9208\n",
      "GOLD AVG SCORE: -10.9694, GOLD PPL: 58069.3216\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-16 15:56:40,825 INFO] Translating shard 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRED AVG SCORE: -0.6641, PRED PPL: 1.9427\n",
      "GOLD AVG SCORE: -10.9644, GOLD PPL: 57782.8891\n",
      "PRED AVG SCORE: -0.6866, PRED PPL: 1.9870\n",
      "GOLD AVG SCORE: -10.8774, GOLD PPL: 52963.2560\n"
     ]
    }
   ],
   "source": [
    "model_id = '6082f2c966d9ea8dbb7afb565d719193'\n",
    "args = build_translate_args_reverse(model_id)\n",
    "args['batch_size'] = 32\n",
    "reverse_runner.translate(args, model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"data\": \"data/nmt15-reverse\",\n",
      "    \"save_model\": \"/mnt/drive-2t/model/opennmt_6082f2c966d9ea8dbb7afb565d719193\",\n",
      "    \"gpu_ranks\": 0,\n",
      "    \"tensorboard\": null,\n",
      "    \"tensorboard_log_dir\": \"/mnt/drive-2t/log/opennmt_6082f2c966d9ea8dbb7afb565d719193/log\",\n",
      "    \"model_dtype\": \"fp32\",\n",
      "    \"encoder_type\": \"brnn\",\n",
      "    \"decoder_type\": \"rnn\",\n",
      "    \"layers\": 3,\n",
      "    \"dropout\": 0.3,\n",
      "    \"enc_rnn_size\": 512,\n",
      "    \"dec_rnn_size\": 512,\n",
      "    \"src_word_vec_size\": 512,\n",
      "    \"tgt_word_vec_size\": 512,\n",
      "    \"batch_size\": 128,\n",
      "    \"train_steps\": 20000,\n",
      "    \"optim\": \"adam\",\n",
      "    \"learning_rate\": 0.001,\n",
      "    \"valid_step\": 500,\n",
      "    \"valid_batch\": 64,\n",
      "    \"early_stop_round\": 5,\n",
      "    \"early_stop_threshold\": 0.01,\n",
      "    \"report_every\": 100\n",
      "}\n",
      "37.86 22.47\n",
      "Now I ended up , and as I mentioned earlier before , of course I have a lot of data if you care , but I just want to give you this basic idea of communication to the brain with its language , and the potential power of the possibility .\n",
      "\n",
      "Now I just want to wrap up , and as I was mentioning earlier of course I have a lot of other data if you &apos;re interested , but I just wanted to give this sort of basic idea of being able to communicate with the brain in its language , and the potential power of being able to do that .\n",
      "-------------\n",
      "This is very different with artificial design equipment that you have to have to communicate from the brain to a device . There we have to have to have the right to communicate from the brain to a device .\n",
      "\n",
      "So it &apos;s different from the motor prosthetics where you &apos;re communicating from the brain to a device . Here we have to communicate from the outside world into the brain and be understood , and be understood by the brain .\n",
      "-------------\n",
      "And then , the last thing I &apos;d like to say , really , I want to highlight that this idea can evolve .\n",
      "\n",
      "And then the last thing I wanted to say , really , is to emphasize that the idea generalizes .\n",
      "-------------\n",
      "The strategy that we use to find code for the retina , we can also use to find code for other areas , for example , with cerebral brain relationships and lenses , to điều khiếm and motion .\n",
      "\n",
      "So the same strategy that we used to find the code for the retina we can also use to find the code for other areas , for example , the auditory system and the motor system , so for treating deafness and for motor disorders .\n",
      "-------------\n",
      "And by the way , we passed through the current network that &apos;s broken by the retina to the stem cells , and we can ignore the mạch force of the retina , and we can ignore the mạch of hỏng in the ears to come to the mental brain , or to ignore the damaged areas in the cortex , in the cortex cortex , to ignore the hư of the cerebral cortex , in the cortex cortex , to put rid of it .\n",
      "\n",
      "So just the same way that we were able to jump over the damaged circuitry in the retina to get to the retina &apos;s output cells , we can jump over the damaged circuitry in the cochlea to get the auditory nerve , or jump over damaged areas in the cortex , in the motor cortex , to bridge the gap produced by a stroke .\n",
      "-------------\n",
      "I just want to end with a simple message that understanding the code real , really important , and if we can understand code , the language of the brain , which previously seems impossible to be possible . Thank you .\n",
      "\n",
      "I just want to end with a simple message that understanding the code is really , really important , and if we can understand the code , the language of the brain , things become possible that didn &apos;t seem obviously possible before . Thank you .\n",
      "-------------\n",
      "Jonathan Harris : tầm with stories\n",
      "\n",
      "Jonathan Harris : The web as art\n",
      "-------------\n",
      "In the hội Conference of 2007 , artist Porter sketch talks about the latest projects of his latest projects , including collecting stories : stories of strangers , strangers , and stories from the Internet , there &apos;s amazing projects , &quot; We &apos;re okay . &quot;\n",
      "\n",
      "At the EG conference in December 2007 , artist Jonathan Harris discusses his latest projects , which involve collecting stories : his own , strangers &apos; , and stories collected from the Internet , including his amazing &quot; We Feel Fine . &quot;\n",
      "-------------\n",
      "I &apos;m going to talk about collections in a few different ways .\n",
      "\n",
      "So I &apos;m going to talk today about collecting stories in some unconventional ways .\n",
      "-------------\n",
      "So this is a photograph I &apos;m nervous -- a very awkward thing .\n",
      "\n",
      "This is a picture of me from a very awkward stage in my life .\n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "evaluate_human('6082f2c966d9ea8dbb7afb565d719193')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Change early stop criterion to loss from acc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-17 14:32:11,800 INFO]  * src vocab size = 8870\n",
      "[2019-02-17 14:32:11,803 INFO]  * tgt vocab size = 20071\n",
      "[2019-02-17 14:32:11,803 INFO] Building model...\n",
      "[2019-02-17 14:32:15,534 INFO] NMTModel(\n",
      "  (encoder): RNNEncoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(8870, 512, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (rnn): LSTM(512, 256, num_layers=3, dropout=0.3, bidirectional=True)\n",
      "  )\n",
      "  (decoder): InputFeedRNNDecoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(20071, 512, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.3)\n",
      "    (rnn): StackedLSTM(\n",
      "      (dropout): Dropout(p=0.3)\n",
      "      (layers): ModuleList(\n",
      "        (0): LSTMCell(1024, 512)\n",
      "        (1): LSTMCell(512, 512)\n",
      "        (2): LSTMCell(512, 512)\n",
      "      )\n",
      "    )\n",
      "    (attn): GlobalAttention(\n",
      "      (linear_in): Linear(in_features=512, out_features=512, bias=False)\n",
      "      (linear_out): Linear(in_features=1024, out_features=512, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (generator): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=20071, bias=True)\n",
      "    (1): Cast()\n",
      "    (2): LogSoftmax()\n",
      "  )\n",
      ")\n",
      "[2019-02-17 14:32:15,536 INFO] encoder: 9272320\n",
      "[2019-02-17 14:32:15,536 INFO] decoder: 28711527\n",
      "[2019-02-17 14:32:15,537 INFO] * number of parameters: 37983847\n",
      "[2019-02-17 14:32:15,669 INFO] Starting training on GPU: [0]\n",
      "[2019-02-17 14:32:15,670 INFO] Start training loop and validate every 500 steps...\n",
      "[2019-02-17 14:32:17,130 INFO] Loading dataset from data/nmt15-reverse.train.0.pt, number of examples: 95844\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Testing GPU memory capability with batch shape: torch.Size([200, 128, 1]) torch.Size([190, 128, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-17 14:33:19,592 INFO] Step 100/100000; loss: 4036334.778355 acc:   8.13; ppl: 785.17; xent: 6.67; lr: 0.00100; 11879/9473 tok/s;     64 sec\n",
      "[2019-02-17 14:34:06,330 INFO] Step 200/100000; loss: 2039835.030886 acc:  19.86; ppl: 219.52; xent: 5.39; lr: 0.00100; 9789/8096 tok/s;    111 sec\n",
      "[2019-02-17 14:34:49,150 INFO] Step 300/100000; loss: 1385664.103316 acc:  28.66; ppl: 110.14; xent: 4.70; lr: 0.00100; 8058/6883 tok/s;    153 sec\n",
      "[2019-02-17 14:35:28,084 INFO] Step 400/100000; loss: 991060.433813 acc:  34.69; ppl: 67.62; xent: 4.21; lr: 0.00100; 6953/6041 tok/s;    192 sec\n"
     ]
    }
   ],
   "source": [
    "model_args = {\n",
    "    'model_dtype': 'fp32',\n",
    "    'encoder_type': 'brnn',\n",
    "    'decoder_type': 'rnn',\n",
    "#     'rnn_type': 'GRU',\n",
    "    'layers': 3,\n",
    "#     'max_generator_batches': 2,\n",
    "    'dropout': 0.3,\n",
    "    'enc_rnn_size': 512,\n",
    "    'dec_rnn_size': 512,\n",
    "    'src_word_vec_size': 512,\n",
    "    'tgt_word_vec_size': 512\n",
    "}\n",
    "train_args = {\n",
    "    'batch_size': 128,\n",
    "    'train_steps': 100000,\n",
    "    'optim': 'adam',\n",
    "    'learning_rate': 0.001,\n",
    "    'valid_step': 500,\n",
    "    'valid_batch': 64,\n",
    "    'early_stop_round': 10,\n",
    "    'early_stop_threshold': 0,\n",
    "    'report_every': 100\n",
    "}\n",
    "\n",
    "total_args = {}\n",
    "total_args.update(model_args)\n",
    "total_args.update(train_args)\n",
    "\n",
    "reverse_runner.run(total_args)\n",
    "reverse_runner.model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-17 11:47:52,429 INFO] Translating shard 0.\n",
      "[2019-02-17 11:49:32,224 INFO] Translating shard 1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRED AVG SCORE: -0.5925, PRED PPL: 1.8085\n",
      "GOLD AVG SCORE: -11.1411, GOLD PPL: 68947.5836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-17 11:51:14,967 INFO] Translating shard 2.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PRED AVG SCORE: -0.6029, PRED PPL: 1.8274\n",
      "GOLD AVG SCORE: -11.1182, GOLD PPL: 67386.9716\n",
      "PRED AVG SCORE: -0.6267, PRED PPL: 1.8714\n",
      "GOLD AVG SCORE: -11.0101, GOLD PPL: 60480.0534\n"
     ]
    }
   ],
   "source": [
    "model_id = '091ebe12a299ced931600c799f0937b5'\n",
    "args = build_translate_args_reverse(model_id)\n",
    "args['batch_size'] = 32\n",
    "reverse_runner.translate(args, model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"data\": \"data/nmt15-reverse\",\n",
      "    \"save_model\": \"/mnt/drive-2t/model/opennmt_091ebe12a299ced931600c799f0937b5\",\n",
      "    \"gpu_ranks\": 0,\n",
      "    \"tensorboard\": null,\n",
      "    \"tensorboard_log_dir\": \"/mnt/drive-2t/log/opennmt_091ebe12a299ced931600c799f0937b5/log\",\n",
      "    \"model_dtype\": \"fp32\",\n",
      "    \"encoder_type\": \"brnn\",\n",
      "    \"decoder_type\": \"rnn\",\n",
      "    \"layers\": 3,\n",
      "    \"dropout\": 0.3,\n",
      "    \"enc_rnn_size\": 512,\n",
      "    \"dec_rnn_size\": 512,\n",
      "    \"src_word_vec_size\": 512,\n",
      "    \"tgt_word_vec_size\": 512,\n",
      "    \"batch_size\": 64,\n",
      "    \"train_steps\": 100000,\n",
      "    \"optim\": \"adam\",\n",
      "    \"learning_rate\": 0.001,\n",
      "    \"valid_step\": 500,\n",
      "    \"valid_batch\": 32,\n",
      "    \"early_stop_round\": 5,\n",
      "    \"early_stop_threshold\": 0.01,\n",
      "    \"report_every\": 100\n",
      "}\n",
      "37.51 22.73\n",
      "Now , it &apos;s time that I end up , and as I mentioned earlier , of course , I have a lot of data if you care , but I just want to show this fundamental idea of communication to the brain in its language , and the potential power of that capacity .\n",
      "\n",
      "Now I just want to wrap up , and as I was mentioning earlier of course I have a lot of other data if you &apos;re interested , but I just wanted to give this sort of basic idea of being able to communicate with the brain in its language , and the potential power of being able to do that .\n",
      "-------------\n",
      "This is very different from the computer that you have to be the interface from the brain to a device . Here we have to be the communication of the outside outside in the brain , and understands , the brain understands .\n",
      "\n",
      "So it &apos;s different from the motor prosthetics where you &apos;re communicating from the brain to a device . Here we have to communicate from the outside world into the brain and be understood , and be understood by the brain .\n",
      "-------------\n",
      "And then the last thing I &apos;d like to say , really , I want to emphasize that this idea can be broadly .\n",
      "\n",
      "And then the last thing I wanted to say , really , is to emphasize that the idea generalizes .\n",
      "-------------\n",
      "The strategy that we use for the code of the arc , we also can use the code for other areas , for example , like hearing and bones , to treat endangered and disorders .\n",
      "\n",
      "So the same strategy that we used to find the code for the retina we can also use to find the code for other areas , for example , the auditory system and the motor system , so for treating deafness and for motor disorders .\n",
      "-------------\n",
      "Now , by the way we passed through the network that collapsed in the retina to go to the first cells of the arc of the retina , we can also ignore the network that collapsed in the ears to get the acoustic nerves , or to ignore the marine regions in the cortex , in the prefrontal cortex , to tr��m in the cracks that the stroke creates .\n",
      "\n",
      "So just the same way that we were able to jump over the damaged circuitry in the retina to get to the retina &apos;s output cells , we can jump over the damaged circuitry in the cochlea to get the auditory nerve , or jump over damaged areas in the cortex , in the motor cortex , to bridge the gap produced by a stroke .\n",
      "-------------\n",
      "I just want to end with a simple message that understands the fact that the code is really , really important , and if we can understand the code , the language of the brain , what it has before doesn &apos;t seem to be possible . Thank you .\n",
      "\n",
      "I just want to end with a simple message that understanding the code is really , really important , and if we can understand the code , the language of the brain , things become possible that didn &apos;t seem obviously possible before . Thank you .\n",
      "-------------\n",
      "Jonathan Harris : Moving stories\n",
      "\n",
      "Jonathan Harris : The web as art\n",
      "-------------\n",
      "In the conference of December 2007 , artist Jonathan Harris talks about his latest project , including collect stories : stories of self , and the Internet , in which there was great projects , &quot; We were okay . &quot;\n",
      "\n",
      "At the EG conference in December 2007 , artist Jonathan Harris discusses his latest projects , which involve collecting stories : his own , strangers &apos; , and stories collected from the Internet , including his amazing &quot; We Feel Fine . &quot;\n",
      "-------------\n",
      "I &apos;m going to talk today about cooperation in some other way in common .\n",
      "\n",
      "So I &apos;m going to talk today about collecting stories in some unconventional ways .\n",
      "-------------\n",
      "This is a photograph me -- a very humbling awkward photograph .\n",
      "\n",
      "This is a picture of me from a very awkward stage in my life .\n",
      "-------------\n"
     ]
    }
   ],
   "source": [
    "model_id = '091ebe12a299ced931600c799f0937b5'\n",
    "evaluate_human(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-17 00:21:24,263 INFO]  * src vocab size = 8870\n",
      "[2019-02-17 00:21:24,264 INFO]  * tgt vocab size = 20071\n",
      "[2019-02-17 00:21:24,266 INFO] Building model...\n",
      "[2019-02-17 00:21:28,526 INFO] NMTModel(\n",
      "  (encoder): RNNEncoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(8870, 500, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (rnn): LSTM(500, 500, num_layers=2, dropout=0.3)\n",
      "  )\n",
      "  (decoder): InputFeedRNNDecoder(\n",
      "    (embeddings): Embeddings(\n",
      "      (make_embedding): Sequential(\n",
      "        (emb_luts): Elementwise(\n",
      "          (0): Embedding(20071, 500, padding_idx=1)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (dropout): Dropout(p=0.3)\n",
      "    (rnn): StackedLSTM(\n",
      "      (dropout): Dropout(p=0.3)\n",
      "      (layers): ModuleList(\n",
      "        (0): LSTMCell(1000, 500)\n",
      "        (1): LSTMCell(500, 500)\n",
      "      )\n",
      "    )\n",
      "    (attn): GlobalAttention(\n",
      "      (linear_in): Linear(in_features=500, out_features=500, bias=False)\n",
      "      (linear_out): Linear(in_features=1000, out_features=500, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (generator): Sequential(\n",
      "    (0): Linear(in_features=500, out_features=20071, bias=True)\n",
      "    (1): Cast()\n",
      "    (2): LogSoftmax()\n",
      "  )\n",
      ")\n",
      "[2019-02-17 00:21:28,528 INFO] encoder: 8443000\n",
      "[2019-02-17 00:21:28,529 INFO] decoder: 25849071\n",
      "[2019-02-17 00:21:28,529 INFO] * number of parameters: 34292071\n",
      "[2019-02-17 00:21:28,720 INFO] Starting training on GPU: [0]\n",
      "[2019-02-17 00:21:28,721 INFO] Start training loop and validate every 500 steps...\n",
      "[2019-02-17 00:21:30,167 INFO] Loading dataset from data/nmt15-reverse.train.0.pt, number of examples: 95844\n",
      "[2019-02-17 00:21:56,652 INFO] Step 100/100000; loss: 876133.121722 acc:   8.99; ppl: 679.25; xent: 6.52; lr: 0.00100; 5641/4810 tok/s;     28 sec\n",
      "[2019-02-17 00:22:23,407 INFO] Step 200/100000; loss: 753904.715921 acc:  18.18; ppl: 273.13; xent: 5.61; lr: 0.00100; 5897/5023 tok/s;     55 sec\n",
      "[2019-02-17 00:22:50,802 INFO] Step 300/100000; loss: 705689.513741 acc:  22.22; ppl: 187.17; xent: 5.23; lr: 0.00100; 5757/4924 tok/s;     82 sec\n",
      "[2019-02-17 00:23:18,578 INFO] Step 400/100000; loss: 679971.900039 acc:  24.22; ppl: 157.94; xent: 5.06; lr: 0.00100; 5687/4836 tok/s;    110 sec\n",
      "[2019-02-17 00:23:46,625 INFO] Step 500/100000; loss: 654099.696236 acc:  26.15; ppl: 134.49; xent: 4.90; lr: 0.00100; 5608/4759 tok/s;    138 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last accent valid acc: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-17 00:23:47,013 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n",
      "[2019-02-17 00:24:08,988 INFO] Validation perplexity: 103.056\n",
      "[2019-02-17 00:24:08,989 INFO] Validation accuracy: 28.6024\n",
      "[2019-02-17 00:24:38,637 INFO] Step 600/100000; loss: 630023.420390 acc:  29.17; ppl: 107.71; xent: 4.68; lr: 0.00100; 3046/2589 tok/s;    190 sec\n",
      "[2019-02-17 00:25:08,394 INFO] Step 700/100000; loss: 602951.637411 acc:  31.57; ppl: 88.65; xent: 4.48; lr: 0.00100; 5284/4518 tok/s;    220 sec\n",
      "[2019-02-17 00:25:38,678 INFO] Step 800/100000; loss: 591457.869202 acc:  33.23; ppl: 77.78; xent: 4.35; lr: 0.00100; 5279/4486 tok/s;    250 sec\n",
      "[2019-02-17 00:26:09,433 INFO] Step 900/100000; loss: 565543.698792 acc:  35.44; ppl: 64.40; xent: 4.17; lr: 0.00100; 5207/4415 tok/s;    281 sec\n",
      "[2019-02-17 00:26:40,538 INFO] Step 1000/100000; loss: 542394.963114 acc:  36.82; ppl: 56.35; xent: 4.03; lr: 0.00100; 5059/4325 tok/s;    312 sec\n",
      "[2019-02-17 00:26:40,646 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last accent valid acc: -1042816.4230957031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-17 00:27:03,315 INFO] Validation perplexity: 45.7972\n",
      "[2019-02-17 00:27:03,315 INFO] Validation accuracy: 38.9023\n",
      "[2019-02-17 00:27:34,912 INFO] Step 1100/100000; loss: 535783.966767 acc:  37.60; ppl: 52.39; xent: 3.96; lr: 0.00100; 2914/2489 tok/s;    366 sec\n",
      "[2019-02-17 00:28:06,741 INFO] Step 1200/100000; loss: 520141.352391 acc:  39.06; ppl: 46.32; xent: 3.84; lr: 0.00100; 5013/4261 tok/s;    398 sec\n",
      "[2019-02-17 00:28:38,868 INFO] Step 1300/100000; loss: 502548.408494 acc:  40.11; ppl: 41.99; xent: 3.74; lr: 0.00100; 4902/4185 tok/s;    430 sec\n",
      "[2019-02-17 00:29:11,669 INFO] Step 1400/100000; loss: 501804.743335 acc:  40.71; ppl: 39.98; xent: 3.69; lr: 0.00100; 4883/4148 tok/s;    463 sec\n",
      "[2019-02-17 00:29:45,750 INFO] Loading dataset from data/nmt15-reverse.train.0.pt, number of examples: 95844\n",
      "[2019-02-17 00:29:46,645 INFO] Step 1500/100000; loss: 482832.172116 acc:  41.61; ppl: 36.18; xent: 3.59; lr: 0.00100; 4517/3847 tok/s;    498 sec\n",
      "[2019-02-17 00:29:46,749 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last accent valid acc: -860350.5795898438\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-17 00:30:09,541 INFO] Validation perplexity: 29.6313\n",
      "[2019-02-17 00:30:09,542 INFO] Validation accuracy: 43.9113\n",
      "[2019-02-17 00:30:42,452 INFO] Step 1600/100000; loss: 465076.911014 acc:  42.50; ppl: 33.32; xent: 3.51; lr: 0.00100; 2781/2377 tok/s;    554 sec\n",
      "[2019-02-17 00:31:15,285 INFO] Step 1700/100000; loss: 464986.962854 acc:  42.82; ppl: 31.78; xent: 3.46; lr: 0.00100; 4811/4095 tok/s;    587 sec\n",
      "[2019-02-17 00:31:48,501 INFO] Step 1800/100000; loss: 460733.670228 acc:  43.17; ppl: 30.54; xent: 3.42; lr: 0.00100; 4747/4057 tok/s;    620 sec\n",
      "[2019-02-17 00:32:21,704 INFO] Step 1900/100000; loss: 451177.222448 acc:  43.84; ppl: 28.60; xent: 3.35; lr: 0.00100; 4762/4052 tok/s;    653 sec\n",
      "[2019-02-17 00:32:54,753 INFO] Step 2000/100000; loss: 444609.743636 acc:  44.78; ppl: 26.80; xent: 3.29; lr: 0.00100; 4821/4091 tok/s;    686 sec\n",
      "[2019-02-17 00:32:54,857 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last accent valid acc: -762398.6516113281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-17 00:33:17,766 INFO] Validation perplexity: 22.7443\n",
      "[2019-02-17 00:33:17,767 INFO] Validation accuracy: 46.3507\n",
      "[2019-02-17 00:33:50,921 INFO] Step 2100/100000; loss: 436440.474475 acc:  45.03; ppl: 25.55; xent: 3.24; lr: 0.00100; 2824/2398 tok/s;    742 sec\n",
      "[2019-02-17 00:34:23,520 INFO] Step 2200/100000; loss: 421839.985645 acc:  45.35; ppl: 24.35; xent: 3.19; lr: 0.00100; 4743/4053 tok/s;    775 sec\n",
      "[2019-02-17 00:34:56,999 INFO] Step 2300/100000; loss: 445731.467810 acc:  45.40; ppl: 24.18; xent: 3.19; lr: 0.00100; 4921/4180 tok/s;    808 sec\n",
      "[2019-02-17 00:35:29,979 INFO] Step 2400/100000; loss: 406463.014549 acc:  46.82; ppl: 21.31; xent: 3.06; lr: 0.00100; 4747/4029 tok/s;    841 sec\n",
      "[2019-02-17 00:36:03,009 INFO] Step 2500/100000; loss: 409024.372836 acc:  46.75; ppl: 21.04; xent: 3.05; lr: 0.00100; 4757/4065 tok/s;    874 sec\n",
      "[2019-02-17 00:36:03,117 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last accent valid acc: -702889.7547607422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-17 00:36:26,093 INFO] Validation perplexity: 19.6206\n",
      "[2019-02-17 00:36:26,094 INFO] Validation accuracy: 48.1447\n",
      "[2019-02-17 00:36:59,606 INFO] Step 2600/100000; loss: 419114.404120 acc:  46.64; ppl: 21.01; xent: 3.04; lr: 0.00100; 2850/2432 tok/s;    931 sec\n",
      "[2019-02-17 00:37:32,438 INFO] Step 2700/100000; loss: 393030.335198 acc:  47.93; ppl: 19.27; xent: 2.96; lr: 0.00100; 4749/4046 tok/s;    964 sec\n",
      "[2019-02-17 00:38:05,350 INFO] Step 2800/100000; loss: 398077.013215 acc:  47.94; ppl: 18.92; xent: 2.94; lr: 0.00100; 4808/4114 tok/s;    997 sec\n",
      "[2019-02-17 00:38:38,580 INFO] Step 2900/100000; loss: 396421.135048 acc:  48.18; ppl: 18.70; xent: 2.93; lr: 0.00100; 4804/4074 tok/s;   1030 sec\n",
      "[2019-02-17 00:39:12,310 INFO] Loading dataset from data/nmt15-reverse.train.0.pt, number of examples: 95844\n",
      "[2019-02-17 00:39:13,893 INFO] Step 3000/100000; loss: 391482.034637 acc:  48.54; ppl: 17.83; xent: 2.88; lr: 0.00100; 4514/3848 tok/s;   1065 sec\n",
      "[2019-02-17 00:39:13,995 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last accent valid acc: -669653.4422607422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-17 00:39:36,689 INFO] Validation perplexity: 17.2655\n",
      "[2019-02-17 00:39:36,690 INFO] Validation accuracy: 49.5066\n",
      "[2019-02-17 00:40:09,931 INFO] Step 3100/100000; loss: 375768.247467 acc:  49.15; ppl: 17.00; xent: 2.83; lr: 0.00100; 2777/2367 tok/s;   1121 sec\n",
      "[2019-02-17 00:40:42,986 INFO] Step 3200/100000; loss: 375364.015007 acc:  49.08; ppl: 16.86; xent: 2.83; lr: 0.00100; 4718/4020 tok/s;   1154 sec\n",
      "[2019-02-17 00:41:16,263 INFO] Step 3300/100000; loss: 378911.260649 acc:  49.14; ppl: 16.68; xent: 2.81; lr: 0.00100; 4739/4046 tok/s;   1188 sec\n",
      "[2019-02-17 00:41:49,655 INFO] Step 3400/100000; loss: 376604.460515 acc:  49.37; ppl: 16.20; xent: 2.79; lr: 0.00100; 4755/4049 tok/s;   1221 sec\n",
      "[2019-02-17 00:42:22,640 INFO] Step 3500/100000; loss: 375680.314608 acc:  50.05; ppl: 15.66; xent: 2.75; lr: 0.00100; 4878/4140 tok/s;   1254 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last accent valid acc: -640885.4837036133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-17 00:42:22,858 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n",
      "[2019-02-17 00:42:45,820 INFO] Validation perplexity: 15.8476\n",
      "[2019-02-17 00:42:45,821 INFO] Validation accuracy: 50.3151\n",
      "[2019-02-17 00:43:19,215 INFO] Step 3600/100000; loss: 365947.096657 acc:  50.05; ppl: 15.32; xent: 2.73; lr: 0.00100; 2795/2370 tok/s;   1310 sec\n",
      "[2019-02-17 00:43:51,809 INFO] Step 3700/100000; loss: 353436.281603 acc:  50.30; ppl: 14.89; xent: 2.70; lr: 0.00100; 4683/4016 tok/s;   1343 sec\n",
      "[2019-02-17 00:44:25,255 INFO] Step 3800/100000; loss: 387970.563111 acc:  49.90; ppl: 15.34; xent: 2.73; lr: 0.00100; 5020/4248 tok/s;   1377 sec\n",
      "[2019-02-17 00:44:58,098 INFO] Step 3900/100000; loss: 345625.314829 acc:  51.33; ppl: 13.60; xent: 2.61; lr: 0.00100; 4744/4033 tok/s;   1409 sec\n",
      "[2019-02-17 00:45:31,001 INFO] Step 4000/100000; loss: 347168.978542 acc:  51.25; ppl: 13.63; xent: 2.61; lr: 0.00100; 4718/4039 tok/s;   1442 sec\n",
      "[2019-02-17 00:45:31,103 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last accent valid acc: -621606.5955810547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-17 00:45:54,066 INFO] Validation perplexity: 15.208\n",
      "[2019-02-17 00:45:54,067 INFO] Validation accuracy: 50.8299\n",
      "[2019-02-17 00:46:27,202 INFO] Step 4100/100000; loss: 362597.603580 acc:  50.80; ppl: 13.97; xent: 2.64; lr: 0.00100; 2869/2447 tok/s;   1498 sec\n",
      "[2019-02-17 00:47:00,427 INFO] Step 4200/100000; loss: 349466.582035 acc:  51.85; ppl: 13.11; xent: 2.57; lr: 0.00100; 4803/4087 tok/s;   1532 sec\n",
      "[2019-02-17 00:47:33,568 INFO] Step 4300/100000; loss: 345397.428776 acc:  51.76; ppl: 12.93; xent: 2.56; lr: 0.00100; 4762/4072 tok/s;   1565 sec\n",
      "[2019-02-17 00:48:07,035 INFO] Step 4400/100000; loss: 341799.550614 acc:  52.07; ppl: 12.82; xent: 2.55; lr: 0.00100; 4722/4004 tok/s;   1598 sec\n",
      "[2019-02-17 00:48:39,876 INFO] Loading dataset from data/nmt15-reverse.train.0.pt, number of examples: 95844\n",
      "[2019-02-17 00:48:42,212 INFO] Step 4500/100000; loss: 342140.221690 acc:  52.11; ppl: 12.62; xent: 2.54; lr: 0.00100; 4500/3836 tok/s;   1633 sec\n",
      "[2019-02-17 00:48:42,312 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last accent valid acc: -612339.8005981445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-17 00:49:04,894 INFO] Validation perplexity: 14.131\n",
      "[2019-02-17 00:49:04,895 INFO] Validation accuracy: 51.5882\n",
      "[2019-02-17 00:49:37,834 INFO] Step 4600/100000; loss: 333581.438071 acc:  52.40; ppl: 12.28; xent: 2.51; lr: 0.00100; 2806/2392 tok/s;   1689 sec\n",
      "[2019-02-17 00:50:10,543 INFO] Step 4700/100000; loss: 327371.396634 acc:  52.61; ppl: 12.12; xent: 2.49; lr: 0.00100; 4696/4012 tok/s;   1722 sec\n",
      "[2019-02-17 00:50:43,957 INFO] Step 4800/100000; loss: 338522.389313 acc:  52.44; ppl: 12.24; xent: 2.50; lr: 0.00100; 4738/4045 tok/s;   1755 sec\n",
      "[2019-02-17 00:51:17,382 INFO] Step 4900/100000; loss: 345938.031763 acc:  52.23; ppl: 12.13; xent: 2.50; lr: 0.00100; 4889/4147 tok/s;   1789 sec\n",
      "[2019-02-17 00:51:50,287 INFO] Step 5000/100000; loss: 328542.505356 acc:  53.26; ppl: 11.48; xent: 2.44; lr: 0.00100; 4809/4091 tok/s;   1822 sec\n",
      "[2019-02-17 00:51:50,390 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last accent valid acc: -595814.4460449219\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-17 00:52:13,359 INFO] Validation perplexity: 13.6505\n",
      "[2019-02-17 00:52:13,360 INFO] Validation accuracy: 52.0994\n",
      "[2019-02-17 00:52:13,360 INFO] Saving checkpoint /mnt/drive-2t/model/opennmt_153a72b0efdd37e8921bffa8bd6cc110_step_5000.pt\n",
      "[2019-02-17 00:52:49,001 INFO] Step 5100/100000; loss: 327113.239679 acc:  53.16; ppl: 11.58; xent: 2.45; lr: 0.00100; 2684/2275 tok/s;   1880 sec\n",
      "[2019-02-17 00:53:21,465 INFO] Step 5200/100000; loss: 318204.220895 acc:  53.35; ppl: 11.26; xent: 2.42; lr: 0.00100; 4706/4049 tok/s;   1913 sec\n",
      "[2019-02-17 00:53:54,926 INFO] Step 5300/100000; loss: 345108.844725 acc:  53.06; ppl: 11.58; xent: 2.45; lr: 0.00100; 4993/4211 tok/s;   1946 sec\n",
      "[2019-02-17 00:54:27,990 INFO] Step 5400/100000; loss: 314860.199137 acc:  54.16; ppl: 10.54; xent: 2.36; lr: 0.00100; 4757/4044 tok/s;   1979 sec\n",
      "[2019-02-17 00:55:01,090 INFO] Step 5500/100000; loss: 315849.714545 acc:  53.84; ppl: 10.71; xent: 2.37; lr: 0.00100; 4693/4025 tok/s;   2012 sec\n",
      "[2019-02-17 00:55:01,194 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last accent valid acc: -588030.954284668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-17 00:55:24,158 INFO] Validation perplexity: 13.4929\n",
      "[2019-02-17 00:55:24,159 INFO] Validation accuracy: 52.2745\n",
      "[2019-02-17 00:55:57,309 INFO] Step 5600/100000; loss: 325547.151342 acc:  53.73; ppl: 10.91; xent: 2.39; lr: 0.00100; 2842/2423 tok/s;   2069 sec\n",
      "[2019-02-17 00:56:30,633 INFO] Step 5700/100000; loss: 321352.835857 acc:  54.41; ppl: 10.40; xent: 2.34; lr: 0.00100; 4848/4118 tok/s;   2102 sec\n",
      "[2019-02-17 00:57:03,612 INFO] Step 5800/100000; loss: 320245.878460 acc:  54.24; ppl: 10.38; xent: 2.34; lr: 0.00100; 4862/4150 tok/s;   2135 sec\n",
      "[2019-02-17 00:57:36,718 INFO] Step 5900/100000; loss: 301996.138234 acc:  54.89; ppl: 10.02; xent: 2.30; lr: 0.00100; 4656/3959 tok/s;   2168 sec\n",
      "[2019-02-17 00:58:09,216 INFO] Loading dataset from data/nmt15-reverse.train.0.pt, number of examples: 95844\n",
      "[2019-02-17 00:58:12,168 INFO] Step 6000/100000; loss: 314026.741962 acc:  54.50; ppl: 10.14; xent: 2.32; lr: 0.00100; 4482/3824 tok/s;   2203 sec\n",
      "[2019-02-17 00:58:12,265 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last accent valid acc: -585419.2310180664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-17 00:58:34,962 INFO] Validation perplexity: 12.9807\n",
      "[2019-02-17 00:58:34,963 INFO] Validation accuracy: 52.5647\n",
      "[2019-02-17 00:59:07,671 INFO] Step 6100/100000; loss: 304596.544668 acc:  54.85; ppl:  9.95; xent: 2.30; lr: 0.00100; 2802/2389 tok/s;   2259 sec\n",
      "[2019-02-17 00:59:40,413 INFO] Step 6200/100000; loss: 297574.771949 acc:  55.06; ppl:  9.81; xent: 2.28; lr: 0.00100; 4654/3980 tok/s;   2292 sec\n",
      "[2019-02-17 01:00:13,625 INFO] Step 6300/100000; loss: 309120.098794 acc:  54.74; ppl:  9.93; xent: 2.30; lr: 0.00100; 4759/4055 tok/s;   2325 sec\n",
      "[2019-02-17 01:00:47,117 INFO] Step 6400/100000; loss: 321156.282347 acc:  54.63; ppl:  9.94; xent: 2.30; lr: 0.00100; 4925/4176 tok/s;   2358 sec\n",
      "[2019-02-17 01:01:20,278 INFO] Step 6500/100000; loss: 306497.550754 acc:  55.31; ppl:  9.57; xent: 2.26; lr: 0.00100; 4806/4093 tok/s;   2392 sec\n",
      "[2019-02-17 01:01:20,380 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last accent valid acc: -576712.4891967773\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-17 01:01:43,264 INFO] Validation perplexity: 12.7693\n",
      "[2019-02-17 01:01:43,265 INFO] Validation accuracy: 52.8901\n",
      "[2019-02-17 01:02:16,117 INFO] Step 6600/100000; loss: 296164.481762 acc:  55.37; ppl:  9.50; xent: 2.25; lr: 0.00100; 2783/2356 tok/s;   2447 sec\n",
      "[2019-02-17 01:02:48,662 INFO] Step 6700/100000; loss: 293655.583576 acc:  55.61; ppl:  9.29; xent: 2.23; lr: 0.00100; 4686/4047 tok/s;   2480 sec\n",
      "[2019-02-17 01:03:22,634 INFO] Step 6800/100000; loss: 331008.482047 acc:  54.93; ppl:  9.80; xent: 2.28; lr: 0.00100; 5088/4268 tok/s;   2514 sec\n",
      "[2019-02-17 01:03:55,289 INFO] Step 6900/100000; loss: 279343.524548 acc:  56.71; ppl:  8.56; xent: 2.15; lr: 0.00100; 4673/3985 tok/s;   2547 sec\n",
      "[2019-02-17 01:04:28,188 INFO] Step 7000/100000; loss: 289996.461967 acc:  56.03; ppl:  8.92; xent: 2.19; lr: 0.00100; 4691/4029 tok/s;   2579 sec\n",
      "[2019-02-17 01:04:28,291 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last accent valid acc: -573019.4006347656\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-17 01:04:51,102 INFO] Validation perplexity: 12.673\n",
      "[2019-02-17 01:04:51,103 INFO] Validation accuracy: 53.0826\n",
      "[2019-02-17 01:05:24,516 INFO] Step 7100/100000; loss: 307436.566276 acc:  55.69; ppl:  9.30; xent: 2.23; lr: 0.00100; 2874/2448 tok/s;   2636 sec\n",
      "[2019-02-17 01:05:58,101 INFO] Step 7200/100000; loss: 307484.128592 acc:  56.02; ppl:  9.05; xent: 2.20; lr: 0.00100; 4912/4156 tok/s;   2669 sec\n",
      "[2019-02-17 01:06:30,852 INFO] Step 7300/100000; loss: 286787.657295 acc:  56.57; ppl:  8.57; xent: 2.15; lr: 0.00100; 4752/4075 tok/s;   2702 sec\n",
      "[2019-02-17 01:07:03,768 INFO] Step 7400/100000; loss: 283663.926278 acc:  56.61; ppl:  8.58; xent: 2.15; lr: 0.00100; 4717/4008 tok/s;   2735 sec\n",
      "[2019-02-17 01:07:35,564 INFO] Loading dataset from data/nmt15-reverse.train.0.pt, number of examples: 95844\n",
      "[2019-02-17 01:07:38,990 INFO] Step 7500/100000; loss: 289599.594447 acc:  56.40; ppl:  8.61; xent: 2.15; lr: 0.00100; 4479/3820 tok/s;   2770 sec\n",
      "[2019-02-17 01:07:39,097 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last accent valid acc: -571315.2623291016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-17 01:08:01,765 INFO] Validation perplexity: 12.3772\n",
      "[2019-02-17 01:08:01,766 INFO] Validation accuracy: 53.2853\n",
      "[2019-02-17 01:08:34,547 INFO] Step 7600/100000; loss: 282157.102832 acc:  56.64; ppl:  8.48; xent: 2.14; lr: 0.00100; 2781/2376 tok/s;   2826 sec\n",
      "[2019-02-17 01:09:07,442 INFO] Step 7700/100000; loss: 279370.579670 acc:  56.88; ppl:  8.43; xent: 2.13; lr: 0.00100; 4652/3983 tok/s;   2859 sec\n",
      "[2019-02-17 01:09:41,048 INFO] Step 7800/100000; loss: 292370.379862 acc:  56.46; ppl:  8.58; xent: 2.15; lr: 0.00100; 4762/4049 tok/s;   2892 sec\n",
      "[2019-02-17 01:10:14,312 INFO] Step 7900/100000; loss: 296181.659850 acc:  56.54; ppl:  8.48; xent: 2.14; lr: 0.00100; 4915/4165 tok/s;   2926 sec\n",
      "[2019-02-17 01:10:47,330 INFO] Step 8000/100000; loss: 289491.562200 acc:  56.91; ppl:  8.35; xent: 2.12; lr: 0.00100; 4848/4132 tok/s;   2959 sec\n",
      "[2019-02-17 01:10:47,433 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last accent valid acc: -566002.4651489258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-17 01:11:10,133 INFO] Validation perplexity: 12.4682\n",
      "[2019-02-17 01:11:10,134 INFO] Validation accuracy: 53.3511\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save last accent model with acc: -566002.4651489258\n",
      "Decent round: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-17 01:11:21,350 INFO] Step 8100/100000; loss: 288987.637007 acc:  56.59; ppl:  8.55; xent: 2.15; lr: 0.00100; 4699/3959 tok/s;   2993 sec\n",
      "[2019-02-17 01:11:31,153 INFO] Step 8200/100000; loss: 262200.664243 acc:  57.97; ppl:  7.80; xent: 2.05; lr: 0.00100; 14998/13025 tok/s;   3002 sec\n",
      "[2019-02-17 01:11:42,756 INFO] Step 8300/100000; loss: 312992.622728 acc:  56.45; ppl:  8.59; xent: 2.15; lr: 0.00100; 14946/12544 tok/s;   3014 sec\n",
      "[2019-02-17 01:11:53,163 INFO] Step 8400/100000; loss: 262507.030419 acc:  58.24; ppl:  7.56; xent: 2.02; lr: 0.00100; 14613/12470 tok/s;   3024 sec\n",
      "[2019-02-17 01:12:03,951 INFO] Step 8500/100000; loss: 275160.309348 acc:  57.70; ppl:  7.86; xent: 2.06; lr: 0.00100; 14455/12369 tok/s;   3035 sec\n",
      "[2019-02-17 01:12:04,051 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last accent valid acc: -566002.4651489258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-17 01:12:26,940 INFO] Validation perplexity: 12.3995\n",
      "[2019-02-17 01:12:26,940 INFO] Validation accuracy: 53.5142\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decent round: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-17 01:12:37,872 INFO] Step 8600/100000; loss: 286133.668957 acc:  57.21; ppl:  8.15; xent: 2.10; lr: 0.00100; 4713/4022 tok/s;   3069 sec\n",
      "[2019-02-17 01:12:49,157 INFO] Step 8700/100000; loss: 287386.635457 acc:  57.76; ppl:  7.90; xent: 2.07; lr: 0.00100; 14567/12325 tok/s;   3080 sec\n",
      "[2019-02-17 01:12:59,988 INFO] Step 8800/100000; loss: 280330.546803 acc:  57.69; ppl:  7.74; xent: 2.05; lr: 0.00100; 14807/12654 tok/s;   3091 sec\n",
      "[2019-02-17 01:13:10,566 INFO] Step 8900/100000; loss: 261443.608377 acc:  58.40; ppl:  7.48; xent: 2.01; lr: 0.00100; 14418/12280 tok/s;   3102 sec\n",
      "[2019-02-17 01:13:22,023 INFO] Loading dataset from data/nmt15-reverse.train.0.pt, number of examples: 95844\n",
      "[2019-02-17 01:13:23,375 INFO] Step 9000/100000; loss: 271088.367757 acc:  58.13; ppl:  7.59; xent: 2.03; lr: 0.00100; 12242/10442 tok/s;   3115 sec\n",
      "[2019-02-17 01:13:23,480 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last accent valid acc: -566002.4651489258\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-17 01:13:46,473 INFO] Validation perplexity: 12.2041\n",
      "[2019-02-17 01:13:46,473 INFO] Validation accuracy: 53.7275\n",
      "[2019-02-17 01:14:19,380 INFO] Step 9100/100000; loss: 268392.538209 acc:  58.41; ppl:  7.54; xent: 2.02; lr: 0.00100; 2774/2372 tok/s;   3171 sec\n",
      "[2019-02-17 01:14:52,308 INFO] Step 9200/100000; loss: 272233.236470 acc:  58.11; ppl:  7.67; xent: 2.04; lr: 0.00100; 4743/4058 tok/s;   3204 sec\n",
      "[2019-02-17 01:15:25,659 INFO] Step 9300/100000; loss: 273201.406143 acc:  58.10; ppl:  7.50; xent: 2.01; lr: 0.00100; 4786/4067 tok/s;   3237 sec\n",
      "[2019-02-17 01:15:59,064 INFO] Step 9400/100000; loss: 277196.518365 acc:  58.04; ppl:  7.56; xent: 2.02; lr: 0.00100; 4843/4103 tok/s;   3270 sec\n",
      "[2019-02-17 01:16:31,962 INFO] Step 9500/100000; loss: 271221.367023 acc:  58.50; ppl:  7.45; xent: 2.01; lr: 0.00100; 4808/4104 tok/s;   3303 sec\n",
      "[2019-02-17 01:16:32,068 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last accent valid acc: -562833.7909545898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-17 01:16:55,036 INFO] Validation perplexity: 12.2124\n",
      "[2019-02-17 01:16:55,037 INFO] Validation accuracy: 53.788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save last accent model with acc: -562833.7909545898\n",
      "Decent round: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-17 01:17:06,252 INFO] Step 9600/100000; loss: 275911.765401 acc:  58.16; ppl:  7.66; xent: 2.04; lr: 0.00100; 4692/3952 tok/s;   3338 sec\n",
      "[2019-02-17 01:17:16,154 INFO] Step 9700/100000; loss: 250403.971301 acc:  59.51; ppl:  6.97; xent: 1.94; lr: 0.00100; 15042/13027 tok/s;   3347 sec\n",
      "[2019-02-17 01:17:27,547 INFO] Step 9800/100000; loss: 289771.920270 acc:  58.11; ppl:  7.60; xent: 2.03; lr: 0.00100; 14907/12540 tok/s;   3359 sec\n",
      "[2019-02-17 01:17:38,073 INFO] Step 9900/100000; loss: 251920.432582 acc:  59.51; ppl:  6.82; xent: 1.92; lr: 0.00100; 14609/12466 tok/s;   3369 sec\n",
      "[2019-02-17 01:17:48,876 INFO] Step 10000/100000; loss: 258893.054366 acc:  59.16; ppl:  7.04; xent: 1.95; lr: 0.00100; 14362/12281 tok/s;   3380 sec\n",
      "[2019-02-17 01:17:49,017 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last accent valid acc: -562833.7909545898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-17 01:18:11,974 INFO] Validation perplexity: 12.3844\n",
      "[2019-02-17 01:18:11,975 INFO] Validation accuracy: 53.6671\n",
      "[2019-02-17 01:18:11,976 INFO] Saving checkpoint /mnt/drive-2t/model/opennmt_153a72b0efdd37e8921bffa8bd6cc110_step_10000.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decent round: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-17 01:18:25,317 INFO] Step 10100/100000; loss: 270220.985155 acc:  58.73; ppl:  7.31; xent: 1.99; lr: 0.00100; 4354/3728 tok/s;   3417 sec\n",
      "[2019-02-17 01:18:36,674 INFO] Step 10200/100000; loss: 276800.884822 acc:  58.99; ppl:  7.16; xent: 1.97; lr: 0.00100; 14686/12383 tok/s;   3428 sec\n",
      "[2019-02-17 01:18:47,530 INFO] Step 10300/100000; loss: 265399.246075 acc:  59.13; ppl:  7.01; xent: 1.95; lr: 0.00100; 14671/12556 tok/s;   3439 sec\n",
      "[2019-02-17 01:18:58,235 INFO] Step 10400/100000; loss: 255566.289446 acc:  59.47; ppl:  6.87; xent: 1.93; lr: 0.00100; 14545/12386 tok/s;   3450 sec\n",
      "[2019-02-17 01:19:09,323 INFO] Loading dataset from data/nmt15-reverse.train.0.pt, number of examples: 95844\n",
      "[2019-02-17 01:19:10,826 INFO] Step 10500/100000; loss: 250030.983415 acc:  59.76; ppl:  6.76; xent: 1.91; lr: 0.00100; 12181/10396 tok/s;   3462 sec\n",
      "[2019-02-17 01:19:10,970 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last accent valid acc: -562833.7909545898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-17 01:19:33,790 INFO] Validation perplexity: 12.0941\n",
      "[2019-02-17 01:19:33,791 INFO] Validation accuracy: 54.0071\n",
      "[2019-02-17 01:20:06,761 INFO] Step 10600/100000; loss: 256697.860239 acc:  59.59; ppl:  6.85; xent: 1.92; lr: 0.00100; 2791/2385 tok/s;   3518 sec\n",
      "[2019-02-17 01:20:39,604 INFO] Step 10700/100000; loss: 257913.425544 acc:  59.41; ppl:  6.94; xent: 1.94; lr: 0.00100; 4746/4055 tok/s;   3551 sec\n",
      "[2019-02-17 01:21:13,613 INFO] Step 10800/100000; loss: 271729.811259 acc:  59.18; ppl:  7.01; xent: 1.95; lr: 0.00100; 4838/4104 tok/s;   3585 sec\n",
      "[2019-02-17 01:21:46,601 INFO] Step 10900/100000; loss: 255714.754768 acc:  59.60; ppl:  6.73; xent: 1.91; lr: 0.00100; 4790/4067 tok/s;   3618 sec\n",
      "[2019-02-17 01:22:19,575 INFO] Step 11000/100000; loss: 254901.545631 acc:  59.93; ppl:  6.72; xent: 1.90; lr: 0.00100; 4748/4059 tok/s;   3651 sec\n",
      "[2019-02-17 01:22:19,680 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last accent valid acc: -560795.9261474609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-17 01:22:42,557 INFO] Validation perplexity: 12.1283\n",
      "[2019-02-17 01:22:42,557 INFO] Validation accuracy: 54.0516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Save last accent model with acc: -560795.9261474609\n",
      "Decent round: 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-17 01:22:53,792 INFO] Step 11100/100000; loss: 267352.532779 acc:  59.22; ppl:  7.05; xent: 1.95; lr: 0.00100; 4753/4001 tok/s;   3685 sec\n",
      "[2019-02-17 01:23:03,609 INFO] Step 11200/100000; loss: 234678.988677 acc:  60.88; ppl:  6.31; xent: 1.84; lr: 0.00100; 14992/12975 tok/s;   3695 sec\n",
      "[2019-02-17 01:23:15,108 INFO] Step 11300/100000; loss: 278737.842412 acc:  59.38; ppl:  6.97; xent: 1.94; lr: 0.00100; 14809/12489 tok/s;   3706 sec\n",
      "[2019-02-17 01:23:25,566 INFO] Step 11400/100000; loss: 237502.039271 acc:  61.09; ppl:  6.20; xent: 1.82; lr: 0.00100; 14592/12447 tok/s;   3717 sec\n",
      "[2019-02-17 01:23:36,245 INFO] Step 11500/100000; loss: 248149.248441 acc:  60.45; ppl:  6.45; xent: 1.86; lr: 0.00100; 14595/12471 tok/s;   3728 sec\n",
      "[2019-02-17 01:23:36,342 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last accent valid acc: -560795.9261474609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-17 01:23:59,215 INFO] Validation perplexity: 12.3651\n",
      "[2019-02-17 01:23:59,217 INFO] Validation accuracy: 53.9147\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decent round: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-17 01:24:10,213 INFO] Step 11600/100000; loss: 260379.355594 acc:  59.81; ppl:  6.76; xent: 1.91; lr: 0.00100; 4679/4012 tok/s;   3761 sec\n",
      "[2019-02-17 01:24:21,666 INFO] Step 11700/100000; loss: 267374.583434 acc:  60.19; ppl:  6.60; xent: 1.89; lr: 0.00100; 14740/12373 tok/s;   3773 sec\n",
      "[2019-02-17 01:24:32,373 INFO] Step 11800/100000; loss: 252282.734117 acc:  60.48; ppl:  6.41; xent: 1.86; lr: 0.00100; 14805/12687 tok/s;   3784 sec\n",
      "[2019-02-17 01:24:43,644 INFO] Step 11900/100000; loss: 248375.922256 acc:  60.42; ppl:  6.40; xent: 1.86; lr: 0.00100; 13968/11871 tok/s;   3795 sec\n",
      "[2019-02-17 01:24:54,130 INFO] Loading dataset from data/nmt15-reverse.train.0.pt, number of examples: 95844\n",
      "[2019-02-17 01:24:55,846 INFO] Step 12000/100000; loss: 233175.226304 acc:  61.08; ppl:  6.08; xent: 1.81; lr: 0.00100; 12391/10588 tok/s;   3807 sec\n",
      "[2019-02-17 01:24:55,943 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last accent valid acc: -560795.9261474609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-17 01:25:18,758 INFO] Validation perplexity: 12.2206\n",
      "[2019-02-17 01:25:18,758 INFO] Validation accuracy: 54.2076\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decent round: 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-17 01:25:29,622 INFO] Step 12100/100000; loss: 249227.554929 acc:  60.80; ppl:  6.34; xent: 1.85; lr: 0.00100; 4666/3996 tok/s;   3841 sec\n",
      "[2019-02-17 01:25:40,125 INFO] Step 12200/100000; loss: 240656.699547 acc:  60.68; ppl:  6.33; xent: 1.85; lr: 0.00100; 14523/12420 tok/s;   3851 sec\n",
      "[2019-02-17 01:25:51,685 INFO] Step 12300/100000; loss: 262568.810959 acc:  60.23; ppl:  6.50; xent: 1.87; lr: 0.00100; 14340/12136 tok/s;   3863 sec\n",
      "[2019-02-17 01:26:02,543 INFO] Step 12400/100000; loss: 244385.998502 acc:  60.93; ppl:  6.20; xent: 1.82; lr: 0.00100; 14521/12342 tok/s;   3874 sec\n",
      "[2019-02-17 01:26:13,489 INFO] Step 12500/100000; loss: 248908.065883 acc:  60.99; ppl:  6.27; xent: 1.84; lr: 0.00100; 14507/12383 tok/s;   3885 sec\n",
      "[2019-02-17 01:26:13,585 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last accent valid acc: -560795.9261474609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-17 01:26:36,559 INFO] Validation perplexity: 12.1092\n",
      "[2019-02-17 01:26:36,560 INFO] Validation accuracy: 54.3783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decent round: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-17 01:26:47,803 INFO] Step 12600/100000; loss: 252385.248033 acc:  60.57; ppl:  6.46; xent: 1.87; lr: 0.00100; 4683/3944 tok/s;   3919 sec\n",
      "[2019-02-17 01:26:58,081 INFO] Step 12700/100000; loss: 237192.034684 acc:  61.37; ppl:  6.09; xent: 1.81; lr: 0.00100; 14829/12778 tok/s;   3929 sec\n",
      "[2019-02-17 01:27:09,279 INFO] Step 12800/100000; loss: 257974.215482 acc:  60.85; ppl:  6.23; xent: 1.83; lr: 0.00100; 14897/12595 tok/s;   3941 sec\n",
      "[2019-02-17 01:27:19,766 INFO] Step 12900/100000; loss: 224642.598003 acc:  62.25; ppl:  5.74; xent: 1.75; lr: 0.00100; 14339/12264 tok/s;   3951 sec\n",
      "[2019-02-17 01:27:30,481 INFO] Step 13000/100000; loss: 236886.305157 acc:  61.52; ppl:  5.98; xent: 1.79; lr: 0.00100; 14452/12361 tok/s;   3962 sec\n",
      "[2019-02-17 01:27:30,578 INFO] Loading dataset from data/nmt15-reverse.valid.0.pt, number of examples: 10654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "last accent valid acc: -560795.9261474609\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2019-02-17 01:27:53,652 INFO] Validation perplexity: 12.3892\n",
      "[2019-02-17 01:27:53,653 INFO] Validation accuracy: 54.1907\n",
      "[2019-02-17 01:27:53,654 INFO] Saving checkpoint /mnt/drive-2t/model/opennmt_153a72b0efdd37e8921bffa8bd6cc110_step_11000.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decent round: 5\n",
      "meet early stop condition, prev best loss: -560795.9261474609 current loss: -566220.6053466797\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'153a72b0efdd37e8921bffa8bd6cc110'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_args = {\n",
    "    'model_dtype': 'fp32',\n",
    "    'encoder_type': 'rnn',\n",
    "    'decoder_type': 'rnn',\n",
    "#     'rnn_type': 'GRU',\n",
    "#     'layers': 3,\n",
    "#     'max_generator_batches': 2,\n",
    "    'dropout': 0.3,\n",
    "#     'enc_rnn_size': 512,\n",
    "#     'dec_rnn_size': 512,\n",
    "#     'src_word_vec_size': 512,\n",
    "#     'tgt_word_vec_size': 512\n",
    "}\n",
    "train_args = {\n",
    "    'batch_size': 64,\n",
    "    'train_steps': 100000,\n",
    "    'optim': 'adam',\n",
    "    'learning_rate': 0.001,\n",
    "    'valid_step': 500,\n",
    "    'valid_batch': 32,\n",
    "    'early_stop_round': 5,\n",
    "    'early_stop_threshold': 0.1,\n",
    "    'report_every': 100\n",
    "}\n",
    "\n",
    "total_args = {}\n",
    "total_args.update(model_args)\n",
    "total_args.update(train_args)\n",
    "\n",
    "reverse_runner.run(total_args)\n",
    "reverse_runner.model_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
